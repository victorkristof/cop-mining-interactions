{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b1_extract_all_paragraphes\n",
    "From the list_meetings.csv extract all the paragraphes in the html page for each issues and create a file paragraphes.txt with it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen, Request\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_paragraphe(paragraphes):\n",
    "    \"\"\" Write \"paragraphes\" into a file paragraphes.txt. \"\"\"\n",
    "    paragraphes = list(set(paragraphes))\n",
    "    # generate list_p_tags.txt file\n",
    "    outF = open(\"paragraphes\"+str(number)+\".txt\", \"w\")\n",
    "    for line in paragraphes:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_p(paragraphes):\n",
    "    \"\"\" Remove all the text into paragraphes that is no needed.\"\"\"\n",
    "    paragraphes = [p.replace('<p align=\"justify\">','') for p in paragraphes]\n",
    "    paragraphes = [p.replace('<p align=\"CENTER\">','') for p in paragraphes] \n",
    "    #paragraphes = [re.sub('<strong>.+</strong>',' ',str(p)) for p in paragraphes]\n",
    "    paragraphes = [re.sub('<p align=\"justify\" class=\"ENB-Body\">','',p) for p in paragraphes]\n",
    "    paragraphes = [re.sub('<font face=\"Verdana\" size=\"2\">','',p) for p in paragraphes]\n",
    "    paragraphes = [re.sub(r'<font.*?>','',p) for p in paragraphes]\n",
    "    paragraphes = [re.sub(r'<a.*?>','',p) for p in paragraphes]\n",
    "    paragraphes = [re.sub(r'<.*?>','',p) for p in paragraphes]\n",
    "    #paragraphes = [p for p in paragraphes if not p.isupper()] \n",
    "    return paragraphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_html(page_string):\n",
    "    paragraphes = re.split(r'<p>|</p>|<b>|</b>|<p ',page_string)\n",
    "    start = \"<p>\"\n",
    "    end = \"</p>\"\n",
    "    page_string = [start+ p+ end for p in paragraphes]\n",
    "    return page_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page_to_parse(page_string):\n",
    "    page_string = find_bad_html(str(page_string))\n",
    "    \"\"\" Remove comment at the beginning of the hml and also put the <html> tag in lowercase and <b> tags to be able to collect all paragraphes. \"\"\"\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub('<!-- WWW design Jeff Anderson janderson@iisd.ca ---!>','',str(page_string))\n",
    "    page_string = re.sub(\"<!--.*?<html\", '<html', page_string, flags=re.MULTILINE)\n",
    "\n",
    "    page_string = re.sub(r'<HTML>',r'<html>',str(page_string))\n",
    "    page_string = re.sub(r'</HTML>',r'</html>',str(page_string))\n",
    "\n",
    "    return bytes(page_string,'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(p):\n",
    "    \"\"\" Remove special char to be able to detect easier sentences. \"\"\"\n",
    "    s = p.replace(\"\\r\",\" \")\n",
    "    s = s.replace(r'\\x',\" \")\n",
    "    s = s.replace(\"\\n\",\" \")\n",
    "    s = s.replace(\"\\t\",\" \")\n",
    "    s = s.replace(\"\\\\x\",\" \")\n",
    "    s = s.replace(\"\\\\r\",\" \")\n",
    "    s = s.replace(\"\\\\n\",\" \")\n",
    "    s = s.replace(\"\\\\t\",\" \")\n",
    "    s = s.replace(\"\\\\\",\" \")\n",
    "    q = re.compile('\\s\\s+')\n",
    "    p = re.compile('\\n\\n+')\n",
    "    s = q.sub(' ',s)\n",
    "    s = p.sub(' ',s)\n",
    "    #s = re.split('<p>',s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(paragraphes):\n",
    "    \"\"\" Remove footer of the page with conditions. \"\"\"\n",
    "    k = len(paragraphes)\n",
    "    for i in range(len(paragraphes)):\n",
    "        if('IN THE CORRIDORS' in paragraphes[i] or 'THINGS TO LOOK'  in paragraphes[i] or 'This issue of' in paragraphes[i]  or 'BRIEF ANALYSIS OF' in paragraphes[i]):\n",
    "            k=i\n",
    "            break\n",
    "\n",
    "    return paragraphes[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_p_tags(html_link):\n",
    "    \"\"\" Extract the <p> tag from a specific html link. \"\"\"\n",
    "    #Parse the page \n",
    "    r = Request(html_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(r).read()\n",
    "    page = clean_page_to_parse(page)\n",
    "    soup = soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    list_tp = soup.find_all('p',recursive=False)\n",
    "    if(len(list_tp) == 0):\n",
    "\n",
    "        x = \".\"\n",
    "        list_tp = list(soup.find_all('p'))\n",
    "    #Extract all the text and remove undesired paragraphes\n",
    "    list_tp2 = []\n",
    "    for p in list_tp:\n",
    "        list_tp2 += re.split('<p>',remove_special_char(str(p)))\n",
    "    list_tp = remove_footer(list_tp2)\n",
    "    # Remove titles and sentences all in uppercase\n",
    "    list_tp = remove_unwanted_p(list_tp)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_p_tags_411(html_link):\n",
    "    \"\"\" Extract the <p> tag from a specific html link. \"\"\"\n",
    "    #Parse the page \n",
    "    r = Request(html_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(r).read()\n",
    "    page = clean_page_to_parse(page)\n",
    "    soup = soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    list_tp = soup.find_all('td',recursive=False)\n",
    "    if(len(list_tp) == 0):\n",
    "\n",
    "        x = \".\"\n",
    "        list_tp = list(soup.find_all('td'))\n",
    "    #Extract all the text and remove undesired paragraphes\n",
    "    list_tp2 = []\n",
    "    \n",
    "    for p in list_tp:\n",
    "        list_tp2 += re.split(r'<br/><br/>|<br /><br />',remove_special_char(str(p)))\n",
    "    list_tp = remove_footer(list_tp2)\n",
    "    # Remove titles and sentences all in uppercase\n",
    "    list_tp = remove_unwanted_p(list_tp)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_p_tags_45(html_link):\n",
    "    \"\"\" Extract <p> tags from link inside \"html_link\" for Issue# <45. \"\"\"\n",
    "    r = Request(html_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page_link = urlopen(r).read()\n",
    "    #page_link = urllib.request.urlopen(html_link).read()\n",
    "    soup_link = BeautifulSoup(page_link)\n",
    "    paragraphes = soup_link.findAll('a',href = re.compile('\\d+'))\n",
    "    list_tp = []\n",
    "\n",
    "    for pa in paragraphes:\n",
    "        # doesn't use the link of the main page (all issues)\n",
    "        if(pa['href'] != '1200000e.html'):\n",
    "            html_link = 'https://enb.iisd.org/vol12/'+pa['href']\n",
    "            list_tp += extract_p_tags(html_link)\n",
    "\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_list_issues(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    list_pt = []\n",
    "    return list(csv_f)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_pargraphes(list_paragraphes):\n",
    "    \"\"\" Generate list_p_tags.txt file. \"\"\"\n",
    "    outF = open(\"paragraphes.txt\", \"w\")\n",
    "    for line in list_paragraphes:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_p_tags(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    list_meetings = extract_from_csv_list_issues(csv_file)\n",
    "    list_pt = []\n",
    "    for i, row in enumerate(list_meetings):\n",
    "\n",
    "        #Extract for 0 < Issue# < 45 \n",
    "        if(int(row[4]) < 45 ):\n",
    "            list_pt += extract_p_tags_45(row[6])\n",
    "\n",
    "        #Extract for 66 < Issue# < 775\n",
    "        else :\n",
    "            list_pt = []\n",
    "            url =  row[6]\n",
    "            \n",
    "            request = requests.get(url)\n",
    "            if(int(row[4]) != 175 and int(row[4]) != 300 and request.status_code == 200):\n",
    "                if(411 <= int(row[4]) and int(row[4]) <= 420):\n",
    "                    list_pt += extract_p_tags_411(row[6])\n",
    "                else:\n",
    "                    list_pt = extract_p_tags(row[6]) \n",
    "\n",
    "\n",
    "        print(f'{(i+1)/len(list_meetings)*100:.2f}%', end='\\r')\n",
    "    write_list_pargraphes(list_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 54.1 s, sys: 2.34 s, total: 56.5 s\nWall time: 10min 30s\n"
    }
   ],
   "source": [
    "%%time\n",
    "extract_from_csv_p_tags('Text/list_meetings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}