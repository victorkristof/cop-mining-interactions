{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b1_extract_all_paragraphes\n",
    "From the list_meetings.csv extract all the paragraphes in the html page for each issues and create a file paragraphes.txt with it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_paragraphe(paragraphes):\n",
    "    \"\"\" Write \"paragraphes\" into a file paragraphes.txt. \"\"\"\n",
    "    paragraphes = list(set(paragraphes))\n",
    "    # generate list_p_tags.txt file\n",
    "    outF = open(\"paragraphes\"+str(number)+\".txt\", \"w\")\n",
    "    for line in paragraphes:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_headlines(paragraphes):\n",
    "    \"\"\" Find all the headlines in the html page and remove it from paragraphes. \"\"\"\n",
    "    list_tp_f = []\n",
    "    for p in paragraphes:\n",
    "        if('class=\"ENB-Headline3' not in p):\n",
    "            list_tp_f.append(p)\n",
    "    return list_tp_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_p(paragraphes):\n",
    "    \"\"\" Remove all the text into paragraphes that is no needed.\"\"\"\n",
    "    paragraphes = re.split(r'<p>|</p>','\\n'.join(paragraphes))\n",
    "    paragraphes = check_headlines(paragraphes)\n",
    "    paragraphes = [p.replace('<p align=\"justify\">','') for p in paragraphes]\n",
    "    paragraphes = [p.replace('<p align=\"CENTER\">','') for p in paragraphes]\n",
    "    paragraphes = [re.sub('<strong>.+</strong>',' ',str(p)) for p in paragraphes]\n",
    "    paragraphes = [re.sub(r'<.*?>','',p) for p in paragraphes]\n",
    "    paragraphes = [p for p in paragraphes if not p.isupper()] \n",
    "    return paragraphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page_to_parse(page_string):\n",
    "    \"\"\" Remove comment at the beginning of the hml and also put the <html> tag in lowercase and <b> tags to be able to collect all paragraphes. \"\"\"\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub(r'<HTML>',r'<html>',str(page_string))\n",
    "    page_string = re.sub(r'</HTML>',r'</html>',str(page_string))\n",
    "    page_string = re.sub(r'<b>',r'</p><p>',str(page_string))\n",
    "    page_string = re.sub(r'</b>',r'</p>',str(page_string))\n",
    "\n",
    "    return bytes(page_string,'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(p):\n",
    "    \"\"\" Remove special char to be able to detect easier sentences. \"\"\"\n",
    "    s = p.replace(\"\\r\",\" \")\n",
    "    s = s.replace(r'\\x',\" \")\n",
    "    s = s.replace(\"\\n\",\" \")\n",
    "    s = s.replace(\"\\\\x\",\" \")\n",
    "    s = s.replace(\"\\\\r\",\" \")\n",
    "    s = s.replace(\"\\\\n\",\" \")\n",
    "    q = re.compile('\\s\\s+')\n",
    "    s = q.sub(' ',s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(paragraphes):\n",
    "    \"\"\" Remove footer of the page with conditions. \"\"\"\n",
    "    k = len(paragraphes)\n",
    "    for i in range(len(paragraphes)):\n",
    "        if('THINGS TO LOOK'  in paragraphes[i] or 'This issue of' in paragraphes[i] or 'IN THE CORRIDORS' in paragraphes[i] or 'BRIEF ANALYSIS OF' in paragraphes[i]):\n",
    "            k=i\n",
    "            break\n",
    "    \n",
    "    return paragraphes[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_p_tags(html_link):\n",
    "    \"\"\" Extract the <p> tag from a specific html link. \"\"\"\n",
    "    #Parse the page \n",
    "    page = urlopen(html_link).read()\n",
    "    page = clean_page_to_parse(page)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    list_tp = soup.find_all('p',recursive=False)\n",
    "    if(len(list_tp) == 0):\n",
    "        x = \".\"\n",
    "        list_tp = list(soup.find_all('p',recursive=\"False\"))\n",
    "\n",
    "\n",
    "    #Extract all the text and remove undesired paragraphes\n",
    "    j= '.-.'\n",
    "    list_tp = [remove_special_char(str(p)) for p in list_tp]\n",
    "    list_tp = remove_footer(list_tp)\n",
    "    # Remove titles and sentences all in uppercase\n",
    "    list_tp = remove_unwanted_p(list_tp)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_p_tags_45(html_link):\n",
    "    \"\"\" Extract <p> tags from link inside \"html_link\" for Issue# <45. \"\"\"\n",
    "    page_link = urllib.request.urlopen(html_link).read()\n",
    "    soup_link = BeautifulSoup(page_link)\n",
    "    paragraphes = soup_link.findAll('a',href = re.compile('\\d+'))\n",
    "    list_tp = []\n",
    "\n",
    "    for pa in paragraphes:\n",
    "        # doesn't use the link of the main page (all issues)\n",
    "        if(pa['href'] != '1200000e.html'):\n",
    "            html_link = 'https://enb.iisd.org/vol12/'+pa['href']\n",
    "            list_tp += extract_p_tags(html_link)\n",
    "\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_list_issues(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    list_pt = []\n",
    "    return list(csv_f)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_pargraphes(list_paragraphes):\n",
    "    \"\"\" Generate list_p_tags.txt file. \"\"\"\n",
    "    outF = open(\"paragraphes.txt\", \"w\")\n",
    "    for line in list_paragraphes:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_p_tags(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    list_meetings = extract_from_csv_list_issues(csv_file)\n",
    "    list_pt = []\n",
    "    for i, row in enumerate(list_meetings):\n",
    "\n",
    "        #Extract for 0 < Issue# < 45 \n",
    "        if(int(row[4]) < 45 ):\n",
    "            list_pt += extract_p_tags_45(row[6])\n",
    "    \n",
    "        #Extract for 66 < Issue# < 775\n",
    "        else :\n",
    "            request = requests.get(row[6])\n",
    "            if(request.status_code == 200):\n",
    "                list_pt += extract_p_tags(row[6])\n",
    "\n",
    "        print(f'{(i+1)/len(list_meetings)*100:.2f}%', end='\\r')\n",
    "    write_list_pargraphes(list_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.14%0.27%0.41%0.54%0.68%0.82%0.95%1.09%1.23%1.36%1.50%1.63%1.77%1.91%2.04%2.18%2.32%2.45%2.59%2.72%2.86%3.00%3.13%3.27%3.41%3.54%3.68%3.81%3.95%4.09%4.22%4.36%4.50%4.63%4.77%4.90%5.04%5.18%5.31%5.45%5.59%5.72%5.86%5.99%6.13%6.27%6.40%6.54%6.68%6.81%6.95%7.08%7.22%7.36%7.49%7.63%7.77%7.90%8.04%8.17%8.31%8.45%8.58%8.72%8.86%8.99%9.13%9.26%9.40%9.54%9.67%9.81%9.95%10.08%10.22%10.35%10.49%10.63%10.76%10.90%11.04%11.17%11.31%11.44%11.58%11.72%11.85%11.99%12.13%12.26%12.40%12.53%12.67%12.81%12.94%13.08%13.22%13.35%13.49%13.62%13.76%13.90%14.03%14.17%14.31%14.44%14.58%14.71%14.85%14.99%15.12%15.26%15.40%15.53%15.67%15.80%15.94%16.08%16.21%16.35%16.49%16.62%16.76%16.89%17.03%17.17%17.30%17.44%17.57%17.71%17.85%17.98%18.12%18.26%18.39%18.53%18.66%18.80%18.94%19.07%19.21%19.35%19.48%19.62%19.75%19.89%20.03%20.16%20.30%20.44%20.57%20.71%20.84%20.98%21.12%21.25%21.39%21.53%21.66%21.80%21.93%22.07%22.21%22.34%22.48%22.62%22.75%22.89%23.02%23.16%23.30%23.43%23.57%23.71%23.84%23.98%24.11%24.25%24.39%24.52%24.66%24.80%24.93%25.07%25.20%25.34%25.48%25.61%25.75%25.89%26.02%26.16%26.29%26.43%26.57%26.70%26.84%26.98%27.11%27.25%27.38%27.52%27.66%27.79%27.93%28.07%28.20%28.34%28.47%28.61%28.75%28.88%29.02%29.16%29.29%29.43%29.56%29.70%29.84%29.97%30.11%30.25%30.38%30.52%30.65%30.79%30.93%31.06%31.20%31.34%31.47%31.61%31.74%31.88%32.02%32.15%32.29%32.43%32.56%32.70%32.83%32.97%33.11%33.24%33.38%33.51%33.65%33.79%33.92%34.06%34.20%34.33%34.47%34.60%34.74%34.88%35.01%35.15%35.29%35.42%35.56%35.69%35.83%35.97%36.10%36.24%36.38%36.51%36.65%36.78%36.92%37.06%37.19%37.33%37.47%37.60%37.74%37.87%38.01%38.15%38.28%38.42%38.56%38.69%38.83%38.96%39.10%39.24%39.37%39.51%39.65%39.78%39.92%40.05%40.19%40.33%40.46%40.60%40.74%40.87%41.01%41.14%41.28%41.42%41.55%41.69%41.83%41.96%42.10%42.23%42.37%42.51%42.64%42.78%42.92%43.05%43.19%43.32%43.46%43.60%43.73%43.87%44.01%44.14%44.28%44.41%44.55%44.69%44.82%44.96%45.10%45.23%45.37%45.50%45.64%45.78%45.91%46.05%46.19%46.32%46.46%46.59%46.73%46.87%47.00%47.14%47.28%47.41%47.55%47.68%47.82%47.96%48.09%48.23%48.37%48.50%48.64%48.77%48.91%49.05%49.18%49.32%49.46%49.59%49.73%49.86%50.00%50.14%50.27%50.41%50.54%50.68%50.82%50.95%51.09%51.23%51.36%51.50%51.63%51.77%51.91%52.04%52.18%52.32%52.45%52.59%52.72%52.86%53.00%53.13%53.27%53.41%53.54%53.68%53.81%53.95%54.09%54.22%54.36%54.50%54.63%54.77%54.90%55.04%55.18%55.31%55.45%55.59%55.72%55.86%55.99%56.13%56.27%56.40%56.54%56.68%56.81%56.95%57.08%57.22%57.36%57.49%57.63%57.77%57.90%58.04%58.17%58.31%58.45%58.58%58.72%58.86%58.99%59.13%59.26%59.40%59.54%59.67%59.81%59.95%60.08%60.22%60.35%60.49%60.63%60.76%60.90%61.04%61.17%61.31%61.44%61.58%61.72%61.85%61.99%62.13%62.26%62.40%62.53%62.67%62.81%62.94%63.08%63.22%63.35%63.49%63.62%63.76%63.90%64.03%64.17%64.31%64.44%64.58%64.71%64.85%64.99%65.12%65.26%65.40%65.53%65.67%65.80%65.94%66.08%66.21%66.35%66.49%66.62%66.76%66.89%67.03%67.17%67.30%67.44%67.57%67.71%67.85%67.98%68.12%68.26%68.39%68.53%68.66%68.80%68.94%69.07%69.21%69.35%69.48%69.62%69.75%69.89%70.03%70.16%70.30%70.44%70.57%70.71%70.84%70.98%71.12%71.25%71.39%71.53%71.66%71.80%71.93%72.07%72.21%72.34%72.48%72.62%72.75%72.89%73.02%73.16%73.30%73.43%73.57%73.71%73.84%73.98%74.11%74.25%74.39%74.52%74.66%74.80%74.93%75.07%75.20%75.34%75.48%75.61%75.75%75.89%76.02%76.16%76.29%76.43%76.57%76.70%76.84%76.98%77.11%77.25%77.38%77.52%77.66%77.79%77.93%78.07%78.20%78.34%78.47%78.61%78.75%78.88%79.02%79.16%79.29%79.43%79.56%79.70%79.84%79.97%80.11%80.25%80.38%80.52%80.65%80.79%80.93%81.06%81.20%81.34%81.47%81.61%81.74%81.88%82.02%82.15%82.29%82.43%82.56%82.70%82.83%82.97%83.11%83.24%83.38%83.51%83.65%83.79%83.92%84.06%84.20%84.33%84.47%84.60%84.74%84.88%85.01%85.15%85.29%85.42%85.56%85.69%85.83%85.97%86.10%86.24%86.38%86.51%86.65%86.78%86.92%87.06%87.19%87.33%87.47%87.60%87.74%87.87%88.01%88.15%88.28%88.42%88.56%88.69%88.83%88.96%89.10%89.24%89.37%89.51%89.65%89.78%89.92%90.05%90.19%90.33%90.46%90.60%90.74%90.87%91.01%91.14%91.28%91.42%91.55%91.69%91.83%91.96%92.10%92.23%92.37%92.51%92.64%92.78%92.92%93.05%93.19%93.32%93.46%93.60%93.73%93.87%94.01%94.14%94.28%94.41%94.55%94.69%94.82%94.96%95.10%95.23%95.37%95.50%95.64%95.78%95.91%96.05%96.19%96.32%96.46%96.59%96.73%96.87%97.00%97.14%97.28%97.41%97.55%97.68%97.82%97.96%98.09%98.23%98.37%98.50%98.64%98.77%98.91%99.05%99.18%99.32%99.46%99.59%99.73%99.86%100.00%"
    }
   ],
   "source": [
    "extract_from_csv_p_tags('list_meetings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}