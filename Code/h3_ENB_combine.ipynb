{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import h1_ENB_occ as h1 \n",
    "import h2_ENB_gen as h2 \n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import string\n",
    "import collections\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import ngrams\n",
    "import pandas as pd \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_issue():\n",
    "     with open(\"Text/ENB_relationships.txt\") as infile:\n",
    "        k=0\n",
    "        num = []\n",
    "        for line in infile:\n",
    "            x = line.replace('\"','').split('\\t')[:26]\n",
    "\n",
    "            if(k > 0 and  508 < int(x[23]) and int(x[23]) < 510 ):\n",
    "\n",
    "                num.append(int(x[23]))\n",
    "            k +=1\n",
    "        return list(set(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "list_issue_num = extract_list_issue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gen():\n",
    "    frames1 = []\n",
    "    frames2 = []\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for issue_number in list_issue_num:\n",
    "        df1 = h1.extract_relationships_count(issue_number)\n",
    "        df2 = h2.interactions(issue_number)\n",
    "        frames1.append(df1[0])\n",
    "        frames2.append(df2[0]) \n",
    "        s1.append(df1[1])\n",
    "        s2.append(df2[1])\n",
    "    return pd.concat(frames1), pd.concat(frames2), s1, s2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Issue  509\n"
    }
   ],
   "source": [
    "df_original, df_generated, so, sg = extract_gen()\n",
    "number_original = len(df_original['issue'].to_numpy())\n",
    "number_generated = len(df_generated['issue'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char.\"\"\"\n",
    "    s = re.sub('\\n',' ',sentence)\n",
    "    s = re.sub(r'\\\\',' ',s)\n",
    "    s = s.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace('\\t','')\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, country):\n",
    "    \"\"\"Split the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "    if(country):\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        tokens_entities.append(['for'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "        tokens_entities.append(['speaking','for'])\n",
    "    else :\n",
    "        tokens_entities.append(['speaking','for','the'])\n",
    "        tokens_entities.append(['speaking','for'])\n",
    "        tokens_entities.append(['on','behalf','of','the'])\n",
    "        tokens_entities.append(['spoke','with'])\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        tokens_entities.append(['for','the'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "        \n",
    "    tokens_entities.append(['supported','by'])\n",
    "    tokens_entities.append(['opposed','by'])\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "    if(type(sentence) == list):\n",
    "        line = sentence[0].replace(\",\",\"\")\n",
    "    else: \n",
    "        line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(d_i,d_j):\n",
    "    return np.dot(d_i.T,d_j)/(np.linalg.norm(d_i)*np.linalg.norm(d_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_df(df_original, so, df_generated, sg):\n",
    "    se_orig = set(so[0])\n",
    "    df_original['sentences'] = df_original['sentences'].apply(lambda x :\"orig - \"+x )\n",
    "    se_orig = [\"orig - \"+x for x in se_orig]\n",
    "    se_gen = set(sg[0])\n",
    "    issues = df_original['issue'].drop_duplicates(keep = 'first')\n",
    "    d_original = pd.DataFrame()\n",
    "    d_generated= pd.DataFrame()\n",
    "    #Split sentence\n",
    "    s1 = [tokenize_sentence(x, True) for x in se_orig]\n",
    "    s2 = [tokenize_sentence(x, True) for x in se_gen]\n",
    "    d_original['sentences_splited'] = s1\n",
    "    d_generated['sentences_splited'] = s2\n",
    "    d_original['sentences'] = se_orig\n",
    "    d_generated['sentences'] = se_gen\n",
    "    sent_id_o = np.arange(len(se_orig))\n",
    "    sent_id_g = max(sent_id_o) +1 +np.arange(len(se_gen))\n",
    "    d_combines = pd.concat([d_original,d_generated])\n",
    "    return d_combines, sent_id_o, sent_id_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(description,word):\n",
    "    if(description.count(word) > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n(list_description, word):\n",
    "    #print([find_word(description, word) for description in list_description])\n",
    "    return np.sum([find_word(description, word) for description in list_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_max_k(description):\n",
    "    counter=collections.Counter(description)\n",
    "    return counter.most_common(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_(word,description,max_k):\n",
    "    return description.count(word)/max_k    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_TFIDf_matrix(d_combines, sent_id_o, sent_id_g):\n",
    "\n",
    "    N = max(sent_id_g) +1\n",
    "\n",
    "    #Find all the words in all the documents\n",
    "    sentences = d_combines['sentences_splited'].to_numpy().flatten()\n",
    "    words = []\n",
    "    for d in sentences:\n",
    "        words += d\n",
    "    words = set(words)\n",
    "    #Data frame with words and their index\n",
    "    d = {'word': list(words), 'index': np.arange(len(words))}\n",
    "    df_words =  pd.DataFrame(data=d)\n",
    "\n",
    "    indices = np.arange(N)\n",
    "    d_combines['index'] = indices\n",
    "\n",
    "\n",
    "    dictionary_doc =dict(zip(indices,d_combines['sentences_splited'].to_numpy()))\n",
    "    dictionary_max_k =dict(zip(indices,d_combines.apply(lambda x: define_max_k(x['sentences_splited']),axis =1 )))\n",
    "    df_words['n'] = [find_n(d_combines['sentences_splited'].to_numpy(), w) for w in df_words['word'].to_numpy()]\n",
    "\n",
    "    df_words['IDF'] = df_words['n'].apply(lambda x: - np.log2(int(x)/N))\n",
    "\n",
    "    for id_d in indices:\n",
    "        df_words[str(id_d)] = [count_(w,dictionary_doc[id_d],dictionary_max_k[id_d]) for w in df_words['word'].to_numpy() ]\n",
    "\n",
    "    df_words_TF = df_words.drop(['word', 'index','n','IDF'], axis=1)\n",
    "    matrix_words_TF = df_words_TF.to_numpy()\n",
    "    matrix_words_IDF = df_words['IDF'].to_numpy()\n",
    "    marix_TFIDF = (matrix_words_TF.T * matrix_words_IDF).T\n",
    "    return marix_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_dict_sentences(d_combines, matrix_TFIDF, sent_id_o, sent_id_g):\n",
    "    dict_sentence_idx = d_combines.set_index('sentences')['index'].to_dict()\n",
    "    dict_idx_sentence = d_combines.set_index('index')['sentences'].to_dict()\n",
    "    # Similarties with high Tf_DFI score\n",
    "    keys = []\n",
    "    values = []\n",
    "    print(sent_id_g)\n",
    "    for i in range(len(sent_id_g)):\n",
    "        # Index of sentence in generated\n",
    "        test_idx = sent_id_g[i]\n",
    "\n",
    "\n",
    "        #Find sentence with best similarities\n",
    "        test_v = matrix_TFIDF[:,test_idx ]\n",
    "        similarities_list = [similarities(matrix_TFIDF[:,i],test_v) for i in sent_id_o ]\n",
    "        test_s = np.argmax(similarities_list)\n",
    "        \"\"\" print('Generated sentence : \\n',dict_idx_sentence[test_idx], test_idx)\n",
    "        print('Original sentence : \\n',dict_idx_sentence[test_s], test_s)\n",
    "        print('---------------------------------------------------------------------------------------------------------------------') \"\"\"\n",
    "        keys.append(test_idx)\n",
    "        values.append(test_s)\n",
    "    dictionary_sentences_id = dict(zip(keys, values))\n",
    "    #Find the corresponding index of the sentence in the issue\n",
    "    df_generated['se_index'] = df_generated['sentences'].apply(lambda x :dict_sentence_idx[x])\n",
    "    df_original['se_index'] = df_original['sentences'].apply(lambda x :dict_sentence_idx[x])\n",
    "    return dictionary_sentences_id, dict_sentence_idx, dict_idx_sentence, df_original, df_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_differences_issue(dictionary_sentences_id):\n",
    "    frames = []\n",
    "    num_o = 0\n",
    "    num_g = 0\n",
    "    for x in dictionary_sentences_id:\n",
    "        index_g = x\n",
    "        df_g = df_generated[df_generated.apply(lambda x: x['se_index'] == index_g, axis=1)]\n",
    "        df_o = df_original[df_original.apply(lambda x: x['se_index'] == dictionary_sentences_id[index_g], axis=1)]\n",
    "        df_g = df_g.drop(['behalf','support','agreement','opposition','criticism','se_index'], axis = 1)\n",
    "        df_o = df_o.drop(['behalf','support','agreement','opposition','criticism','se_index'], axis = 1)\n",
    "        num_o += len(df_o['cooperation'].to_numpy())\n",
    "        num_g += len(df_g['cooperation'].to_numpy())\n",
    "        #df_g = df_g.drop(['Country A','Country B','behalf','support','agreement','opposition','criticism','se_index'], axis = 1)\n",
    "        #df_o = df_o.drop(['Country A','Country B','behalf','support','agreement','opposition','criticism','se_index'], axis = 1)\n",
    "        d_concat = pd.concat([df_o,df_g])\n",
    "        d_concat = d_concat.drop_duplicates(subset=['id_ca','id_cb','cooperation'], keep=False)\n",
    "        frames.append(d_concat)\n",
    "\n",
    "    return pd.concat(frames), num_o, num_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "N : 16\n(189, 16)\n[11 12 13 14 15]\n"
    }
   ],
   "source": [
    "df_combines, sent_id_o, sent_id_g =  define_df(df_original, so, df_generated, sg)\n",
    "marix_TFIDF = define_TFIDf_matrix(df_combines, sent_id_o, sent_id_g)\n",
    "print(marix_TFIDF.shape)\n",
    "dictionary_sentences_id, dict_sentence_idx, dict_idx_sentence, df_original, df_generated = define_dict_sentences(df_combines, marix_TFIDF, sent_id_o, sent_id_g)\n",
    "df_result, num_o, num_g = find_differences_issue(dictionary_sentences_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original interactions :  64\nGenerated interactions :  50\nNumber of different interactions :  22\nNumber of different interactions form generated :  4\n"
    }
   ],
   "source": [
    "print('Original interactions : ', num_o)\n",
    "print('Generated interactions : ',num_g)\n",
    "print('Number of different interactions : ',len(df_result['issue'].to_numpy()))\n",
    "print('Number of different interactions form generated : ', len(df_result[df_result.apply(lambda x: x['type'] == 'generated', axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}