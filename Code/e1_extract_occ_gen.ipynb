{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e1_extract_occ_gen\n",
    "From the file list_meetings.csv, find the number of invertions of each entity for one issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from c1_extract_paragraphe_issue.ipynb\nIssue  34\nimporting Jupyter notebook from c2_extract_sentence_issue.ipynb\n"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_occurrence_issue(occurences_meetings, s):\n",
    "        with open(s, \"w\", newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "        #header\n",
    "                writer.writerow(('entity','interventions'))\n",
    "                writer.writerows(occurences_meetings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list_meetings():\n",
    "    \"\"\" Open the csv file that contain all the meetings. \"\"\"\n",
    "    f = open('list_meetings.csv')\n",
    "    return csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tuple(line):\n",
    "    \"\"\" Extract tuple for each row from Paula's dataset. \"\"\"\n",
    "    l = line.replace('\"',\"\")\n",
    "    l = l.replace('\\n',\"\")\n",
    "    l = l.split('\\t')\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char. \"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurences(list_sentences, dict_occ, tokenizer1, tokenizer2, tokenizer3, list_entities, number):\n",
    "    \"\"\" Count number of time each entity in list_entities is mentioned in list_sentences. \"\"\"\n",
    "\n",
    "    for s in list_sentences:\n",
    "        #Split line into words with tokenizer to detetc entity\n",
    "        line = s.replace(\",\",\"\")\n",
    "        line_splited = word_tokenize(line)\n",
    "        tokens = tokenizer1.tokenize(line_splited) \n",
    "        tokens = tokenizer2.tokenize(tokens) \n",
    "        tokens = tokenizer3.tokenize(tokens) \n",
    "        tokens = [clean_tp(token) for token in tokens]\n",
    "        tokens_c = []\n",
    "        for i in range(len(tokens)-1):\n",
    "            if(tokens[i+1] !='.'):\n",
    "                tokens_c.append(tokens[i])\n",
    "\n",
    "        for entity in list_entities:\n",
    "            #Increment value of intervention of the entity\n",
    "            if(entity in tokens_c):\n",
    "                dict_occ[entity] += tokens_c.count(entity)               \n",
    "    rows = [(number, entity, dict_occ[entity]) for entity in dict_occ]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_occurences_issue_ENB(list_sentences, number):\n",
    "    \"\"\" Extract all the occurences for each entities for a specific issue. \"\"\"\n",
    "    #List sentences\n",
    "    sentences = list_tp = list_sentences\n",
    "\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_clean.txt'))]\n",
    "    list_entities = [s.replace(',','') for s in list_entities]\n",
    "    list_entities = [s.replace(':','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "\n",
    "    occurences_meetings = []\n",
    "\n",
    "    dict_occurences = dict.fromkeys(list_entities, 0)\n",
    "    occurences_meetings = count_occurences(sentences, dict_occurences, tokenizer1,tokenizer2,tokenizer3, list_entities, number)\n",
    "    return occurences_meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_list_issues(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    list_pt = []\n",
    "    l =list(csv_f)[1:]\n",
    "    issue = []\n",
    "    for x in l:\n",
    "        issue.append(int(x[4]))\n",
    "    return issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "import csv\n",
    "issues = extract_from_csv_list_issues('Text/list_meetings.csv')\n",
    "occ = []\n",
    "for x in issues:\n",
    "    p = c1.extract_paragraphes_from_issue(x)\n",
    "    s = c2.extract_from_txt_sentences(p)\n",
    "    e = extract_occurences_issue_ENB(s, x)\n",
    "    occ_i = np.sum([x[2] for x in e])\n",
    "    occ.append(occ_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sums = np.sum(occ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import collections\n",
    "from scipy.stats import pareto\n",
    "import matplotlib.pyplot as plt \n",
    "def node_degree_distribution(occ):\n",
    "    degree_sequence = sorted(occ, reverse=True)\n",
    "    degreeCount = collections.Counter(degree_sequence)\n",
    "    deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "    \"\"\"Plot the distribution of node degrees\"\"\"\n",
    "    figsize = (20,10)\n",
    "    fig, axe = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "\n",
    "    csfont = {'fontname':'Helvetica','fontsize':'18' }\n",
    "    hfont = {'fontname':'Helvetica','fontsize':'14'}\n",
    "    plt.title('Distribution of the number of intervention per documents',**csfont)\n",
    "    plt.xlabel('Number of interventions', **hfont)\n",
    "    plt.ylabel('Number of of documents', **hfont)\n",
    "    plt.plot(deg,cnt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node_degree_distribution(occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}