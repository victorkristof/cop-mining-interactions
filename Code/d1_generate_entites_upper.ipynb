{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d1_generate_entities_upper \n",
    "\n",
    "File that extract all the entities with the assumption that entities are written in uppercase when they interact. Write all of them into entities.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(list_w, name):\n",
    "    \"\"\" Write the list list_w in the file name. \"\"\"\n",
    "    list_w = np.sort(list_w)\n",
    "    outF = open(name, \"w\")\n",
    "    for line in list_w:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(name):\n",
    "    \"\"\" Read from the file name. \"\"\"\n",
    "    return open(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number(entities):\n",
    "    \"\"\" Generate blacklist with all entities with numbers. \"\"\"\n",
    "    black_list9 = []\n",
    "    for s in entities : \n",
    "        if( '77' not in s and any(i.isdigit() for i in s)):\n",
    "            black_list9.append(s)\n",
    "    return black_list9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_longuer(entities):\n",
    "    \"\"\" Generate blacklist with entities that are composed from the list patterns. \"\"\"\n",
    "    regex = '\\s[A-Z][A-Z]+'\n",
    "    patterns = ['UN','UNDP','AIJ','GREENPEACE','ICA','ICAO','IEA','II','IOC','IRAN','SBI','SBSTA','TEC','UNEP','US','CACAM','CAN','CBD','DESA','EGTT','GEF','MRV']\n",
    "    patterns += ['WWF','WHO','TFI','AWG','CBD','AIJ','WTO','WMO','UNESCO','UNECE','TSU','TRINIDAD','SRREN','SRCCL','OECD','NEPAL','NAPA','EU','TAR']\n",
    "    black_list11 = []\n",
    "\n",
    "    for b in patterns:\n",
    "        for e in entities:\n",
    "            e_splitee_splited = word_tokenize(e)\n",
    "            if(b == e_splitee_splited[0] and len(e_splitee_splited)>1):\n",
    "                black_list11.append(e)\n",
    "    write(black_list11,'list_patterns.txt')\n",
    "    return black_list11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_smaller(entities):\n",
    "    \"\"\" Generate blacklist with entities that are substring of some specifics words. \"\"\"\n",
    "    patterns = ['BURKINA','CLIMATE','CONGO','COOK','CZECH','DOMINICAN','FRANCISCAN','JAPAN AUSTRALIA','MARSHALL ISLANDS ARMENIA','NEW','ENVIRONMENTAL',\n",
    "    'WORLD','WOMEN','VI','TECHNOLOGY','BY','ANTIGUA','UMBRELLA','SIERRA','S REPUBLIC OF KOREA','PAPUA','OF KOREA','SOUTH','SUBSIDIARY BODIES','THE']\n",
    "    bl = []\n",
    "    \n",
    "\n",
    "    for p in patterns:\n",
    "        for e in entities:\n",
    "            e_splited = word_tokenize(e)\n",
    "            if(p == e):\n",
    "                bl.append(p)\n",
    "\n",
    "    return bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_list_start(entities):\n",
    "    \"\"\" Generate blacklist with a list already made to start the cleaning. \"\"\"\n",
    "    entities = set(entities)\n",
    "    black_list_s =['BRIEF','ANNUAL','AD HOC','ADVERSE EFFECTS','REPORT','AGENDA','ANALYSIS','DEVELOPMENT','DEAL','ACTIVITIES','SESSION','BEST','BOX']\n",
    "    black_list_s +=['BREEZEWAYS','BUSINESS','MEETINGS','CDM','CHAIR OF THE COW','COMMISSION DES FORÊTS','CARIBBEAN COMMUNITY','CONFERENCE','CONCLUSION','COP/']\n",
    "    black_list_s +=['IPCC','JOINT','ISSUE','LULUCF','L…','SUMMARY','CLIMATE CHANGE','GUIDELINES','PROGRAMME','ROUND TABLE','PROTOCOL','CONTACT GROUP']         \n",
    "    black_list_s +=['COULD DO MORE','PROPOSAL','FORUM','MECHANISM','CORRIDORS','XII','XIX','XIV','XVI','II','XVII','XX','XY','I GHG','I MRV','I P','I SPM','CONGRESS',                               'BUSSINESS']\n",
    "    black_list_s +=['CENTER','CHANGE','CLAIMED','FACILITY','POLICY','GLOBAL ENVIRONMENT','INFORMAL','PEOPLE','FORWARD','WORKSHOP','.','/','-','ARTICLE','+',\"D 'IVOIRE\",'\\\\','PRIVILEGES']\n",
    "    black_list1 =set(black_list_s)\n",
    "    bl = []\n",
    "    for e in entities:\n",
    "        for w in black_list1:\n",
    "            if(w in e):\n",
    "                bl.append(e)\n",
    "    return bl\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_length1(list_entities):\n",
    "    \"\"\" Generate blacklist with entities that are length one (letter). \"\"\"\n",
    "    black_l = set()\n",
    "    for e in list_entities:\n",
    "\n",
    "        e_splited = word_tokenize(e)\n",
    "        if(len(e_splited) == 1):\n",
    "            if(len(e_splited[0])==1):\n",
    "                black_l.add(e)\n",
    "\n",
    "    return list(black_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meetings(list_entities):\n",
    "    \"\"\" Generate blacklist with all the entities that contains the name of a meeting. \"\"\"\n",
    "    list_meeting = ['SB','COP','UNFCCC','ADP','AGBM','IPCC','INC','FCCC','CMP','WG','WGI','WGII','WGIII','AWG']\n",
    "    black_list6 = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        for w in e_splited:\n",
    "            if(w in list_meeting):\n",
    "                black_list6.append(e)\n",
    "    return black_list6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(list_entities):\n",
    "    \"\"\" Generate blacklist with entities that are sentences. \"\"\"\n",
    "    regex2 = r'A\\s[A-Z]+'\n",
    "    s = []\n",
    "    for e in list_entities:\n",
    "        s += re.findall(regex2,e)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs(list_entities):\n",
    "    \"\"\" Generate blacklist with specific entities that are verbs or common nouns. \"\"\"\n",
    "    black_list3 = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        for w in e_splited:\n",
    "            if(w.endswith('ING') or w.endswith('TION') or w.endswith('TIONS') or w.endswith('TIVE') or w.endswith('MENTS')or w.endswith('ED')or w.endswith('MENT')):\n",
    "                black_list3.append(e)\n",
    "\n",
    "    return black_list3\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_long_words(list_entities):\n",
    "    \"\"\" Generate blacklist with entities that are too longs. \"\"\"\n",
    "    black_list4 = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        if(len(e_splited)>5):\n",
    "            black_list4.append(e)\n",
    "    return black_list4\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_noun(entities):\n",
    "    \"\"\" Generate blacklist with all the entitites that contain a month. \"\"\"\n",
    "    month = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "    month = [m.upper() for m in month]\n",
    "    blm = []\n",
    "    for e in entities:\n",
    "        for m in month:\n",
    "            if(m in e):\n",
    "                blm.append(e)\n",
    "    return blm\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preposition_start(entities):\n",
    "    \"\"\" Generate blacklist with all the entities that start with a preposition. \"\"\"\n",
    "    patterns = ['AND', 'OF','ON','OTHER','TO']\n",
    "    blacklist = []\n",
    "    for p in patterns:\n",
    "        for e in entities:\n",
    "            e_splited = word_tokenize(e)\n",
    "            if(p == e_splited[0]):\n",
    "                blacklist.append(e)\n",
    "    return blacklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_sentences(entities): \n",
    "    \"\"\" Generate a big blacklist with all the blacklist generated and remove it from entities. \"\"\"\n",
    "    black_list1 = black_list_start(entities)\n",
    "\n",
    "    black_list1 += too_long_words(entities)\n",
    "\n",
    "    black_list1 += common_noun(entities)\n",
    "\n",
    "    black_list1 += patterns_longuer(entities)\n",
    "\n",
    "    black_list1 += sentence(entities)\n",
    "\n",
    "    black_list1 += patterns_smaller(entities)\n",
    "\n",
    "    black_list1 += verbs(entities)\n",
    "\n",
    "    black_list1 += meetings(entities)\n",
    "    \n",
    "    black_list1 += number(entities)\n",
    " \n",
    "    black_list1 += words_length1(entities)\n",
    "\n",
    "    black_list1 += preposition_start(entities)\n",
    "\n",
    "    entities_f = set(entities).difference(set(black_list1))\n",
    "    return list(set(entities_f))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities():\n",
    "    \"\"\" Extract all the entities in uppercase from sentences.txt and write them in the file entities_uppercase.txt. \"\"\"\n",
    "    sentences = read(\"sentences.txt\")\n",
    "    lowercase = 0\n",
    "    entities_b = []\n",
    "    for s in sentences:\n",
    "        list_w = [' '.join(b) for a, b in itertools.groupby(word_tokenize(s.strip()), key=str.isupper) if a]\n",
    "        entities_b +=list_w\n",
    "    write(entities_b, \"entities_c.txt\") \n",
    "    entities = remove_unwanted_sentences(list(set(entities_b)))\n",
    "    write(entities, \"Files/entities.txt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda12cc5b1983ce432e8ce1d0fc698bd97b",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}