{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c1_extract_text\n",
    "## From the file list_meetings.csv can \n",
    "### - extract all sentences for one specific issue or for all issues\n",
    "### - write all sentences for one specific issue or for all issues with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen, Request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import numpy as np\n",
    "import requests\n",
    "import html5lib\n",
    "import urllib.request, urllib.error\n",
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from country_list import countries_for_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract from \"csv_file\" all the html link to be able to extract all the <p> tags\n",
    "def extract_from_csv_list_issues(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    list_pt = []\n",
    "    return list(csv_f)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_all_sentences(texte):\n",
    "    about_text = (texte)\n",
    "    about_doc = nlp(about_text)\n",
    "    sentences = list(about_doc.sents)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(p):\n",
    "    \"\"\" Remove special char to be able to detect easier sentences. \"\"\"\n",
    "    tag0 = re.compile(r'<script.*?</script>')\n",
    "    tag1 = re.compile(r'\\s\\s+')\n",
    "    tag2 = re.compile(r'<.*?>')\n",
    "    tag3 = re.compile(r'\\n\\n+')\n",
    "    tag4 = re.compile(r'&.*?;')\n",
    "    tag5 = re.compile(r'\\\\\\'s')\n",
    "    tag6 = re.compile(\"b\\'.*?HIGHLIGHTS|b\\'.*?IISD\")\n",
    "\n",
    "    s = tag0.sub(' ',p)\n",
    "    s = tag1.sub(' ',s)\n",
    "    s = tag2.sub('',s)\n",
    "    s = tag3.sub('',s)\n",
    "    s = tag4.sub('',s)\n",
    "    s= tag5.sub(\"'s\",s)\n",
    "    s= tag6.sub(\"\",s)\n",
    "\n",
    "\n",
    "    s = s.replace(r'<script.*?</script>','')\n",
    "    s = s.replace(\"\\r\",\" \")\n",
    "    s = s.replace(r'\\x',\" \")\n",
    "    s = s.replace(\"\\n\",\" \")\n",
    "    s = s.replace(\"\\t\",\" \")\n",
    "    s = s.replace(\"\\\\x\",\" \")\n",
    "    s = s.replace(\"\\\\r\",\" \")\n",
    "    s = s.replace(\"\\\\n\",\" \")\n",
    "    s = s.replace(\"\\\\t\",\" \")\n",
    "    s = s.replace(\"\\\\\",\" \")\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra(sentences):\n",
    "    \"\"\" Remove footer of the page with conditions. \"\"\"\n",
    "    k = len(sentences)\n",
    "    black_list = ['IN THE CORRIDORS','THINGS TO LOOK','This issue of','BRIEF ANALYSIS OF']\n",
    "    sentences = [str(p) for p in sentences if not str(p).isupper() and not str(p).isdigit()]\n",
    "    for l in range(len(sentences)):\n",
    "        if(black_list[0] in sentences[l] or black_list[1] in sentences[l] or black_list[2] in sentences[l] or black_list[3] in sentences[l]):\n",
    "            break\n",
    "\n",
    "    return sentences[:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(html_link):\n",
    "    \"\"\" Extract the <p> tag from a specific html link. \"\"\"\n",
    "    r = Request(html_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urlopen(r).read()\n",
    "    ps = remove_special_char(str(page))\n",
    "    sentences_with_extra = detect_all_sentences(ps)\n",
    "    sentences_without_extra= remove_extra(sentences_with_extra)\n",
    "    return sentences_without_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_before(html_link):\n",
    "    \"\"\" Extract <p> tags from link inside \"html_link\" for Issue# <45. \"\"\"\n",
    "    r = Request(html_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page_link = urlopen(r).read()\n",
    "    #page_link = urllib.request.urlopen(html_link).read()\n",
    "    soup_link = BeautifulSoup(page_link)\n",
    "    paragraphes = soup_link.findAll('a',href = re.compile('\\d+'))\n",
    "    liste_sentences = []\n",
    "\n",
    "    for pa in paragraphes:\n",
    "        # doesn't use the link of the main page (all issues)\n",
    "        if(pa['href'] != '1200000e.html'):\n",
    "            html_link = 'https://enb.iisd.org/vol12/'+pa['href']\n",
    "            liste_sentences += extract_sentences(html_link)\n",
    "\n",
    "    return liste_sentences"
   ]
  },
  {
   "source": [
    "### Extract sentences for one issue"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_for_one_issue(number):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags from issue number. \"\"\"\n",
    "    \n",
    "    list_meetings = extract_from_csv_list_issues('Files/list_meetings.csv')\n",
    "    for i in range(len(list_meetings)) :\n",
    "        if(int(list_meetings[i][4])== number):\n",
    "            line = list_meetings[i]\n",
    "            break\n",
    "    \n",
    "    #Extract for 0 < Issue# < 45 \n",
    "    if(number < 45):\n",
    "        list_sentences = extract_html_before(line[6])\n",
    "\n",
    "    #Extract for 66 < Issue# < 775\n",
    "    else:\n",
    "        url = line[6]\n",
    "        list_sentences = []\n",
    "        request = requests.get(url)\n",
    "        if(number != 175 and number != 300 and request.status_code == 200):\n",
    "                list_sentences = extract_sentences(line[6])\n",
    "\n",
    "\n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentences_for_one_issue(number):\n",
    "    \"\"\" Write sentences+number.txt file. \"\"\"\n",
    "    outF = open(\"Text/sentences_issue\"+str(number)+\".txt\", \"w\")\n",
    "    list_sentences = extract_sentences_for_one_issue(number)\n",
    "    for line in list(set(list_sentences)):\n",
    "    # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sentences_for_one_issue(560)"
   ]
  },
  {
   "source": [
    "### Extract sentences of all issues"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_for_all_issues():\n",
    "    \"\"\" Extract from \"csv_file\" all issues on the ENB website \"\"\"\n",
    "    \n",
    "    # Create a list with all issues \n",
    "    list_meetings = extract_from_csv_list_issues('Files/list_meetings.csv')\n",
    "    list_issues = []\n",
    "    for i in range(len(list_meetings)) :\n",
    "        list_issues.append(int(list_meetings[i][4]))\n",
    "\n",
    "    # Extract for each issue the sentences\n",
    "    list_sentences = []\n",
    "    for i in list_issues:\n",
    "        #test to find entities only 100 issues\n",
    "        if(i > 45 and i < 780): \n",
    "            print(i)\n",
    "            list_sentences += extract_sentences_for_one_issue(i)\n",
    "            \n",
    "\n",
    "    return list_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentences_for_all_issues(sentences):\n",
    "    \"\"\" Write sentences+number.txt file. \"\"\"\n",
    "    outF = open(\"Text/all_sentences.txt\", \"w\")\n",
    "    for line in sentences:\n",
    "    # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}