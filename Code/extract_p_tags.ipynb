{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comment at the beginning of the hml and also put the <html> tag in lowercase\n",
    "def clean_page_to_parse(page_string):\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub('<!-- WWW Designer Jeff Anderson janderson@iisd.ca --!>','',str(page_string))\n",
    "    page_string = re.sub(r'<HTML>',r'<html>',str(page_string))\n",
    "    page_string = re.sub(r'</HTML>',r'</html>',str(page_string))\n",
    "\n",
    "    return page_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the <p> tag from a specific html link \n",
    "def extract_p_tags(html_link):\n",
    "    page = urllib.request.urlopen(html_link).read()\n",
    "    page = clean_page_to_parse(page)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    list_tp = soup.find_all('p')\n",
    "    return list(set(list_tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract <p> tags from link inside \"html_link\"  (Issue# <45)\n",
    "def extract_p_tags_45(html_link):\n",
    "    page_link = urllib.request.urlopen(html_link).read()\n",
    "    soup_link = BeautifulSoup(page_link)\n",
    "    paragraphes = soup_link.findAll('a',href = re.compile('\\d+'))\n",
    "    list_tp = []\n",
    "\n",
    "    for pa in paragraphes:\n",
    "        # doesn't use the link of the main page (all issues)\n",
    "        if(pa['href'] != '1200000e.html'):\n",
    "            html_link = 'https://enb.iisd.org/vol12/'+pa['href']\n",
    "            list_tp += extract_p_tags(html_link)\n",
    "\n",
    "    return list_tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract from \"csv_file\" all the html link to be able to extract all the <p> tags\n",
    "def extract_from_csv_p_tags(csv_file):\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    k=0\n",
    "    list_pt = []\n",
    "    for row in list(csv_f)[1:]:\n",
    "\n",
    "        #Extract for 0 < Issue# < 45 \n",
    "        if(int(row[4]) < 45 ):\n",
    "            list_pt += extract_p_tags_45(row[6])\n",
    "    \n",
    "        #Extract for 66 < Issue# < 775\n",
    "        else :\n",
    "            request = requests.get(row[6])\n",
    "            if(request.status_code == 200):\n",
    "                list_pt += extract_p_tags(row[6])\n",
    "\n",
    "    return list_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the sentence by removing special char\n",
    "def clean_tp(sentence):\n",
    "    s = re.sub(\"\\r\\n\\s\\s+\",\" \",sentence)\n",
    "    s = re.sub(\"\\r\\n\",\"\",s)\n",
    "    s = re.sub(\"\\s\\s+\",\" \",s)\n",
    "    s = re.sub(\"\\r\\n\",\"\",s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_meetings = extract_from_csv_p_tags('list_meetings.csv')\n",
    "#list_paragraphes = [clean_tp(str(line)) for line in list_meetings ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate list_p_tags.txt file\n",
    "outF = open(\"list_paragraphes.txt\", \"w\")\n",
    "for line in list_paragraphes:\n",
    "    # write line to output file\n",
    "    outF.write(str(line))\n",
    "    outF.write(\"\\n\")\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}