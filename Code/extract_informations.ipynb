{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the specific page to be able to extract information from it \n",
    "\n",
    "page = urllib.request.urlopen('https://enb.iisd.org/enb/vol12/').read()\n",
    "soup = BeautifulSoup(page)\n",
    "\n",
    "#Global Variable\n",
    "\n",
    "month = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts a date from a given string by returning a tuple of int (day,month,year)\n",
    "\n",
    "def extract_date(sdate):\n",
    "    if(extract_number(sdate) == None ):\n",
    "        return sdate\n",
    "    \n",
    "    m = re.findall('\\d{4}|\\d{2}|January|February|March|April|May|June|July|August|September|October|November|December|\\d{1}',sdate)\n",
    "\n",
    "    if(len(m)==0):\n",
    "        d=0\n",
    "    if(len(m)==5):\n",
    "        d = (int(m[0]),month[m[1]],int(m[len(m)-1]))\n",
    "    if(len(m)==4):\n",
    "        d = (int(m[0]),month[m[2]],int(m[len(m)-1]))\n",
    "    if(len(m)==3):\n",
    "        if(m[0] in month.keys()):\n",
    "            d=(m[1],month[m[0]],m[2])\n",
    "        else :\n",
    "            d = (m[0],month[m[1]],m[2])\n",
    "    \n",
    "    d_str = str(d[2])+\"-\"+str(d[1])+\"-\"+str(d[0])\n",
    "    \n",
    "    return d_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts digit from a given string and return an int \n",
    "\n",
    "def extract_number(sname):\n",
    "    \n",
    "    for i in sname.split():\n",
    "\n",
    "        if i.isdigit():\n",
    "            return int(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that helps to compose and order the list. Returns an ordered, by date, list who contains all the COPs with their attributes\n",
    "\n",
    "def compute_list(list_string):\n",
    "    list_cop = []\n",
    "\n",
    "    for s in list_string:\n",
    "        l = s.split('|')\n",
    "        list_cop.append((extract_number(l[0]),extract_date(l[1]),l[2]))\n",
    "        \n",
    "    list_cop.sort(key=lambda a: a[0], reverse=False)\n",
    "\n",
    "    return list_cop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract all the issues for each COPs and return a list in which for each COPs we have the issues corresponding with their number  date and html link \n",
    "\n",
    "def extract_details_meetings(soup,meeting_type):\n",
    "    \n",
    "    detail_meetings = []\n",
    "    meeting_num = 1\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        for col in row.find_all('td'):\n",
    "        \n",
    "            #Detect a new COP \n",
    "            if(\"Issue\" in col.string):\n",
    "                \n",
    "                a = row.find_previous_sibling('tr')\n",
    "                b= a.find_next('th')\n",
    "                detail = []\n",
    "                # Variable to help to detect the first issue \n",
    "                issue_start = 0\n",
    "                \n",
    "                if(\"</h3>\"+meeting_type in str(b) and not(\"BIS\" in str(b))):\n",
    "                    date_td = col.find_next_sibling('td')\n",
    "                    \n",
    "                    while( \"<a name=\" not in str(date_td.find_next('tr'))):              \n",
    "                \n",
    "                        #extract issue number\n",
    "                        issue = int(re.findall('\\d+',col.string)[0])\n",
    "                        \n",
    "                        # define the issue type\n",
    "                        if(issue - issue_start>1):\n",
    "                            issue_type = 'First'\n",
    "                        else:\n",
    "                            issue_type = 'Issue'\n",
    "                        \n",
    "                        pdf_td = date_td.find_next_sibling('td')\n",
    "                        \n",
    "                        # extract html link\n",
    "                        html_td = pdf_td.find_next_sibling('td')\n",
    "                        html = 'https://enb.iisd.org'+html_td.find('a',href=True)['href']\n",
    "                    \n",
    "                        # extract date\n",
    "                        s = date_td.string\n",
    "                        date = extract_date(s)    \n",
    "                        \n",
    "                        #Check if at the end of the webpage and return the final \n",
    "                        #list otherwise continue to find new issues\n",
    "                        if(pdf_td.find_next('tr') == None):\n",
    "                            break\n",
    "                        else:\n",
    "                            col = pdf_td.find_next('tr').find_next('td')\n",
    "                            date_td = col.find_next_sibling('td')\n",
    "                            issue_start = issue\n",
    "                       \n",
    "                        # add the issue into the list\n",
    "                        detail.append((issue,date,html,issue_type))\n",
    "                    \n",
    "                    # Handle case when we are at the end of the COP and we have the summary\n",
    "                    if( \"<a name=\" in str(date_td.find_next('tr'))): \n",
    "                    #extract issue number\n",
    "                        issue = int(re.findall('\\d+',col.string)[0])\n",
    "                        \n",
    "                        pdf_td = date_td.find_next_sibling('td')\n",
    "                    #extract html link\n",
    "                        html_td = pdf_td.find_next_sibling('td')\n",
    "                        html = 'https://enb.iisd.org'+html_td.find('a',href=True)['href']\n",
    "                    \n",
    "                    #extract date\n",
    "                        s = date_td.string\n",
    "                        date = extract_date(s)\n",
    "                        detail.append((issue,date,html,'Summary'))\n",
    "                    \n",
    "                    if(pdf_td.find_next('tr') == None):\n",
    "                            detail.append((issue,date,html,'Summary'))\n",
    "                            detail_meetings.append(detail)\n",
    "                            return detail_meetings\n",
    "                    \n",
    "                     \n",
    "                    detail_meetings.append(detail)\n",
    "                    meeting_num = meeting_num +1\n",
    "    return detail_meetings\n",
    "                    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ COP ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the COPs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "\n",
    "def extract_list_cops(soup):\n",
    "    \n",
    "    # find all the different COPs (not named the same way)\n",
    "    \n",
    "    #Case 1 :\"COP\"+\"\\s\"+\"\\d\"+\"\\s\"+\".\"+\"\\s\"  // COP 1-9\n",
    "    list = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"\\d\"+\"\\s\"+\".\"+\"\\s\"))\n",
    "\n",
    "    #Case 2 : \"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"\\d\" // COP 10,23,24,25\n",
    "    list += soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"\\d\"))\n",
    "    \n",
    "    #Case 3 :\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\s\"+\"\\d\"+\".\"+\".\" // COP 11,12,13,14,15,16,21,22\n",
    "    list_2 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\s\"+\"\\d+\"+\"\\s\"))\n",
    "\n",
    "    # Case 4 : \"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\d\"+\"\\s\" //COP 17,18,19\n",
    "    list_3 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\d+\"+\"\\s\"))\n",
    "    \n",
    "    # Clean the lists to have all the same structure\n",
    "    # Clean list_2\n",
    "    for i in range(len(list_2)) :\n",
    "        list_2[i] =  re.sub(\"- CMP\"+\"\\s\"+\".\"+\".\", '', list_2[i])\n",
    "\n",
    "    # Clean list_3\n",
    "    for i in range(len(list_3)) :\n",
    "        list_3[i] =  re.sub(\"- CMP\"+\".\", '', list_3[i])\n",
    "    \n",
    "    #combine all the lists\n",
    "    list += list_2\n",
    "    list += list_3\n",
    "   \n",
    "    return compute_list(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ INC ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the INCs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "\n",
    "def extract_list_incs(soup):\n",
    "    \n",
    "    # find all the different INCs (not named the same way)\n",
    "    # Only one\n",
    "    \n",
    "    #Case 1 :\"INC\"+\"\\s\"+\"\\d+\"+\"\\s\"  // INC 11\n",
    "    list = soup.find_all(string=re.compile(\"INC\"+\"\\s\"+\"\\d+\"+\"\\s\"))\n",
    "\n",
    "    return compute_list(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ SB ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the SBs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "\n",
    "def extract_list_sbs(soup):\n",
    "    \n",
    "    # find all the different INCs (not named the same way)\n",
    "    # Only one \n",
    "    \n",
    "\n",
    "    \n",
    "    #Case 1 :\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\".\"+\"\\s\"+\"\\d+\"\n",
    "    list_1 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\".\"+\"\\s\"+\"\\d+\"))\n",
    "    \n",
    "    #Case 2 : \"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"-\"+\"\\s\"+\"AG\"+\".\"+\".\"+\"\\s\"+\"\\d\"+\"...\\d+\"\n",
    "    list_2 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"-\"+\"\\s\"+\"AG\"+\".\"+\".\"+\"\\s\"+\"\\d\"+\"...\\d+\"))\n",
    "    \n",
    "    #Case 3 : \n",
    "    liste_3 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"-\"+\"\\s\"+\"AG\"+\"..\"+\"\\s\"+\"\\d\"+\"............\\d\"))\n",
    "    \n",
    "    #Case 4 : \n",
    "    list_4 = soup.find_all(string=re.compile(\"SB\"+\"-...\"))\n",
    "    \n",
    "    #Case 5 : \n",
    "    list_5 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"- AWG...\"))\n",
    "    \n",
    "    # Clean list_2\n",
    "    for i in range(len(list_2)) :\n",
    "        list_2[i] =  re.sub(\"- AG\\d+ \\d \", '', list_2[i])\n",
    "    \n",
    "    for i in range(len(list_3)) :\n",
    "        list_3[i] =  re.sub(\"- AGBM \\d . AG....\", '', list_3[i])\n",
    "    \n",
    "    for i in range(len(list_4)) :\n",
    "        list_4[i] =  re.sub(\"-\", ' ', list_4[i])\n",
    "    \n",
    "    for i in range(len(list_5)) :\n",
    "        list_5[i] =  re.sub(\" . AWGs\", '', list_5[i])\n",
    "\n",
    "    list = list_1+list_2+list_3+list_4+list_5\n",
    "\n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ IPCC ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the IPCCs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "\n",
    "def extract_list_ipccs(soup):\n",
    "    \n",
    "    # find all the different IPCCs (not named the same way)\n",
    "    \n",
    "    #Case 1 :\"IPCC-\\d+ . \"\n",
    "    list = soup.find_all(string=re.compile(\"IPCC-\\d+ . \"))\n",
    "\n",
    "    for i in range(len(list)) :\n",
    "        list[i] =  re.sub(\"-\", ' ', list[i])\n",
    "        \n",
    "    return compute_list(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ AGBM ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the AGMBs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "\n",
    "def extract_list_agbms(soup):\n",
    "    \n",
    "    # find all the different AGBMs (not named the same way)    \n",
    "    #Case 1 : \"AGBM \\d+ . \\d+\"\n",
    "    list = soup.find_all(string=re.compile(\"AGBM \\d+ . \\d+\"))\n",
    "        \n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ UNFCCC WS ------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extract the list of all the AGMBs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "# A TERMINER - PAS DE NUMERO DE CONFERENCE MAIS DES LETTRES \n",
    "def extract_list_unfcccs(soup):\n",
    "    \n",
    "    # find all the different AGBMs (not named the same way)    \n",
    "    #Case 1 : \"AGBM \\d+ . \\d+\"\n",
    "    list = soup.find_all(string=re.compile(\"UNFCCC WS.............\"))\n",
    "       \n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_information_meetings(soup,meeting_type,extract_type):\n",
    "    list_meetings = extract_type(soup)\n",
    "    list_meetings_information = extract_details_meetings(soup,meeting_type)\n",
    "    total = []\n",
    "    for i in range(len(list_meetings)):\n",
    "        (number,date,place) = list_meetings[i]\n",
    "        list_meeting = list_meetings_information[i]\n",
    "        \n",
    "        for x in list_meeting:\n",
    "            total.append((meeting_type,number,date,place,x[0],x[1],x[2],x[3]))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
