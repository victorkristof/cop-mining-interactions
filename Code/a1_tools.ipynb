{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize\n",
    "from itertools import combinations  \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dict_interaction():\n",
    "    dict_entities = {}\n",
    "    with open(\"Files/dict_inter.csv\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            dict_entities[row[0]] = row[1].split(', ')\n",
    "    return dict_entities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict():\n",
    "    \"\"\"Create dictionary that can map entities with an id but also map entities from the original to the generated dataset. \"\"\"\n",
    "    DICTIONARY = open_dict_interaction()\n",
    "    n = 0\n",
    "    DICTIONARY_NUM = {}\n",
    "    for k in DICTIONARY:\n",
    "        DICTIONARY_NUM[k] = n \n",
    "        n +=1\n",
    "    NAMES = []\n",
    "    for x in list(DICTIONARY.values()):\n",
    "        if(len(x) == 1):\n",
    "            NAMES.append(x[0])\n",
    "        else:\n",
    "            NAMES += [k for k in x]\n",
    "\n",
    "    return DICTIONARY, DICTIONARY_NUM, NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, country):\n",
    "    \"\"\"Split the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    ENTITIES = [s.replace('\\n','') for s in list(open('Files/entities_interactions.txt'))]\n",
    "    tokens_entities = [l.split(' ') for l in ENTITIES]\n",
    "    tokens_entities.append(['on','behalf','of'])\n",
    "    tokens_entities.append(['on','behalf','of','the'])\n",
    "    tokens_entities.append(['for'])\n",
    "    tokens_entities.append(['for','the'])\n",
    "    tokens_entities.append(['speaking','for'])\n",
    "    tokens_entities.append(['speaking','for','the'])\n",
    "    tokens_entities.append(['US','$'])\n",
    "    tokens_entities.append(['concerns', 'of', 'the'])\n",
    "    tokens_entities.append(['concerns', 'of'])\n",
    "    tokens_entities.append(['spoke','with'])\n",
    "    tokens_entities.append(['for', 'a', 'number','of', 'members' ,'of' ,'the'])\n",
    "    tokens_entities.append(['for', 'several'])\n",
    "    tokens_entities.append(['speaking','on','behalf', 'of', 'the'])  \n",
    "    tokens_entities.append(['supported','by'])\n",
    "    tokens_entities.append(['supported','by','the'])\n",
    "    tokens_entities.append(['opposed','by'])\n",
    "    tokens_entities.append(['opposed','by','the'])\n",
    "    tokens_entities.append(['proposed','by','the'])\n",
    "    tokens_entities.append(['proposed','by'])\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "    \n",
    "    if(type(sentence) == list):\n",
    "        line = sentence[0].replace(\",\",\"\")\n",
    "    else: \n",
    "        line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rSubset(arr, cop): \n",
    "    \"\"\" function that return all the tuples needed for the interactions. \"\"\"\n",
    "    l = list(set(list(itertools.product(arr, arr))))\n",
    "\n",
    "    return [(c1.upper(),c2.upper(),cop) for c1,c2 in l if c1 != c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv_list_issues(csv_file):\n",
    "    \"\"\" Extract from \"csv_file\" all the html link to be able to extract all the <p> tags. \"\"\"\n",
    "    f = open(csv_file)\n",
    "    csv_f = csv.reader(f)\n",
    "    list_pt = []\n",
    "    l =list(csv_f)[1:]\n",
    "    issue = []\n",
    "    for x in l:\n",
    "        issue.append(int(x[4]))\n",
    "    return issue"
   ]
  }
 ]
}