{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "importing Jupyter notebook from extract_p_tags.ipynb\n"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import csv\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "from extract_p_tags import clean_page_to_parse\n",
    "from extract_p_tags import extract_p_tags\n",
    "from extract_p_tags import extract_p_tags_45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_list_meetings():\n",
    "    f = open('list_meetings.csv')\n",
    "    return csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract tuple for each row from Paula's dataset\n",
    "def extract_tuple(line):\n",
    "    l = line.replace('\"',\"\")\n",
    "    l = l.replace('\\n',\"\")\n",
    "    l = l.split('\\t')\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the sentence by removing special char\n",
    "def clean_tp(sentence):\n",
    "    s = re.sub(\"\\r\\n\\s\\s+\",\" \",sentence)\n",
    "    s = re.sub(\"\\r\\n\",\"\",s)\n",
    "    s = re.sub(\"\\s\\s+\",\" \",s)\n",
    "    x = ' '\n",
    "    s = x.join(s.split('\\r\\n'))\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the desired sentences (without sponsors)\n",
    "def extract_from_paragraphes_sentences(list_paragraphes):\n",
    "    list_paragraphes_foot = set()\n",
    "    #Remove footer\n",
    "    for i in range(len(list_paragraphes)):\n",
    "        if('Sustaining Donors' not in list_paragraphes[i] and 'This issue of' not in list_paragraphes[i]):\n",
    "            if(list_paragraphes[i] not in list_paragraphes_foot):\n",
    "                list_paragraphes_foot.add(list_paragraphes[i])\n",
    "    list_paragraphes_foot= list(list_paragraphes_foot)\n",
    "    #Split into sentence from paragraph\n",
    "    splited_sentence = []\n",
    "    for s in list_paragraphes_foot:\n",
    "        splited_sentence += sent_tokenize(s)\n",
    "    return splited_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract complete dataset of Paula, put it into list of tuples\n",
    "def extract_dataset():\n",
    "    list_data = list(open('statements_count.csv','r'))\n",
    "    list_dataset = []\n",
    "    for line in list_data:\n",
    "        list_dataset.append(extract_tuple(line))\n",
    "    return list_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the entities mentioned in Paula's dataset\n",
    "def extract_entities():\n",
    "    list_dataset = extract_dataset()\n",
    "    set_entities = set()\n",
    "    for line in list_dataset:\n",
    "        set_entities.add(line[0])\n",
    "    return set_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of time each entity in list_entities is mentioned in list_sentences\n",
    "def count_occurences(list_sentences, dict_occ, tokenizer, list_entities, meeting):\n",
    "    list_sentences = list(set(list_sentences))\n",
    "    print_s(list_sentences)\n",
    "    for s in list_sentences:\n",
    "        #Split line into words with tokenizer to detetc entity\n",
    "        line = s.replace(\",\",\"\")\n",
    "\n",
    "        line_splited = word_tokenize(line)\n",
    "        tokens = tokenizer.tokenize(line_splited)    \n",
    "        tokens = [clean_tp(token) for token in tokens]\n",
    "        \n",
    "        for entity in list_entities:\n",
    "            #Increment value of intervention of the entity\n",
    "            if(entity in tokens):\n",
    "                dict_occ[entity] += 1                \n",
    "    rows = [(meeting[0],entity,meeting[4],dict_occ[entity],meeting[5]) for entity in dict_occ]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_s(list_p):\n",
    "    outF = open(\"list_p.txt\", \"w\") \n",
    "    for line in list_p:\n",
    "\n",
    "    # write line to output file\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_occurences_issue_ENB():\n",
    "    #List meetings\n",
    "    list_ENBs = [list(open_list_meetings())[2]]\n",
    "    print(list_ENBs)\n",
    "\n",
    "    #tokenizer = MWETokenizer(extract_entities(), separator=' ')\n",
    "    list_entities = list(extract_entities())\n",
    "    tokens_entities = [l.split(' ') for l in list(extract_entities())]\n",
    "    tokenizer = MWETokenizer(tokens_entities, separator=' ')\n",
    "    occurences_meetings = []\n",
    "    k=0\n",
    "    #2 Count number time entities mentioned\n",
    "    for meeting in list_ENBs:\n",
    "        if(int(meeting[4]) < 45):\n",
    "            list_paragraphes = [clean_tp(str(line)) for line in extract_p_tags_45(meeting[6])]\n",
    "\n",
    "        else :\n",
    "            request = requests.get(meeting[6])\n",
    "            if(request.status_code == 200):\n",
    "                list_paragraphes = [clean_tp(str(line)) for line in extract_p_tags(meetng[6])]\n",
    "\n",
    "\n",
    "                    \n",
    "        list_sentences = extract_from_paragraphes_sentences(list_paragraphes)\n",
    "        dict_occurences = dict.fromkeys(extract_entities(), 0)\n",
    "        occurences_meetings += count_occurences(list_sentences, dict_occurences, tokenizer, list_entities, meeting)\n",
    "\n",
    "    \n",
    "    with open('list_occurences.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        #header\n",
    "        writer.writerow(('meeting_type','entity','issue_number','interventions','date'))\n",
    "        writer.writerows(occurences_meetings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['INC', '11', '1995-02-06', ' New York, USA', '2', '1995-02-07', 'https://enb.iisd.org/vol12/1202000e.html', 'Issue']]\n"
    }
   ],
   "source": [
    "extract_occurences_issue_ENB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}