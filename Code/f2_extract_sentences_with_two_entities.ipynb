{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f2_extract_sentences_with_two_entities\n",
    "From the file sentences.txt extract all the sentence who contain at least two entities and write them in the file sentences-with-two-entities.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from c1_extract_paragraphe_issue.ipynb\nimporting Jupyter notebook from c2_extract_sentence_issue.ipynb\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected(list_with_two_entities):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    outF = open(\"sentences-with-two-entities.txt\", \"w\")\n",
    "    for line in list_with_two_entities :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected_issue(list_with_two_entities, number):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    s = \"s2e\"+str(number)\n",
    "    outF = open(s+\".txt\", \"w\")\n",
    "    for line in list_with_two_entities :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_txt_sentences(txt_file):\n",
    "    \"\"\" Open file that contains all the sentences. \"\"\"\n",
    "    list_tp = open(txt_file)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char. \"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "\n",
    "    line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e(list_tp_cleaned):\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "    tokens_countries = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    tokenizer = MWETokenizer(tokens_countries, separator=' ')\n",
    "    sentences_country = set()\n",
    "\n",
    "    for line in list_tp_cleaned:\n",
    "        #split sentence with tokenizer to detect a country \n",
    "        founded = 0    \n",
    "        line_splited = word_tokenize(line)\n",
    "        tokens = tokenizer.tokenize(line_splited)\n",
    "        for country1 in lists_countries: \n",
    "            for country2 in lists_countries:\n",
    "                if(country1 in tokens and country2 in tokens and country1 != country2):\n",
    "                    sentences_country.add(line)\n",
    "                    founded = 1\n",
    "                    break\n",
    "            if(founded):\n",
    "                founded =0\n",
    "                break \n",
    "\n",
    "    return sentences_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue(number):\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(paragraphes)\n",
    "\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2e = []\n",
    "    for s in sentences:\n",
    "    #Split line into words with tokenizer to detetc entity\n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            for e2 in list_entities:\n",
    "                if(set([e1,e2]).issubset(set(tokens)) and e1 != e2):\n",
    "                    sentences_s2e.append(s)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done:\n",
    "                break\n",
    "    print(len(sentences_s2e))\n",
    "    return sentences_s2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Issue  452\n15\n"
    }
   ],
   "source": [
    "number = 452\n",
    "list_s2e = extract_s2e_issue(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_tagged_s2e(list_s2e):\n",
    "    # VBD: Verb, past tense\n",
    "    # NNP: Proper noun, singular Phrase\n",
    "    verbs = []\n",
    "    nnp = []\n",
    "    for s in list_s2e:\n",
    "        tokens = tokenize_sentence(s)\n",
    "        pos_tagged = nltk.pos_tag(tokens)   \n",
    "   \n",
    "    return pos_tagged\n",
    "    \n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('CHINA', 'NNP')\n('VENEZUELA', 'NNP')\n('PAPUA NEW GUINEA', 'NNP')\n('UNITED', 'NNP')\n('ARAB EMIRATES', 'NNP')\n('BRAZIL', 'NNP')\n('NIGERIA', 'NNP')\n('BAHRAIN', 'NNP')\n('Tuvalu', 'NNP')\n('’', 'NNP')\n('Protocol', 'NNP')\n('“', 'NNP')\n('Protocol.', 'NNP')\n('”', 'NNP')\n('Protocol', 'NNP')\n('Article', 'NNP')\n('AWG-KP', 'NNP')\n"
    }
   ],
   "source": [
    "pos_tagged = find_pos_tagged_s2e(list_s2e)\n",
    "\n",
    "verbs = filter(lambda x:x[1]=='NNP',pos_tagged)\n",
    "\n",
    "for x in verbs:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}