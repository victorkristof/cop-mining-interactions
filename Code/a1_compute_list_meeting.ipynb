{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a1_compute-list-meetings\n",
    "File that compute a list for each meeting type and each corresponding issues with their date, place, issue number and HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of the months\n",
    "month = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(sdate):\n",
    "    \"\"\" Extracts a date from a given string by returning a tuple of int (day,month,year). \"\"\"\n",
    "    if(extract_number(sdate) == None ):\n",
    "        return sdate\n",
    "    \n",
    "    m = re.findall('\\d{4}|\\d{2}|January|February|March|April|May|June|July|August|September|October|November|December|\\d{1}',sdate)\n",
    "\n",
    "    if(len(m)==0):\n",
    "        d=0\n",
    "    if(len(m)==5):\n",
    "        d = (int(m[0]),month[m[1]],int(m[len(m)-1]))\n",
    "    if(len(m)==4):\n",
    "        d = (int(m[0]),month[m[2]],int(m[len(m)-1]))\n",
    "    if(len(m)==3):\n",
    "        if(m[0] in month.keys()):\n",
    "            d=(m[1],month[m[0]],m[2])\n",
    "        else :\n",
    "            d = (m[0],month[m[1]],m[2])\n",
    "    \n",
    "    d_str = str(d[2])+\"-\"+str(d[1])+\"-\"+str(d[0])\n",
    "    \n",
    "    return d_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(sname):\n",
    "    \"\"\"\"Extract digit from a given string and return an int.\"\"\"\n",
    "    for i in sname.split():\n",
    "\n",
    "        if i.isdigit():\n",
    "            return int(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_list(list_string):\n",
    "    \"\"\" Help to compose and order the list. Returns an ordered, by date, list who contains all the meetings in list_string with their attributes. \"\"\"\n",
    "    list_cop = []\n",
    "\n",
    "    for s in list_string:\n",
    "        l = s.split('|')\n",
    "        list_cop.append((extract_number(l[0]),extract_date(l[1]),l[2]))\n",
    "        \n",
    "    list_cop.sort(key=lambda a: str(a[0]), reverse=False)\n",
    "\n",
    "    return list_cop\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_details_meetings(soup, meeting_type):\n",
    "    \"\"\" Extract for one meeting_type, all the corresponding issues. For each issue extract the issue number, the date, the html link. \"\"\"\n",
    "    detail_meetings = []\n",
    "    meeting_num = 1\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        for col in row.find_all('td'):\n",
    "        \n",
    "            #Detect a new meeting \n",
    "            if(\"Issue\" in col.string):\n",
    "                \n",
    "                a = row.find_previous_sibling('tr')\n",
    "                b= a.find_next('th')\n",
    "                detail = []\n",
    "                # Variable to help to detect the first issue \n",
    "                issue_start = 0\n",
    "                # and not(\"BIS\" in str(b))\n",
    "                if(\"</h3>\"+meeting_type in str(b)):\n",
    "                    \n",
    "                    date_td = col.find_next_sibling('td')\n",
    "                    \n",
    "                    while( \"<a name=\" not in str(date_td.find_next('tr'))):              \n",
    "                \n",
    "                        #extract issue number\n",
    "                        issue = int(re.findall('\\d+',col.string)[0])\n",
    "                        \n",
    "                        # define the issue type\n",
    "                        if(issue - issue_start>1):\n",
    "                            issue_type = 'First'\n",
    "                        else:\n",
    "                            issue_type = 'Issue'\n",
    "                        \n",
    "                        pdf_td = date_td.find_next_sibling('td')\n",
    "                        \n",
    "                        # extract html link\n",
    "                        html_td = pdf_td.find_next_sibling('td')\n",
    "                        html = 'https://enb.iisd.org'+html_td.find('a',href=True)['href']\n",
    "                    \n",
    "                        # extract date\n",
    "                        s = date_td.string\n",
    "                        date = extract_date(s)    \n",
    "                        \n",
    "                        #Check if at the end of the webpage and return the final \n",
    "                        #list otherwise continue to find new issues\n",
    "                        if(pdf_td.find_next('tr') == None):\n",
    "                            break\n",
    "                        else:\n",
    "                            col = pdf_td.find_next('tr').find_next('td')\n",
    "                            date_td = col.find_next_sibling('td')\n",
    "                            issue_start = issue\n",
    "                       \n",
    "                        # add the issue into the list\n",
    "                        detail.append((issue,date,html,issue_type))\n",
    "                    \n",
    "                    # Handle case when we are at the end of the COP and we have the summary\n",
    "                    if( \"<a name=\" in str(date_td.find_next('tr'))): \n",
    "                    #extract issue number\n",
    "                        issue = int(re.findall('\\d+',col.string)[0])\n",
    "                        \n",
    "                        pdf_td = date_td.find_next_sibling('td')\n",
    "                    #extract html link\n",
    "                        html_td = pdf_td.find_next_sibling('td')\n",
    "                        html = 'https://enb.iisd.org'+html_td.find('a',href=True)['href']\n",
    "                    \n",
    "                    #extract date\n",
    "                        s = date_td.string\n",
    "                        date = extract_date(s)\n",
    "                        detail.append((issue,date,html,'Summary'))\n",
    "                    \n",
    "                    if(pdf_td.find_next('tr') == None):\n",
    "                            detail.append((issue,date,html,'Summary'))\n",
    "                            detail_meetings.append(detail)\n",
    "                            return detail_meetings\n",
    "                    \n",
    "                     \n",
    "                    detail_meetings.append(detail)\n",
    "                    meeting_num = meeting_num +1\n",
    "    return detail_meetings\n",
    "                    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_cops(soup):\n",
    "    \"\"\"Extract the list of all the COPs from a webpage and return a list containing all the COPs with their number, date and place. \"\"\"\n",
    "    \n",
    "    # find all the different COPs (not named the same way)\n",
    "    \n",
    "    #Case 1 :1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "    list = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"\\d\"+\"\\s\"+\".\"+\"\\s\"))\n",
    "\n",
    "    #Case 2 :10, 23, 24, 25\n",
    "    list += soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"\\d\"))\n",
    "    \n",
    "    #Case 3 :11, 12, 13, 14, 15, 16, 20, 21 , 22\n",
    "    list_2 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\s\"+\"\\d+\"+\"\\s\"))\n",
    "\n",
    "    # Case 4 : 17, 18, 19\n",
    "    list_3 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\d+\"+\"\\s\"))\n",
    "\n",
    "    # Case 4 : BIS \n",
    "    list_4 = [soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"BIS\"))[0]]\n",
    "    # Clean the lists to have all the same structure\n",
    "    # Clean list_2\n",
    "    for i in range(len(list_2)) :\n",
    "        list_2[i] =  re.sub(\"- CMP\"+\"\\s\"+\".\"+\".\", '', list_2[i])\n",
    "\n",
    "    # Clean list_3\n",
    "    for i in range(len(list_3)) :\n",
    "        list_3[i] =  re.sub(\"- CMP\"+\".\", '', list_3[i])\n",
    "\n",
    "    # Clean list_4\n",
    "    for i in range(len(list_4)) :\n",
    "        list_4[i] =  re.sub(\" BIS\"+\".2\", '', list_4[i])\n",
    "\n",
    "    #combine all the lists\n",
    "    list += list_2\n",
    "    list += list_3\n",
    "    list += list_4\n",
    "   \n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_incs(soup):\n",
    "    \"\"\" Extract of all the INC meetings from a webpage and return a list containing all of them with their number, date and place \"\"\"\n",
    "    \n",
    "    #Case 1 : 11 \n",
    "    list = soup.find_all(string=re.compile(\"INC\"+\"\\s\"+\"\\d+\"+\"\\s\"))\n",
    "\n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_sbs(soup):\n",
    "    \"\"\"Extract of all the SB meetings from a webpage and return a list containing all of them with their number, date and place.\"\"\"\n",
    "    \n",
    "\n",
    "    #Case 1 : 1, 3, 7, 8, 10, 12, 13, 18, 20 ,22, 24, 26, 28, 30, 34, 36, 38, 40, 42, 44, 46, 48, 48-2, 50\n",
    "    list_1 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\".\"+\"\\s\"+\"\\d+\"))\n",
    "    \n",
    "    #Case 2 : 4, 6\n",
    "    list_2 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"-\"+\"\\s\"+\"AG\"+\".\"+\".\"+\"\\s\"+\"\\d\"+\"...\\d+\"))\n",
    "    \n",
    "    #Case 3 : 5\n",
    "    list_3 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"-\"+\"\\s\"+\"AG\"+\"..\"+\"\\s\"+\"\\d\"+\"............\\d\"))\n",
    "    \n",
    "    #Case 4 : 50 \n",
    "    list_4 = soup.find_all(string=re.compile(\"SB\"+\"-...\"))\n",
    "    \n",
    "    #Case 5 : 32\n",
    "    list_5 = soup.find_all(string=re.compile(\"SB\"+\"\\s\"+\"\\d+\"+\"\\s\"+\"- AWG...\"))\n",
    "    \n",
    "    # Clean list_2\n",
    "    for i in range(len(list_2)) :\n",
    "        list_2[i] =  re.sub(\"- AG\\d+ \\d \", '', list_2[i])\n",
    "    \n",
    "    for i in range(len(list_3)) :\n",
    "        list_3[i] =  re.sub(\"- AGBM \\d . AG....\", '', list_3[i])\n",
    "    \n",
    "    for i in range(len(list_4)) :\n",
    "        list_4[i] =  re.sub(\"-\", ' ', list_4[i])\n",
    "    \n",
    "    for i in range(len(list_5)) :\n",
    "        list_5[i] =  re.sub(\" . AWGs\", '', list_5[i])\n",
    "\n",
    "    list = list_1+list_2+list_3+list_4+list_5\n",
    "\n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_ipccs(soup):\n",
    "    \"\"\" Extract of all the IPCC meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    \n",
    "    #Case 1 : 17, 18, 22, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52\n",
    "    list = soup.find_all(string=re.compile(\"IPCC-\\d+ . \"))\n",
    "\n",
    "    for i in range(len(list)) :\n",
    "        list[i] =  re.sub(\"-\", ' ', list[i])\n",
    "        \n",
    "    return compute_list(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_agbms(soup):\n",
    "    \"\"\" Extract of all the AGMB meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "  \n",
    "    #Case 1 : 1, 2, 3, 6, 7\n",
    "    list = soup.find_all(string=re.compile(\"AGBM \\d+ . \\d+\"))\n",
    "        \n",
    "    return compute_list(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNFCCC WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_unfcccs(soup):\n",
    "    \"\"\" Extract of all the UNFCC WS meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    \n",
    "    list = soup.find_all(string=re.compile(\"UNFCCC WS.*?\"))\n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\")\n",
    "        sep[0] = re.sub(\"UNFCCC WS\", '', sep[0])  \n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_adps(soup):\n",
    "    \"\"\" Extract of all the ADP meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    #Case 1 : 2, 2-10, 2-11, 2-4, 2-6, 2-8\n",
    "    list = soup.find_all(string=re.compile(\"ADP \\d......\"))\n",
    "    \n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\")\n",
    "        sep[0] = re.sub(\"ADP\", '', sep[0])  \n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWGS CCWG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_awgs_t1(soup):\n",
    "    \"\"\" Extract of all the AWGs CCWG meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    \n",
    "    #Case 1 : 1, 7, 9, 11, 12, 14, 16, 17i\n",
    "    list = soup.find_all(string=re.compile(\"AWGs CCWG\\d...\"))\n",
    "    \n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\") \n",
    "        sep[0] = re.sub(\"AWGs CCWG\", '', sep[0]) \n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWGS RCCWG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_awgs_t2(soup):\n",
    "     \"\"\" Extract of all the AWGs RCCWG meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    \n",
    "    #Case 1 : 7\n",
    "    list = soup.find_all(string=re.compile(\"AWGs RCCWG\\d ...\"))\n",
    "    \n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\") \n",
    "        sep[0] = re.sub(\"AWGs RCCWG\", '', sep[0])\n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWGLCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_awgs_t3(soup):\n",
    "    \"\"\" Extract of all the AWGLCA meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "\n",
    "    #Case 1 : 1, 2, 5\n",
    "    list = soup.find_all(string=re.compile(\"AWGLCA \\d ...\"))\n",
    "    \n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\") \n",
    "        sep[0] = re.sub(\"AWGLCA\", '', sep[0])\n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_list_awgs_t4(soup):\n",
    "    \"\"\" Extract of all the AWGLCA meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    #Case 1 : 4   \n",
    "    list = soup.find_all(string=re.compile(\"AWG-\\d...\"))\n",
    "\n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\") \n",
    "        sep[0] = re.sub(\"AWG-\", '', sep[0])\n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech-Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_awgs_t4(soup):\n",
    "    \"\"\" Extract of all the Tech-Work meetings from a webpage and return a list containing all of them with their number, date and place. \"\"\"\n",
    "    #Case 1 :    \n",
    "    list = soup.find_all(string=re.compile(\"Tech-Work |\"))\n",
    "\n",
    "    separated = []\n",
    "    for l in list:\n",
    "        sep = l.split(\"|\") \n",
    "        sep[0] = re.sub(\"Tech-Work\", '', sep[0])\n",
    "        sep[1] = extract_date(sep[1])\n",
    "        separated.append(sep)\n",
    "\n",
    "    return separated"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}