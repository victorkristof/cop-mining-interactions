{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h2_ENB_gen\n",
    "Find all interactions in the original dataset for one issue or a range of issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from c1_extract_text.ipynb\n",
      "importing Jupyter notebook from a1_tools.ipynb\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tatianacogne/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "import itertools\n",
    "import collections\n",
    "from collections import Counter\n",
    "import import_ipynb\n",
    "import c1_extract_text as c1 \n",
    "import a1_tools as tools\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY, DICTIONARY_NUM, NAMES = tools.create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = [s.replace('\\n','') for s in list(open('Files/entities_interactions.txt'))]\n",
    "SUPPORTS_LINKS = ['with','and','for the','on behalf of the','supported by','speaking for the','for several']\n",
    "OPPOSITION_LINKS= ['opposed by','while','opposed by the']\n",
    "LIST_TAGS = ['IN', 'CC', 'NN', 'NNP', 'JJ','NNPS','MD','VBP','VB','VBZ','VBD','RB','VBN','PRP', 'NNS']\n",
    "PARTY_GROUPINGS = sorted(set([s.replace('\\n','').upper() for s in list(open('Files/party_groupings.txt'))] + [s.replace('\\n','').title() for s in list(open('Files/party_groupings.txt'))] + [s.replace('\\n','') for s in list(open('Files/party_groupings.txt'))]))\n",
    "PARTIES = sorted(set(ENTITIES).difference(set(PARTY_GROUPINGS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue_number(issue_number):\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "    #List sentences\n",
    "    sentences = c1.extract_sentences_for_one_issue(issue_number)\n",
    "    \n",
    "    list_entities = ENTITIES\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2 = []\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        tokens = tools.tokenize_sentence(s,False)\n",
    "\n",
    "        if(len(set(tokens).intersection(set(ENTITIES)))> 1):\n",
    "            sentences_s2.append(s)\n",
    "\n",
    "    return sentences_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_tagged_s2e(list_s2e):\n",
    "    \"\"\" Use NLTK to pos_tag all the sentences from list_s2e and return a list of all the sentences pos_tagged\"\"\"\n",
    "    pos_tagged = []\n",
    "    for s in list_s2e:\n",
    "        s = s.replace('\\\\t','')\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "        tokens = tools.tokenize_sentence(s, False)\n",
    "        pos_tagged.append(nltk.pos_tag(tokens))  \n",
    "   \n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(pos_tagged, list_tags):  \n",
    "    \"\"\" Find all the pattern in list_tags needed in sentences pos_tagged. \"\"\"\n",
    "    groups = [x[0] for x in pos_tagged[0] if x[1] in list_tags]\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1g(groups): \n",
    "    \"\"\" Find all the entities in the groups. Return a list of entities\"\"\"\n",
    "    groups = [g.replace(',','') for g in groups]\n",
    "    entities = set(groups).intersection(set(ENTITIES))\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2g(groups, opp):\n",
    "    \"\"\" Find all the entities for each groups. Return two lists of entities\"\"\"\n",
    "\n",
    "    index = groups.index(opp)\n",
    "\n",
    "    g1 = find_1g(groups[:index])\n",
    "    g2 = find_1g(groups[index +1:])\n",
    "\n",
    "    return g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_groups_cooperations(groups):\n",
    "    \"\"\" Return one or two groups with only entities and the original sentence. Return a list with one or two list\"\"\"\n",
    "\n",
    "    # Case 1 : Opposition between two groups\n",
    "    if(set(OPPOSITION_LINKS).intersection(set(groups)) != set()):\n",
    "        opp = list(set(OPPOSITION_LINKS).intersection(set(groups)))[0]\n",
    "        \n",
    "        g1, g2 = find_2g(groups, opp)\n",
    "        return [g1, g2], [opp]\n",
    "\n",
    "    # Case 2 : Only support\n",
    "    else:\n",
    "        g1 = find_1g(groups)\n",
    "        return [g1], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coalitions(groups, sentence, opposition_index):\n",
    "    \"\"\" Remove all the parties in groups that speak for a coalition, return list group updated if the case. \"\"\"\n",
    "    group_updates = []\n",
    "\n",
    "    new_tokens = []\n",
    "\n",
    "    for group in groups: \n",
    "        set_group = set(group)\n",
    "\n",
    "        truples_c = []\n",
    "        token =tools.tokenize_sentence(sentence,False)\n",
    "\n",
    "        links = ['for the','for several','on behalf of the','speaking for the','on behalf of','for','speaking for','for a number of members of the', 'speaking on behalf of the']\n",
    "\n",
    "        if(set(token).intersection(set(links)) != set()):\n",
    "\n",
    "            for i in range(len(token)-2):\n",
    "\n",
    "                if(token[i] in group and token[i+1] in links and token[i+2] in PARTY_GROUPINGS):\n",
    "\n",
    "                    set_group.remove(token[i])\n",
    "\n",
    "                    if(len(opposition_index) !=0):\n",
    "                        s = sentence[:opposition_index[0]]\n",
    "                        v = sentence[opposition_index[0]:]\n",
    "                        s = s.replace(token[i+1],'').replace(token[i],'')\n",
    "                        u = ' '\n",
    "                        sentence = u.join([s,v])\n",
    "                    else:\n",
    "                        sentence = sentence.replace(token[i+1],'').replace(token[i],'')\n",
    "                        \n",
    "\n",
    "        group_updates.append(list(set_group))\n",
    "        \n",
    "    return group_updates, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_concern_of(groups, sentence):\n",
    "    \"\"\" Filter that remove all entities when they are mentioned with the pattern from or concern of. \"\"\"\n",
    "    group_updates = []\n",
    "\n",
    "    for group in groups: \n",
    "        truples_c = []\n",
    "        token = tools.tokenize_sentence(sentence,False)\n",
    "\n",
    "        links = ['from','from the','concerns of the','concern of']\n",
    "\n",
    "\n",
    "        if(set(token).intersection(set(links)) != set()):\n",
    "            \n",
    "            for i in range(len(token)-2):\n",
    "                if((token[i+1] in PARTIES or token[i+1] in PARTY_GROUPINGS) and token[i] in links):\n",
    "                    if(token[i+2] in PARTIES or token[i+2] in PARTY_GROUPINGS):\n",
    "                        \n",
    "                        group = [g for g in group if g != token[i+2]]\n",
    "                        sentence = sentence.replace(token[i+2],'')\n",
    "                    group = [g for g in group if g != token[i+1]]\n",
    "\n",
    "                    sentence = sentence.replace(token[i+1],'')\n",
    "\n",
    "        group_updates.append(group)\n",
    "        \n",
    "    \n",
    "    return group_updates, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behalf_of(sentence, group_cooperation, link):\n",
    "    \"\"\"Function that find all the interaction of type \"behalf\". \"\"\"\n",
    "    token =tools.tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "    for x in countries_B:\n",
    "\n",
    "        tuples.append((list(country_A)[0].upper(),x,['behalf','cooperation']))\n",
    "        \n",
    "    tuples += tools.rSubset(list(countries_B),['agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supported_by(sentence, group_cooperation, link ):\n",
    "    \"\"\"Function that find all the interaction of type \"support\". \"\"\"\n",
    "    token = tools.tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "\n",
    "    for x in countries_B:\n",
    "        \n",
    "        tuples.append((x,list(country_A)[0].upper(),['support','cooperation']))\n",
    "        \n",
    "    tuples += tools.rSubset(list(countries_B),['agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(link, sentence, group_cooperation):\n",
    "    \"\"\"Verify that there is entities in both sides of the link\"\"\"\n",
    "    token =tools.tokenize_sentence(sentence, True)\n",
    "\n",
    "    index = token.index(list(link)[0])\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    country_B = set(token[index:]).intersection(set(group_cooperation))\n",
    "    return country_A != set() and country_B != set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coop(sentence, group_cooperation):\n",
    "    \"\"\"Function that find all the interaction of type \"cooperation\" and classify them. \"\"\"\n",
    "    \n",
    "    token =tools.tokenize_sentence(sentence, True)\n",
    "\n",
    "    behalf = ['speaking for','on behalf of','speaking for the ','on behalf of the']\n",
    "    behalf = set(token).intersection(set(behalf))\n",
    "\n",
    "    support = ['supported by','supported by the']\n",
    "    support = set(token).intersection(set(support))\n",
    "\n",
    "\n",
    "    tuples = []\n",
    "    cooperation = []\n",
    "    if(behalf != set()and check_link(behalf, sentence, group_cooperation)):\n",
    "        return behalf_of(sentence, group_cooperation, behalf)\n",
    "    else: \n",
    "        if(support != set() and  check_link(support, sentence, group_cooperation)):\n",
    "            return supported_by(sentence, group_cooperation, support)\n",
    "\n",
    "        else :\n",
    "            return tools.rSubset(group_cooperation,['agreement','cooperation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposed_by(sentence, group_cooperation, link):\n",
    "    \"\"\" Function that find all the interactions of type \"opposition\" .\"\"\"\n",
    "    token =tools.tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = group_cooperation[0]\n",
    "    countries_B = group_cooperation[1]\n",
    "\n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "\n",
    "    tuples = [(c2.upper(),c1.upper(),['opposition']) for c1,c2 in tuples] \n",
    "\n",
    "\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence.split(list(link)[0])\n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop([splited[0]], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop([splited[1]], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticized_by(sentence, group_cooperation, link):\n",
    "    \"\"\" Function that find all the interactions of type \"criticism\" .\"\"\"\n",
    "    token =tools.tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = list(set(group_cooperation[0]).intersection(set(token[:index])))\n",
    "    countries_B = list(set(group_cooperation[0]).intersection(set(token[index+1:])))\n",
    "    \n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "    tuples = [(c1.upper(),c2.upper(),['criticism']) for c1,c2 in tuples]\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence[0].split(list(link)[0])\n",
    "    \n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop(splited[0], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop(splited[1], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_double_s(sentences):\n",
    "    \"\"\" Filter that try to find if one sentence contain two entities but they are not related. \"\"\"\n",
    "    tags_wanted = ['VBD','MD']\n",
    "    words_wanted = ['and']\n",
    "    s2 = []\n",
    "    sentences_filtered = []\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    set_sentences = set(sentences)\n",
    "    for s in sentences:\n",
    "        pos_tagged = find_pos_tagged_s2e([s])\n",
    "\n",
    "        \n",
    "        \n",
    "        filtered = [x for x in pos_tagged[0] if x[0] in ENTITIES or x[0] in words_wanted or x[1] in tags_wanted]\n",
    "        filtered_only_VBD = [x for x in pos_tagged[0] if x[0] in PARTIES or x[0] in words_wanted or x[1]== 'VBD']\n",
    "\n",
    "\n",
    "        if(len(filtered_only_VBD)>= 5):\n",
    "            for i in range(len(filtered_only_VBD)-4):\n",
    "                if( filtered_only_VBD[i][0] in ENTITIES and filtered_only_VBD[i+1][1]=='VBD' and filtered_only_VBD[i+2][0] == 'and' and  filtered_only_VBD[i+3][0] in PARTIES and  filtered_only_VBD[i+4][1]in tags_wanted and s in set_sentences):\n",
    "\n",
    "                    s_to_filter.append((s,filtered_only_VBD[i+1]))\n",
    "                    set_sentences.remove(s)\n",
    "\n",
    "        if(len(filtered)>= 5):\n",
    "            for i in range(len(filtered)-4):\n",
    "                if( filtered[i][0] in ENTITIES and filtered[i+1][1] in tags_wanted and filtered[i+2][0] == 'and' and  filtered[i+3][0] in ENTITIES and  filtered[i+4][1]in tags_wanted and s in set_sentences):\n",
    "\n",
    "                    s_to_filter.append((s,filtered[i+1]))\n",
    "                    set_sentences.remove(s)\n",
    "\n",
    "    sentences = list(set_sentences) \n",
    "    for s in s_to_filter:\n",
    "\n",
    "        index = s[0].index(s[1][0])\n",
    "        s1 = s[0][:index]\n",
    "        s2 = s[0][index+1:]\n",
    "        sentences.append(s1)\n",
    "        sentences.append(s2)\n",
    "    \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_on_99s_from_programme(sentences):\n",
    "    \"\"\" Filter that remove entities that are related to some patterns that are not interactions. \"\"\"\n",
    "    set_sentences = set(sentences)\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    note = ['92s','Programme','proposed by the','proposed by']\n",
    "    for s in sentences:\n",
    "\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "\n",
    "      \n",
    "        for i in range(len(pos_tagged)-1):\n",
    "\n",
    "            if(pos_tagged[i][0] == 'on' and pos_tagged[i+1][0] in ENTITIES and s in set_sentences):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i+1][0]))\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "            if((pos_tagged[i+1][0] == '92s' or pos_tagged[i+1][0] == 'Programme') and pos_tagged[i][0] in ENTITIES and s in set_sentences):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0]))\n",
    "                set_sentences.remove(s)\n",
    "            \n",
    "            if((pos_tagged[i][0] == 'proposed by the' or pos_tagged[i+1][0] == 'proposed by') and pos_tagged[i+1][0] in ENTITIES and s in set_sentences):\n",
    "  \n",
    "                s_to_filter.append((s.replace (pos_tagged[i+1][0],''),pos_tagged[i+1][0]))\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "\n",
    "    x = ' '\n",
    "    for s in s_to_filter:\n",
    "        token = tools.tokenize_sentence(s[0],False)\n",
    "        tokens = []\n",
    "        for i in range(len(token)-1):\n",
    "            if(token[i] not in note and  not (token[i+1]==s[1])):\n",
    "                tokens.append(token[i])\n",
    "\n",
    "        #s_f = s[0].replace(s[1],'')\n",
    "        s_f = x.join(tokens)\n",
    "        s_filtered.append(s_f)\n",
    "\n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "    \n",
    "    return list(set(s_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doubles(sentence, group_cooperation):\n",
    "    \"\"\" Filter that find if one entity is mentioned twice in two different manner and we should not count and interaction between them. \"\"\"\n",
    "    gc_new = []\n",
    "    \n",
    "    for g in group_cooperation:\n",
    "\n",
    "        g_upper = [e.upper() for e in g]\n",
    "        g_title = [e.title() for e in g]\n",
    "        if(len(set(g_upper)) != len(g)):\n",
    "            gc_new.append(list(set(g).difference(set(g_title))))\n",
    "            removed = list(set(g).intersection(set(g_title)))\n",
    "            for r in removed:\n",
    "                sentence = sentence.replace(r,'')\n",
    "        else:\n",
    "            gc_new.append(g)\n",
    "    return gc_new, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inversions(sentences):\n",
    "    \"\"\" Filter tht try to detect sentence that have been inverted (verb - entity - entity) and change it to be able to detect the interaction. \"\"\"\n",
    "    set_sentences = set(sentences)\n",
    "\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    verbs_int =['Supported','Opposed']\n",
    "    for s in sentences:\n",
    "        # Inversion by starting by the verb\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "        words = [s[0] for s in pos_tagged]\n",
    "\n",
    "\n",
    "        for i in range(len(pos_tagged)-5):\n",
    "            # verb_int + entities (no the) + entites (not the)\n",
    "            if(words[i] in verbs_int and words[i+1] == 'by' and words[i+2] in ENTITIES and words[i+3] in ENTITIES):\n",
    "                supporters = words[i+3]\n",
    "                x = ' '\n",
    "                reverse_s = words[i+2] + \" \" + words[i].lower() + \" by\" + ' ' + supporters\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    "            \n",
    "            # verb_int + the +entities + entites (not the)\n",
    "            if(words[i] in verbs_int and words[i+1] == 'by' and words[i+2] == 'the' and words[i+3] in ENTITIES and words[i+4] in ENTITIES):\n",
    "                supporters = words[i+4]\n",
    "                x = ' '\n",
    "                reverse_s = words[i+3] + \" \" + words[i].lower() + \" by\" + ' ' + supporters\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "            # verb_int + entities (not the) + the + entites \n",
    "            if(words[i] in verbs_int and words[i+1] == 'by' and words[i+2] in ENTITIES and words[i+3] == 'the' and words[i+4] in ENTITIES):\n",
    "                supporters = words[i+4]\n",
    "                x = ' '\n",
    "                reverse_s = words[i+2] + \" \" + words[i].lower() + \" by\" + ' ' + supporters\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    "            \n",
    "            # verb_int + the + entities + the + entites \n",
    "            if(words[i] in verbs_int and words[i+1] == 'by' and words[i+2] == 'the' and words[i+3] in ENTITIES and words[i+4] == 'the' and words[i+5] in ENTITIES):\n",
    "                supporters = words[i+5]\n",
    "                x = ' '\n",
    "                reverse_s = words[i+3] + \" \" + words[i].lower() + \" by\" + ' ' + supporters\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    "        \n",
    "        # Inversion in the middle of the sentence\n",
    "        for i in range(len(words)-3):\n",
    "\n",
    "            if(words[i] == 'supported' and words[i+1]== 'the' and words[i+2] in ENTITIES):\n",
    "                supporters = list(set(ENTITIES).intersection(set(list(words)[:i])))\n",
    "                x = ' '\n",
    "                reverse_s = words[i+2] + \" \" + 'supported by' + ' ' + x.join(w for w in supporters)\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    "            \n",
    "            if(words[i] == 'supported' and words[i+1] in ENTITIES):\n",
    "                supporters = list(set(ENTITIES).intersection(set(list(words)[:i])))\n",
    "                x = ' '\n",
    "                reverse_s = words[i+1] + \" \" + 'supported by' + ' ' + x.join(w for w in supporters)\n",
    "                s_filtered.append(reverse_s)\n",
    "                set_sentences.remove(s)\n",
    " \n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "\n",
    "    return list(set(s_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cooperation(sentence, entities):\n",
    "    index = []\n",
    "    words = tools.tokenize_sentence(sentence, True)\n",
    "\n",
    "    for a in entities:\n",
    "        index.append(words.index(a))\n",
    "    index.sort()\n",
    "    return index[1]== 1 + index[0] or index[1] == index[0] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(group_cooperation):\n",
    "    \"\"\" Function that return if there is or not an interaction. \"\"\"\n",
    "    return len(group_cooperation) == 2 or len(group_cooperation[0])>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_representent(pos_tagged):\n",
    "    \"\"\" Remove all the entities mentioned when it is related to a presentant and so not a interaction. \"\"\"\n",
    "    entities_repr = []\n",
    "\n",
    "    for i in range(len(pos_tagged)-2):\n",
    "        if(pos_tagged[i][0] in ENTITIES and pos_tagged[i+1][0] == 'for' and pos_tagged[i+2][0] in PARTY_GROUPINGS):\n",
    "            entities_repr.append(pos_tagged[i])\n",
    "            entities_repr.append(pos_tagged[i+1])\n",
    "        if(pos_tagged[i][0] in ENTITIES and pos_tagged[i+1][0] == 'for the' and pos_tagged[i+2][0] in PARTY_GROUPINGS):\n",
    "            entities_repr.append(pos_tagged[i])\n",
    "            entities_repr.append(pos_tagged[i+1])\n",
    "\n",
    "    y = [[g for g in pos_tagged if g not in entities_repr]]\n",
    "    x = ' '\n",
    "    sentences_updated = x.join(word[0] for word in y[0])\n",
    "\n",
    "    return y, sentences_updated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_line(issue_number, x):\n",
    "    \"\"\" Function that help to whrite a line wit all the interactions. \"\"\"\n",
    "    \"\"\"  'behalf'\t 'support'\t 'spokewith'\t 'agreement'\t 'delay'\t 'opposition'\t 'criticism'\t 'cooperation'\"\"\"\n",
    "    dict_interactions = {'behalf' : 0, 'support' : 0, 'agreement':0 , 'opposition':0,'criticism' :0, 'cooperation':0}\n",
    "    for int in x[2]:\n",
    "        dict_interactions[int] = 1\n",
    "    values = list(dict_interactions.values())\n",
    "\n",
    "    v = []\n",
    "    v.append(issue_number)\n",
    "    v.append(x[0])\n",
    "    v.append(x[1])\n",
    "    v += values\n",
    "    v.append(x[3])\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    \"\"\" Find the key of one value in the dicitonary of the entities. \"\"\"\n",
    "    for key, value in DICTIONARY.items(): \n",
    "       \n",
    "        if (len(value) == 1 and val == value[0]): \n",
    "             key_country = key \n",
    "\n",
    "        if(len(value) > 1 and val in value):\n",
    "            key_country = key\n",
    "    if key_country not in list(DICTIONARY_NUM.keys()):\n",
    "        print(val)\n",
    "    return DICTIONARY_NUM[key_country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id(interactions_):\n",
    "    \"\"\" Match for each country its id. \"\"\"\n",
    "    tup = []\n",
    "    \n",
    "    for i in interactions_:\n",
    "\n",
    "        c_a = i[1]\n",
    "        c_b = i[2]\n",
    "\n",
    "        id_cb = 9999\n",
    "        id_ca = 9999\n",
    "    \n",
    "        if(c_a in NAMES):\n",
    "\n",
    "            id_ca = get_key(c_a)\n",
    "\n",
    "        if(c_a.upper() in NAMES):\n",
    "\n",
    "            id_ca = get_key(c_a.upper())\n",
    "\n",
    "        if(c_b in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b.upper())\n",
    "\n",
    "        if(c_b.upper() in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b.upper())\n",
    "\n",
    "        if(id_ca == 9999 ):\n",
    "            print('not added c_a :', repr(c_a))\n",
    "        \n",
    "        if(id_cb == 9999 ):\n",
    "            print('not added c_b :', repr(c_b))\n",
    "        i.append(id_ca)\n",
    "        i.append(id_cb)\n",
    "        \n",
    "\n",
    "    return interactions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cooperations, issue_number):\n",
    "    \"\"\" Combine all the information to create the dataframe. \"\"\"\n",
    "    cooperations = [i for i in cooperations if len(i) == 12]\n",
    "    if(len(cooperations)>0):\n",
    "        ca = [x[1].upper() for x in cooperations]\n",
    "        cb = [x[2].upper() for x in cooperations]\n",
    "        id_ca = [x[10] for x in cooperations]\n",
    "        id_cb = [x[11] for x in cooperations]\n",
    "        behalf = [x[3] for x in cooperations]\n",
    "        support = [x[4] for x in cooperations]\n",
    "        agreement =[x[5] for x in cooperations]\n",
    "        opposition =[x[6] for x in cooperations]\n",
    "        criticism =[x[7] for x in cooperations]\n",
    "        cooperation =[x[8] for x in cooperations]\n",
    "        sentences = [x[9] for x in cooperations]\n",
    "        dict_issue = {'type': 'generated','issue': cooperations[0][0],'id_ca':id_ca,'id_cb':id_cb,'Country A':ca, 'Country B': cb, 'behalf':behalf,'support':support,'agreement':agreement,'opposition':opposition,'criticism':criticism,'cooperation':cooperation,'sentences': sentences}\n",
    "    else:\n",
    "        dict_issue = {'type': 'generated','issue': issue_number,'id_ca':1111,'id_cb':1111, 'behalf':[],'support':[],'agreement':[],'opposition':[],'criticism':[],'cooperation':[],'sentences':[]}\n",
    "\n",
    "    df = pd.DataFrame(dict_issue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(issue_number):\n",
    "\n",
    "    \"\"\" Function that combine all the filters to find all the interaction for one specific issue. \"\"\"\n",
    "    \n",
    "    sentences = extract_s2e_issue_number(issue_number)\n",
    "    #sentences = ['']\n",
    "\n",
    "\n",
    "    # Filters to remove to pre-procss sentences to be able to detect if there is an interaction or not\n",
    "\n",
    "    sentences = remove_on_99s_from_programme(sentences)\n",
    "    sentences = remove_double_s(sentences)\n",
    "    sentences = find_inversions(sentences)\n",
    "\n",
    "\n",
    "    interactions_ = []\n",
    "    sentences_int = []\n",
    "\n",
    "    for sentence_original in set(sentences):\n",
    "        \n",
    "        interactions = []\n",
    "        # Use NLTK to do pos tagging the sentence and filter the tags to keep only the one wanted\n",
    "        pos_tagged = find_pos_tagged_s2e([sentence_original])\n",
    "        \n",
    "        tags_filtered1, sentences_updated = remove_representent(pos_tagged[0])\n",
    "        sentence_original = sentences_updated\n",
    "        \n",
    "        tags_filtered = find_patterns(tags_filtered1, LIST_TAGS)\n",
    "\n",
    "        # Create the group of entities that interacts in the sentence. Use filters to keep the correct one\n",
    "        group_cooperation, opp = detect_groups_cooperations(tags_filtered)\n",
    "        \n",
    "        if(len(opp)!=0):\n",
    "            opposition_index = [sentence_original.index(opp[0])]\n",
    "        else:\n",
    "            opposition_index = []\n",
    "        group_cooperation, sentence  = find_coalitions(group_cooperation, sentence_original, opposition_index)\n",
    "\n",
    "        group_cooperation, sentence = remove_from_concern_of(group_cooperation ,sentence)\n",
    "        \n",
    "        \n",
    "        group_cooperation, sentence = check_doubles(sentence, group_cooperation)\n",
    "        \n",
    "        # Check if interactions after all the filters\n",
    "        interaction_bool = check_interaction(group_cooperation)\n",
    "\n",
    "        if(interaction_bool):\n",
    "            \n",
    "            # Try to find an opposition link \n",
    "            token = tools.tokenize_sentence(sentence, True)\n",
    "            opposition = ['opposed by','while','opposed by the']\n",
    "            opposition = set(token).intersection(set(opposition))\n",
    "\n",
    "            # Try to find an criticism link\n",
    "            criticism = ['criticized']\n",
    "            criticism= set(token).intersection(set(criticism))\n",
    "            \n",
    "\n",
    "            # Check if there is an opposition in the sentence\n",
    "            if(opposition != set() and len(group_cooperation) ==2):\n",
    "                interactions += opposed_by(sentence, group_cooperation, opposition)\n",
    "            else:\n",
    "                # Check if there is a criticism in the sentence\n",
    "                if(criticism != set() and check_link(criticism, sentence, group_cooperation[0])):\n",
    "                    interactions += criticized_by(sentence, group_cooperation, criticism)         \n",
    "                else :\n",
    "                    \n",
    "                    if(check_cooperation(sentence_original, group_cooperation[0])):\n",
    "                        # Find all the cooperations\n",
    "                        \n",
    "                        interactions += coop(sentence, group_cooperation[0])\n",
    "\n",
    " \n",
    "            \n",
    "            # Add the interactions in the list of all the interactions and the sentence realated to it if there is at least one interaction  \n",
    "            if(len(interactions)!= 0):\n",
    "                s = str(sentence_original)\n",
    "                sentences_int.append(sentence_original)     \n",
    "                interactions = [(x[0],x[1],x[2],s) for x in interactions]\n",
    "                interactions_ += interactions\n",
    "\n",
    "    # Create a dataframe with all the information of the interactions   \n",
    "    interactions_ = [ write_line(issue_number, x) for x in interactions_]\n",
    "    interactions_ = add_id(interactions_)\n",
    "    df = create_df(interactions_,issue_number)\n",
    "    \n",
    "    return df, sentences_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interaction_range(list_issues):\n",
    "    \"\"\" Function that find all interaction for a range of issues. \"\"\"\n",
    "    issues = tools.extract_from_csv_list_issues('Files/list_meetings.csv')\n",
    "    #issues = list_issues\n",
    "    df = pd.DataFrame()\n",
    "    sentences = []\n",
    "    for issue_number in sorted(issues):\n",
    "        print(issue_number)\n",
    "        df_temp,s  = interactions(issue_number)\n",
    "        frames = [df, df_temp]\n",
    "        df = pd.concat(frames)\n",
    "        sentences += s\n",
    "    \n",
    "    return df, sentences"
   ]
  },
  {
   "source": [
    "df, s = extract_interaction_range(range(1,778))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "19\n",
      "20\n",
      "21\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_a : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "not added c_b : 'CAPE VERDE'\n",
      "22\n",
      "23\n",
      "24\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "not added c_a : 'AMERICAN SAMOA'\n",
      "not added c_b : 'AMERICAN SAMOA'\n",
      "not added c_b : 'AMERICAN SAMOA'\n",
      "not added c_b : 'AMERICAN SAMOA'\n",
      "not added c_a : 'AMERICAN SAMOA'\n",
      "not added c_a : 'AMERICAN SAMOA'\n",
      "not added c_b : 'AMERICAN SAMOA'\n",
      "not added c_a : 'AMERICAN SAMOA'\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "not added c_a : 'LATIN AMERICA'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'LATIN AMERICA'\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'SERBIA'\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "not added c_b : 'ALBANIA AND MOLDOVA GROUP'\n",
      "not added c_b : 'ALBANIA AND MOLDOVA GROUP'\n",
      "not added c_a : 'ALBANIA AND MOLDOVA GROUP'\n",
      "not added c_a : 'ALBANIA AND MOLDOVA GROUP'\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_b : 'CLIMATE ACTION NETWORK'\n",
      "not added c_a : 'CLIMATE ACTION NETWORK'\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "not added c_b : 'GRULAC'\n",
      "not added c_b : 'GRULAC'\n",
      "not added c_b : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_a : 'GRULAC'\n",
      "not added c_b : 'GRULAC'\n",
      "not added c_b : 'GRULAC'\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_a : 'TIMOR-LESTE'\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "699\n",
      "700\n",
      "701\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_b : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "not added c_a : 'MONTENEGRO'\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "not added c_b : 'TIMOR-LESTE'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_b : 'SERBIA'\n",
      "not added c_a : 'SERBIA'\n",
      "not added c_b : 'SERBIA'\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_b : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "not added c_a : 'ANDORRA'\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_a : 'NORTH MACEDONIA'\n",
      "not added c_b : 'NORTH MACEDONIA'\n",
      "760\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Text/interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          type  issue  id_ca  id_cb     Country A     Country B  behalf  \\\n",
       "0    generated      1    148     14   NEW ZEALAND       AUSTRIA     0.0   \n",
       "0    generated      2     75    165            EU        POLAND     0.0   \n",
       "1    generated      2     13    213     AUSTRALIA            US     0.0   \n",
       "2    generated      2     13    104     AUSTRALIA         JAPAN     0.0   \n",
       "3    generated      2    165    104        POLAND         JAPAN     0.0   \n",
       "..         ...    ...    ...    ...           ...           ...     ...   \n",
       "719  generated    776    172     29  SAUDI ARABIA        BRAZIL     0.0   \n",
       "720  generated    776     29    172        BRAZIL  SAUDI ARABIA     0.0   \n",
       "721  generated    776     66    172       ECUADOR  SAUDI ARABIA     0.0   \n",
       "722  generated    776    153     37        NORWAY        CANADA     0.0   \n",
       "723  generated    776     37    153        CANADA        NORWAY     0.0   \n",
       "\n",
       "     support  agreement  opposition  criticism  cooperation  \\\n",
       "0        1.0        0.0         0.0        0.0          1.0   \n",
       "0        0.0        1.0         0.0        0.0          1.0   \n",
       "1        0.0        1.0         0.0        0.0          1.0   \n",
       "2        0.0        1.0         0.0        0.0          1.0   \n",
       "3        0.0        1.0         0.0        0.0          1.0   \n",
       "..       ...        ...         ...        ...          ...   \n",
       "719      0.0        1.0         0.0        0.0          1.0   \n",
       "720      0.0        1.0         0.0        0.0          1.0   \n",
       "721      0.0        1.0         0.0        0.0          1.0   \n",
       "722      0.0        1.0         0.0        0.0          1.0   \n",
       "723      0.0        1.0         0.0        0.0          1.0   \n",
       "\n",
       "                                             sentences  \n",
       "0    Austria supported by New Zealand stated that c...  \n",
       "0    The EU the US Australia Canada Japan and Polan...  \n",
       "1    The EU the US Australia Canada Japan and Polan...  \n",
       "2    The EU the US Australia Canada Japan and Polan...  \n",
       "3    The EU the US Australia Canada Japan and Polan...  \n",
       "..                                                 ...  \n",
       "719  Brazil with Saudi Arabia and Ecuador stressed ...  \n",
       "720  Brazil with Saudi Arabia and Ecuador stressed ...  \n",
       "721  Brazil with Saudi Arabia and Ecuador stressed ...  \n",
       "722  Canada and Norway underscored the need to avoi...  \n",
       "723  Canada and Norway underscored the need to avoi...  \n",
       "\n",
       "[78861 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>issue</th>\n      <th>id_ca</th>\n      <th>id_cb</th>\n      <th>Country A</th>\n      <th>Country B</th>\n      <th>behalf</th>\n      <th>support</th>\n      <th>agreement</th>\n      <th>opposition</th>\n      <th>criticism</th>\n      <th>cooperation</th>\n      <th>sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>generated</td>\n      <td>1</td>\n      <td>148</td>\n      <td>14</td>\n      <td>NEW ZEALAND</td>\n      <td>AUSTRIA</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Austria supported by New Zealand stated that c...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>generated</td>\n      <td>2</td>\n      <td>75</td>\n      <td>165</td>\n      <td>EU</td>\n      <td>POLAND</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>The EU the US Australia Canada Japan and Polan...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>generated</td>\n      <td>2</td>\n      <td>13</td>\n      <td>213</td>\n      <td>AUSTRALIA</td>\n      <td>US</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>The EU the US Australia Canada Japan and Polan...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>generated</td>\n      <td>2</td>\n      <td>13</td>\n      <td>104</td>\n      <td>AUSTRALIA</td>\n      <td>JAPAN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>The EU the US Australia Canada Japan and Polan...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>generated</td>\n      <td>2</td>\n      <td>165</td>\n      <td>104</td>\n      <td>POLAND</td>\n      <td>JAPAN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>The EU the US Australia Canada Japan and Polan...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>719</th>\n      <td>generated</td>\n      <td>776</td>\n      <td>172</td>\n      <td>29</td>\n      <td>SAUDI ARABIA</td>\n      <td>BRAZIL</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Brazil with Saudi Arabia and Ecuador stressed ...</td>\n    </tr>\n    <tr>\n      <th>720</th>\n      <td>generated</td>\n      <td>776</td>\n      <td>29</td>\n      <td>172</td>\n      <td>BRAZIL</td>\n      <td>SAUDI ARABIA</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Brazil with Saudi Arabia and Ecuador stressed ...</td>\n    </tr>\n    <tr>\n      <th>721</th>\n      <td>generated</td>\n      <td>776</td>\n      <td>66</td>\n      <td>172</td>\n      <td>ECUADOR</td>\n      <td>SAUDI ARABIA</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Brazil with Saudi Arabia and Ecuador stressed ...</td>\n    </tr>\n    <tr>\n      <th>722</th>\n      <td>generated</td>\n      <td>776</td>\n      <td>153</td>\n      <td>37</td>\n      <td>NORWAY</td>\n      <td>CANADA</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Canada and Norway underscored the need to avoi...</td>\n    </tr>\n    <tr>\n      <th>723</th>\n      <td>generated</td>\n      <td>776</td>\n      <td>37</td>\n      <td>153</td>\n      <td>CANADA</td>\n      <td>NORWAY</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Canada and Norway underscored the need to avoi...</td>\n    </tr>\n  </tbody>\n</table>\n<p>78861 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}