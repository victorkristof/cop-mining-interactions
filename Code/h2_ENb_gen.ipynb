{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from c1_extract_paragraphe_issue.ipynb\nIssue  34\nimporting Jupyter notebook from c2_extract_sentence_issue.ipynb\nimporting Jupyter notebook from g1_generate_dictionary.ipynb\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from itertools import combinations  \n",
    "import itertools\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "import g1_generate_dictionary as g1\n",
    "import collections\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY = g1.compute_dictionary()\n",
    "n = 0\n",
    "DICTIONARY_NUM = dict()\n",
    "for k in DICTIONARY:\n",
    "\n",
    "    DICTIONARY_NUM[k] = n \n",
    "    n +=1\n",
    "NAMES = []\n",
    "for x in DICTIONARY.values():\n",
    "    if(len(x) == 1):\n",
    "        NAMES.append(x[0])\n",
    "\n",
    "    else:\n",
    "        NAMES += [k for k in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char.\"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace('\\t','')\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_title(entities):\n",
    "    new_entities = []\n",
    "    for e in entities:\n",
    "        e = e.replace('\\n','')\n",
    "        splited = e.split(' ')\n",
    "        \n",
    "        if(len(splited) == 1 or len(splited) == 2):\n",
    "            new_entities.append(e.title())\n",
    "        else:\n",
    "\n",
    "            s =''\n",
    "            for i in range(len(splited)):\n",
    "                if(splited[i] == 'AND' or splited[i] == 'OF' or splited[i] == 'THE'):\n",
    "                    s += splited[i].lower() + ' '\n",
    "                else:\n",
    "                    s += splited[i].title() + ' '\n",
    "\n",
    "            s = s[:-1]\n",
    "\n",
    "            new_entities.append(s)\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue_number(issue_number):\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    p = c1.extract_paragraphes_from_issue(issue_number)\n",
    "    sentences = c2.extract_from_txt_sentences(p)\n",
    "    #list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))]\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2 = []\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        tokens = tokenize_sentence(s,False)\n",
    "\n",
    "        if(len(set(tokens).intersection(set(ENTITIES)))> 1):\n",
    "            sentences_s2.append(s)\n",
    "    return sentences_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = list(set([s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + make_title(list(open('Text/entities_interactions.txt')))+ ['Saint Vincent and the Grenadines','Republic of Korea','St. Lucia']))\n",
    "COALITIONS = []\n",
    "SUPPORTS_LINKS = ['with','and','for the','on behalf of the','supported by','speaking for the','for several']\n",
    "OPPOSITION_LINKS= ['opposed by','while','opposed by the']\n",
    "LIST_TAGS = ['IN', 'CC', 'NN', 'NNP', 'JJ','NNPS','MD','VBP','VB','VBZ','VBD','RB','VBN','PRP', 'NNS']\n",
    "PARTY_GROUPINGS = sorted(set([s.replace('\\n','').upper() for s in list(open('Text/party_grouping_clean.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/party_grouping_clean.txt'))] + [s.replace('\\n','') for s in list(open('Text/party_grouping_clean.txt'))]))\n",
    "list_entities = list(set([s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]))\n",
    "PARTIES = sorted(set(list_entities).difference(set(PARTY_GROUPINGS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, country):\n",
    "    \"\"\"Split the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    list_entities = ENTITIES\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "    if(country):\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        \n",
    "        tokens_entities.append(['for'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "        tokens_entities.append(['speaking','for'])\n",
    "    else :\n",
    "        tokens_entities.append(['speaking','for','the'])\n",
    "        tokens_entities.append(['concerns', 'of', 'the'])\n",
    "        tokens_entities.append(['concerns', 'of'])\n",
    "        tokens_entities.append(['speaking','for'])\n",
    "        tokens_entities.append(['on','behalf','of','the'])\n",
    "        tokens_entities.append(['spoke','with'])\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        tokens_entities.append(['for','the'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "    tokens_entities.append(['for', 'a', 'number','of', 'members' ,'of' ,'the'])\n",
    "    tokens_entities.append(['for', 'several'])\n",
    "    tokens_entities.append(['speaking','on','behalf', 'of', 'the'])  \n",
    "    tokens_entities.append(['supported','by'])\n",
    "    tokens_entities.append(['opposed','by'])\n",
    "    tokens_entities.append(['proposed','by','the'])\n",
    "    tokens_entities.append(['proposed','by'])\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "    if(type(sentence) == list):\n",
    "        line = sentence[0].replace(\",\",\"\")\n",
    "    else: \n",
    "        line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_tagged_s2e(list_s2e):\n",
    "    \"\"\" Use NLTK to pos_tag all the sentences from list_s2e and return a list of all the sentences pos_tagged\"\"\"\n",
    "    pos_tagged = []\n",
    "    for s in list_s2e:\n",
    "        s = s.replace('\\\\t','')\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "        tokens = tokenize_sentence(s, False)\n",
    "\n",
    "        pos_tagged.append(nltk.pos_tag(tokens))  \n",
    "   \n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(pos_tagged, list_tags):  \n",
    "    \"\"\" Find all the pattern in list_tags needed in sentences pos_tagged. \"\"\"\n",
    "    groups = [x[0] for x in pos_tagged[0] if x[1] in list_tags]\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1g(groups): \n",
    "    \"\"\" Find all the entities in the groups. Return a list of entities\"\"\"\n",
    "    groups = [g.replace(',','') for g in groups]\n",
    "    entities = set(groups).intersection(set(ENTITIES))\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2g(groups, opp):\n",
    "    \"\"\" Find all the entities for each groups. Return two lists of entities\"\"\"\n",
    "\n",
    "    index = groups.index(opp)\n",
    "\n",
    "    g1 = find_1g(groups[:index])\n",
    "    g2 = find_1g(groups[index +1:])\n",
    "\n",
    "    return g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_groups_cooperations(groups):\n",
    "    \"\"\" Return one or two groups with only entities and the original sentence. Return a list with one or two list\"\"\"\n",
    "\n",
    "    # Case 1 : Opposition between two groups\n",
    "    if(set(OPPOSITION_LINKS).intersection(set(groups)) != set()):\n",
    "        opp = list(set(OPPOSITION_LINKS).intersection(set(groups)))[0]\n",
    "        \n",
    "        g1, g2 = find_2g(groups, opp)\n",
    "        return [g1, g2], [opp]\n",
    "\n",
    "    # Case 2 : Only support\n",
    "    else:\n",
    "        g1 = find_1g(groups)\n",
    "        return [g1], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coalitions(groups, sentence, opposition_index):\n",
    "    \"\"\" Remove all the parties in groups that speak for a coalition, return list group updated if the case. \"\"\"\n",
    "    group_updates = []\n",
    "\n",
    "    new_tokens = []\n",
    "\n",
    "    for group in groups: \n",
    "        set_group = set(group)\n",
    "\n",
    "        truples_c = []\n",
    "        token =tokenize_sentence(sentence,False)\n",
    "\n",
    "        links = ['for the','for several','on behalf of the','speaking for the','on behalf of','for','speaking for','for a number of members of the', 'speaking on behalf of the']\n",
    "\n",
    "        if(set(token).intersection(set(links)) != set()):\n",
    "\n",
    "            for i in range(len(token)-2):\n",
    "\n",
    "                if(token[i] in group and token[i+1] in links and token[i+2] in PARTY_GROUPINGS):\n",
    "\n",
    "                    set_group.remove(token[i])\n",
    "\n",
    "                    if(len(opposition_index) !=0):\n",
    "                        s = sentence[:opposition_index[0]]\n",
    "                        v = sentence[opposition_index[0]:]\n",
    "                        s = s.replace(token[i+1],'').replace(token[i],'')\n",
    "                        u = ' '\n",
    "                        sentence = u.join([s,v])\n",
    "                    else:\n",
    "                        sentence = sentence.replace(token[i+1],'').replace(token[i],'')\n",
    "                        \n",
    "\n",
    "        group_updates.append(list(set_group))\n",
    "        #print(group_updates)\n",
    "    return group_updates, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_concern_of(groups, sentence):\n",
    "    \"\"\" Filter that remove all entities when they are mentioned with the pattern from or concern of. \"\"\"\n",
    "    group_updates = []\n",
    "\n",
    "    for group in groups: \n",
    "        truples_c = []\n",
    "        token =tokenize_sentence(sentence,False)\n",
    "\n",
    "        links = ['from','from the','concerns of the','concern of']\n",
    "\n",
    "\n",
    "        if(set(token).intersection(set(links)) != set()):\n",
    "            \n",
    "            for i in range(len(token)-2):\n",
    "                if((token[i+1] in PARTIES or token[i+1] in PARTY_GROUPINGS) and token[i] in links):\n",
    "                    if(token[i+2] in PARTIES or token[i+2] in PARTY_GROUPINGS):\n",
    "                        \n",
    "                        group = [g for g in group if g != token[i+2]]\n",
    "                        sentence = sentence.replace(token[i+2],'')\n",
    "                    group = [g for g in group if g != token[i+1]]\n",
    "\n",
    "                    sentence = sentence.replace(token[i+1],'')\n",
    "\n",
    "        group_updates.append(group)\n",
    "        \n",
    "    \n",
    "    return group_updates, sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rSubset(arr, cop): \n",
    "    \"\"\" function that return all the tuples needed for the interactions. \"\"\"\n",
    "    l = list(set(list(itertools.product(arr, arr))))\n",
    "\n",
    "    return [(c1.upper(),c2.upper(),cop) for c1,c2 in l if c1 != c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behalf_of(sentence, group_cooperation, link):\n",
    "    \"\"\"Function that find all the interaction of type \"behalf\". \"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "    for x in countries_B:\n",
    "\n",
    "        tuples.append((list(country_A)[0].upper(),x,['behalf','cooperation']))\n",
    "        \n",
    "    tuples += rSubset(list(countries_B),['agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supported_by(sentence, group_cooperation, link ):\n",
    "    \"\"\"Function that find all the interaction of type \"support\". \"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "\n",
    "    for x in countries_B:\n",
    "        \n",
    "        tuples.append((x,list(country_A)[0].upper(),['support','cooperation']))\n",
    "        \n",
    "    tuples += rSubset(list(countries_B),['agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(link, sentence, group_cooperation):\n",
    "    \"\"\"Verify that there is entities in both sides of the link\"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "\n",
    "    index = token.index(list(link)[0])\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    country_B = set(token[index:]).intersection(set(group_cooperation))\n",
    "    return country_A != set() and country_B != set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coop(sentence, group_cooperation):\n",
    "    \"\"\"Function that find all the interaction of type \"cooperation\" and classify them. \"\"\"\n",
    "    \n",
    "    token =tokenize_sentence(sentence, True)\n",
    "\n",
    "    behalf = ['speaking for','on behalf of']\n",
    "    behalf = set(token).intersection(set(behalf))\n",
    "\n",
    "    support = ['supported by','supported by the']\n",
    "    support = set(token).intersection(set(support))\n",
    "\n",
    "\n",
    "    tuples = []\n",
    "    cooperation = []\n",
    "    if(behalf != set()and check_link(behalf, sentence, group_cooperation)):\n",
    "        return behalf_of(sentence, group_cooperation, behalf)\n",
    "    else: \n",
    "        if(support != set() and  check_link(support, sentence, group_cooperation)):\n",
    "            return supported_by(sentence, group_cooperation, support)\n",
    "\n",
    "        else :\n",
    "            return rSubset(group_cooperation,['agreement','cooperation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def opposed_by(sentence, group_cooperation, link):\n",
    "    \"\"\" Function that find all the interactions of type \"opposition\" .\"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = group_cooperation[0]\n",
    "    countries_B = group_cooperation[1]\n",
    "\n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "\n",
    "    tuples = [(c2.upper(),c1.upper(),['opposition']) for c1,c2 in tuples] \n",
    "\n",
    "\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence.split(list(link)[0])\n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop([splited[0]], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop([splited[1]], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticized_by(sentence, group_cooperation, link):\n",
    "    \"\"\" Function that find all the interactions of type \"criticism\" .\"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = list(set(group_cooperation[0]).intersection(set(token[:index])))\n",
    "    countries_B = list(set(group_cooperation[0]).intersection(set(token[index+1:])))\n",
    "    \n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "    tuples = [(c1.upper(),c2.upper(),['criticism']) for c1,c2 in tuples]\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence[0].split(list(link)[0])\n",
    "    \n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop(splited[0], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop(splited[1], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_line(issue_number, x):\n",
    "    \"\"\" Function that help to whrite a line wit all the interactions. \"\"\"\n",
    "    \"\"\"  'behalf'\t 'support'\t 'spokewith'\t 'agreement'\t 'delay'\t 'opposition'\t 'criticism'\t 'cooperation'\"\"\"\n",
    "    dict_interactions = {'behalf' : 0, 'support' : 0, 'agreement':0 , 'opposition':0,'criticism' :0, 'cooperation':0}\n",
    "    for int in x[2]:\n",
    "        dict_interactions[int] = 1\n",
    "    values = list(dict_interactions.values())\n",
    "\n",
    "    v = []\n",
    "    v.append(issue_number)\n",
    "    v.append(x[0])\n",
    "    v.append(x[1])\n",
    "    v += values\n",
    "    v.append(x[3])\n",
    "\n",
    "    #v.append(x[4])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_double_s(sentences):\n",
    "    \"\"\" Filter that try to find if one sentence contain two entities but they are not related. \"\"\"\n",
    "    tags_wanted = ['VBD','MD']\n",
    "    words_wanted = ['and']\n",
    "    s2 = []\n",
    "    sentences_filtered = []\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    set_sentences = set(sentences)\n",
    "    for s in sentences:\n",
    "        pos_tagged = find_pos_tagged_s2e([s])\n",
    "\n",
    "        \n",
    "        \n",
    "        filtered = [x for x in pos_tagged[0] if x[0] in ENTITIES or x[0] in words_wanted or x[1] in tags_wanted]\n",
    "        filtered_only_VBD = [x for x in pos_tagged[0] if x[0] in PARTIES or x[0] in words_wanted or x[1]== 'VBD']\n",
    "\n",
    "\n",
    "        if(len(filtered_only_VBD)>= 5):\n",
    "            for i in range(len(filtered_only_VBD)-4):\n",
    "                if( filtered_only_VBD[i][0] in ENTITIES and filtered_only_VBD[i+1][1]=='VBD' and filtered_only_VBD[i+2][0] == 'and' and  filtered_only_VBD[i+3][0] in PARTIES and  filtered_only_VBD[i+4][1]in tags_wanted and s in set_sentences):\n",
    "\n",
    "                    s_to_filter.append((s,filtered_only_VBD[i+1]))\n",
    "                    set_sentences.remove(s)\n",
    "\n",
    "        if(len(filtered)>= 5):\n",
    "            for i in range(len(filtered)-4):\n",
    "                if( filtered[i][0] in ENTITIES and filtered[i+1][1] in tags_wanted and filtered[i+2][0] == 'and' and  filtered[i+3][0] in ENTITIES and  filtered[i+4][1]in tags_wanted and s in set_sentences):\n",
    "\n",
    "                    s_to_filter.append((s,filtered[i+1]))\n",
    "                    set_sentences.remove(s)\n",
    "\n",
    "    sentences = list(set_sentences) \n",
    "    for s in s_to_filter:\n",
    "\n",
    "        index = s[0].index(s[1][0])\n",
    "        s1 = s[0][:index]\n",
    "        s2 = s[0][index+1:]\n",
    "        sentences.append(s1)\n",
    "        sentences.append(s2)\n",
    "    \n",
    "    return list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_on_99s_from_programme(sentences):\n",
    "    \"\"\" Filter that remove entities that are related to some patterns that are not interactions. \"\"\"\n",
    "    set_sentences = set(sentences)\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    note = ['92s','Programme','proposed by the','proposed by']\n",
    "    for s in sentences:\n",
    "\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "\n",
    "      \n",
    "        for i in range(len(pos_tagged)-1):\n",
    "\n",
    "            if(pos_tagged[i][0] == 'on' and pos_tagged[i+1][0] in ENTITIES and s in set_sentences):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i+1][0]))\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "            if((pos_tagged[i+1][0] == '92s' or pos_tagged[i+1][0] == 'Programme') and pos_tagged[i][0] in ENTITIES and s in set_sentences):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0]))\n",
    "                set_sentences.remove(s)\n",
    "            \n",
    "            if((pos_tagged[i][0] == 'proposed by the' or pos_tagged[i+1][0] == 'proposed by') and pos_tagged[i+1][0] in ENTITIES and s in set_sentences):\n",
    "  \n",
    "                s_to_filter.append((s.replace (pos_tagged[i+1][0],''),pos_tagged[i+1][0]))\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "\n",
    "    x = ' '\n",
    "    for s in s_to_filter:\n",
    "        token = tokenize_sentence(s[0],False)\n",
    "        tokens = []\n",
    "        for i in range(len(token)-1):\n",
    "            if(token[i] not in note and  not (token[i+1]==s[1])):\n",
    "                tokens.append(token[i])\n",
    "\n",
    "        #s_f = s[0].replace(s[1],'')\n",
    "        s_f = x.join(tokens)\n",
    "        s_filtered.append(s_f)\n",
    "\n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "    \n",
    "    return list(set(s_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(group_cooperation):\n",
    "    \"\"\" Function that return if there is or not an interaction. \"\"\"\n",
    "    return len(group_cooperation) == 2 or len(group_cooperation[0])>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doubles(sentence, group_cooperation):\n",
    "    \"\"\" Filter that find if one entity is mentioned twice in two different manner and we should not count and interaction between them. \"\"\"\n",
    "    gc_new = []\n",
    "    \n",
    "    for g in group_cooperation:\n",
    "\n",
    "        g_upper = [e.upper() for e in g]\n",
    "        g_title = [e.title() for e in g]\n",
    "        if(len(set(g_upper)) != len(g)):\n",
    "            gc_new.append(list(set(g).difference(set(g_title))))\n",
    "            removed = list(set(g).intersection(set(g_title)))\n",
    "            for r in removed:\n",
    "                sentence = sentence.replace(r,'')\n",
    "        else:\n",
    "            gc_new.append(g)\n",
    "    return gc_new, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inversions(sentences):\n",
    "    \"\"\" Filter tht try to detect sentence that have been inverted (verb - entity - entity) and change it to be able to detect the interaction. \"\"\"\n",
    "    set_sentences = set(sentences)\n",
    "\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    verbs_int =['Supported','Opposed']\n",
    "    for s in sentences:\n",
    "\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "\n",
    "        for i in range(len(pos_tagged)-3):\n",
    "\n",
    "            if(pos_tagged[i][0] in verbs_int and pos_tagged[i+1][0] == 'by' and pos_tagged[i+2][0] in ENTITIES and pos_tagged[i+3][0] in ENTITIES):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0],pos_tagged[i+1][0],pos_tagged[i+2][0],pos_tagged[i+3][0]))\n",
    "\n",
    "                set_sentences.remove(s)\n",
    "        \n",
    "        ## The EU ...\n",
    "        for i in range(len(pos_tagged)-4):\n",
    "\n",
    "            if(pos_tagged[i][0] in verbs_int and pos_tagged[i+1][0] == 'by' and pos_tagged[i+2][0] == 'the' and pos_tagged[i+3][0] in ENTITIES and pos_tagged[i+4][0] in ENTITIES):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0],pos_tagged[i+1][0],pos_tagged[i+3][0],pos_tagged[i+4][0]))\n",
    "                set_sentences.remove(s)\n",
    "        \n",
    "    \n",
    "    for s in s_to_filter:\n",
    "    \n",
    "        sentence = s[0]\n",
    "\n",
    "        verb = s[1]\n",
    "        by = s[2]\n",
    "        c_a = s[3]\n",
    "        c_b = s[4]\n",
    "        s_f =sentence.replace(by,'').replace(c_a,'').replace(c_b,'').replace(verb,'')\n",
    "        s_f = c_b + \" \"+ verb.lower() + \" \" + by + \" \" + c_a + \" \" + s_f\n",
    "        s_filtered.append(s_f)\n",
    "\n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "\n",
    "    return list(set(s_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    \"\"\" Find the key of one value in the dicitonary of the entities. \"\"\"\n",
    "    for key, value in DICTIONARY.items(): \n",
    "\n",
    "        if (len(value) == 1 and val == value[0]): \n",
    "             key_country = key \n",
    "\n",
    "        if(len(value) > 1 and val in value):\n",
    "            key_country = key\n",
    "        \n",
    "    return DICTIONARY_NUM[key_country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id(interactions_):\n",
    "    \"\"\" Match for each country its id. \"\"\"\n",
    "    tup = []\n",
    "    \n",
    "    for i in interactions_:\n",
    "\n",
    "        c_a = i[1]\n",
    "        c_b = i[2]\n",
    "\n",
    "        id_cb = 9999\n",
    "        id_ca = 9999\n",
    "    \n",
    "        if(c_a in NAMES):\n",
    "\n",
    "            id_ca = get_key(c_a)\n",
    "\n",
    "        if(c_a.upper() in NAMES):\n",
    "\n",
    "            id_ca = get_key(c_a.upper())\n",
    "\n",
    "        if(c_b in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b.upper())\n",
    "\n",
    "        if(c_b.upper() in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b.upper())\n",
    "\n",
    "        if(id_ca == 9999 ):\n",
    "            print('not added c_a :', repr(c_a))\n",
    "        \n",
    "        if(id_cb == 9999 ):\n",
    "            print('not added c_b :', repr(c_b))\n",
    "        i.append(id_ca)\n",
    "        i.append(id_cb)\n",
    "        \n",
    "\n",
    "    return interactions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cooperations, issue_number):\n",
    "    \"\"\" Combine all the information to create the dataframe. \"\"\"\n",
    "    cooperations = [i for i in cooperations if len(i) == 12]\n",
    "    if(len(cooperations)>0):\n",
    "        ca = [x[1].upper() for x in cooperations]\n",
    "        cb = [x[2].upper() for x in cooperations]\n",
    "        id_ca = [x[10] for x in cooperations]\n",
    "        id_cb = [x[11] for x in cooperations]\n",
    "        behalf = [x[3] for x in cooperations]\n",
    "        support = [x[4] for x in cooperations]\n",
    "        agreement =[x[5] for x in cooperations]\n",
    "        opposition =[x[6] for x in cooperations]\n",
    "        criticism =[x[7] for x in cooperations]\n",
    "        cooperation =[x[8] for x in cooperations]\n",
    "        sentences = [x[9] for x in cooperations]\n",
    "        dict_issue = {'type': 'generated','issue': cooperations[0][0],'id_ca':id_ca,'id_cb':id_cb,'Country A':ca, 'Country B': cb, 'behalf':behalf,'support':support,'agreement':agreement,'opposition':opposition,'criticism':criticism,'cooperation':cooperation,'sentences': sentences}\n",
    "    else:\n",
    "        dict_issue = {'type': 'generated','issue': issue_number,'id_ca':1111,'id_cb':1111, 'behalf':[],'support':[],'agreement':[],'opposition':[],'criticism':[],'cooperation':[],'sentences':[]}\n",
    "\n",
    "    df = pd.DataFrame(dict_issue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_representent(pos_tagged):\n",
    "    \"\"\" Remove all the entities mentioned when it is related to a presentant and so not a interaction. \"\"\"\n",
    "    entities_repr = []\n",
    "    for i in range(len(pos_tagged)-2):\n",
    "        if(pos_tagged[i][1] =='NNP' and pos_tagged[i][0] not in ENTITIES and pos_tagged[i+1][1] == 'IN' and pos_tagged[i+2][0] in ENTITIES):\n",
    "            entities_repr.append(pos_tagged[i+2])\n",
    "    return [[g for g in pos_tagged if g not in entities_repr]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_point(sentences):\n",
    "    \"\"\" Split special sentence where there is an sentence ta finish with .\" .\"\"\"\n",
    "    \n",
    "    new_sentences = []\n",
    "    index = -1\n",
    "    for s in sentences:\n",
    "        \n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "\n",
    "        for i in range(len(pos_tagged)-1):\n",
    "            if(pos_tagged[i][0] == '”' and pos_tagged[i+1][0] in ENTITIES):\n",
    "\n",
    "                index = pos_tagged[i+1][0]\n",
    "        if(index != -1):\n",
    "\n",
    "            splited = s.split(index)\n",
    "            s1= splited[0]\n",
    "\n",
    "            s2 = splited[1]+' ' + index\n",
    "\n",
    "            new_sentences.append(s1)\n",
    "            new_sentences.append(s2)\n",
    "            index = -1\n",
    "        else:\n",
    "            new_sentences.append(s)\n",
    "    return new_sentences          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    \"\"\" Try to remove special character still present from the webscrapping. \"\"\"\n",
    "\n",
    "    sentence_original = re.sub('class=\"ENB-Body\" align=\"justify\"','',sentence)\n",
    "    sentence_original = re.sub('>','',sentence_original)\n",
    "    sentence_original = re.sub('class=\"textstory\"','',sentence_original)\n",
    "\n",
    "    return sentence_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(issue_number):\n",
    "\n",
    "    \"\"\" Function that combine all the filters to find all the interaction for one specific issue. \"\"\"\n",
    "    \n",
    "    sentences = extract_s2e_issue_number(issue_number)\n",
    "    sentences = [re.sub('class=\"ENB-Body\" align=\"justify\"','',x) for x in sentences]\n",
    "    #print(repr(sentences[0]))\n",
    "    \n",
    "\n",
    "    # Filters to remove to pre-procss sentences to be able to detect if there is an interaction or not\n",
    "    sentences = remove_double_s(sentences)\n",
    "\n",
    "    sentences = remove_on_99s_from_programme(sentences)\n",
    "    \n",
    "    sentences = find_inversions(sentences)\n",
    "    \n",
    "    sentences = add_point(sentences)\n",
    "\n",
    "\n",
    "    interactions_ = []\n",
    "    sentences_int = []\n",
    "\n",
    "    for sentence_original in set(sentences):\n",
    "        \n",
    "        interactions = []\n",
    "        sentence_original = clean_sentence(sentence_original)\n",
    "\n",
    "        # Use NLTK to do pos tagging the sentence and filter the tags to keep only the one wanted\n",
    "        pos_tagged = find_pos_tagged_s2e([sentence_original])\n",
    "\n",
    "        tags_filtered1 = remove_representent(pos_tagged[0])\n",
    "        \n",
    "        tags_filtered = find_patterns(tags_filtered1, LIST_TAGS)\n",
    "\n",
    "        # Create the group of entities that interacts in the sentence. Use filters to keep the correct one\n",
    "        group_cooperation, opp = detect_groups_cooperations(tags_filtered)\n",
    "\n",
    "        if(len(opp)!=0):\n",
    "            opposition_index = [sentence_original.index(opp[0])]\n",
    "        else:\n",
    "            opposition_index = []\n",
    "        group_cooperation, sentence  = find_coalitions(group_cooperation, sentence_original, opposition_index)\n",
    "\n",
    "        group_cooperation, sentence = remove_from_concern_of(group_cooperation ,sentence)\n",
    "        \n",
    "        \n",
    "        group_cooperation, sentence = check_doubles(sentence, group_cooperation)\n",
    "        \n",
    "        # Check if interactions after all the filters\n",
    "        interaction_bool = check_interaction(group_cooperation)\n",
    "\n",
    "        if(interaction_bool):\n",
    "    \n",
    "            # Try to find an opposition link \n",
    "            token = tokenize_sentence(sentence, True)\n",
    "            opposition = ['opposed by','while','opposed by the']\n",
    "            opposition = set(token).intersection(set(opposition))\n",
    "\n",
    "            # Try to find an criticism link\n",
    "            criticism = ['criticized']\n",
    "            criticism= set(token).intersection(set(criticism))\n",
    "            \n",
    "\n",
    "            # Check if there is an opposition in the sentence\n",
    "            if(opposition != set() and len(group_cooperation) ==2):\n",
    "                interactions += opposed_by(sentence, group_cooperation, opposition)\n",
    "            else:\n",
    "                # Check if there is a criticism in the sentence\n",
    "                if(criticism != set() and check_link(criticism, sentence, group_cooperation[0])):\n",
    "                    interactions += criticized_by(sentence, group_cooperation, criticism)         \n",
    "                else :\n",
    "                    # Find all the cooperations\n",
    "                    interactions += coop(sentence, group_cooperation[0])\n",
    "\n",
    " \n",
    "            \n",
    "            # Add the interactions in the list of all the interactions and the sentence realated to it if there is at least one interaction  \n",
    "            if(len(interactions)!= 0):\n",
    "                s = str(sentence_original)\n",
    "                sentences_int.append(sentence_original)     \n",
    "                interactions = [(x[0],x[1],x[2],s) for x in interactions]\n",
    "                interactions_ += interactions\n",
    "\n",
    "    # Create a dataframe with all the information of the interactions   \n",
    "    interactions_ = [ write_line(issue_number, x) for x in interactions_]\n",
    "    interactions_ = add_id(interactions_)\n",
    "    df = create_df(interactions_,issue_number)\n",
    "\n",
    "    return df, sentences_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences= ['IRAN, ARGENTINA and ALGERIA said that technology transfer should proceed under the principles and provisions of the Convention.']\n",
    "interactions(sentences, 613)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences= ['While KENYA pointed to the challenges of national circumstances concerning countries’ capacities, NEW ZEALAND observed that national circumstances also include feasibility.']\n",
    "interactions(sentences, 613)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences= ['On institutional arrangements, SOUTH AFRICA, CHINA, INDIA, IRAN, TANZANIA, Tuvalu, for the LDCs, and others, opposed by CANADA, JAPAN, the EU, the US and others, supported the establishment of an international capacity-building mechanism. ']\n",
    "interactions(sentences, 613)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit15afa4b5d9a84aa2af9f4a46f3f973aa",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}