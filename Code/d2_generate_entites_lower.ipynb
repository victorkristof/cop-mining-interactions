{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda12cc5b1983ce432e8ce1d0fc698bd97b",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the list list_w in the file name\n",
    "def write(list_w, name):\n",
    "    list_w = np.sort(list_w)\n",
    "    outF = open(name, \"w\")\n",
    "    for line in list_w:\n",
    "        outF.write(str(line))\n",
    "        outF.write(\"\\n\")\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from the file name\n",
    "def read(name):\n",
    "    return open(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number(entities):\n",
    "    black_list = []\n",
    "    for s in entities : \n",
    "        if(any(i.isdigit() for i in s) and '77' not in s):\n",
    "            black_list.append(s)\n",
    "    return black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_longuer(entities):\n",
    "    regex = '\\s[A-Z][A-Z]+'\n",
    "    patterns = ['UN','UNDP','AIJ','GREENPEACE','ICA','ICAO','IEA','II','IOC','IRAN','SBI','SBSTA','TEC','UNEP','US','CACAM','CAN','CBD','DESA','EGTT','GEF','MRV']\n",
    "    patterns += ['WWF','WHO','TFI','AWG','CBD','AIJ','WTO','WMO','UNESCO','UNECE','TSU','TRINIDAD','SRREN','SRCCL','OECD','NEPAL','NAPA','EU']\n",
    "    patterns = [p.title for p in patterns]\n",
    "    black_list = []\n",
    "    l = []\n",
    "    for b in patterns:\n",
    "        pattern = b+regex\n",
    "        for string in entities:\n",
    "           l +=re.findall(pattern, string)\n",
    "    for b in l:\n",
    "        for e in entities:\n",
    "            if(b in e and b == e[0:len(b)]):\n",
    "                black_list.append(e)\n",
    "    return black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_smaller(entities):\n",
    "    patterns = ['BURKINA','CLIMATE','CONGO','COOK','CZECH','DOMINICAN','FRANCISCAN','JAPAN AUSTRALIA','MARSHALL ISLANDS ARMENIA','NEW','ENVIRONMENTAL',\n",
    "    'WORLD','WOMEN','VI','TECHNOLOGY','BY','ANTIGUA','UMBRELLA','SIERRA','S REPUBLIC OF KOREA','PAPUA','OF KOREA']\n",
    "    patterns = [p.title for p in patterns]\n",
    "    black_list = []\n",
    "    \n",
    "    for p in patterns:\n",
    "        for e in entities:\n",
    "            e_splited = word_tokenize(e)\n",
    "            if(p == e):\n",
    "                black_list.append(p)\n",
    "\n",
    "    return black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_list_start(entities):\n",
    "    entities = set(entities)\n",
    "    patterns =['BRIEF','ANNUAL','AD HOC','ADVERSE EFFECTS','REPORT','AGENDA','ANALYSIS','DEVELOPMENT','DEAL','ACTIVITIES','SESSION','BEST','BOX']\n",
    "    patterns +=['BREEZEWAYS','BUSINESS','MEETINGS','CDM','CHAIR OF THE COW','COMMISSION DES FORÊTS','CARIBBEAN COMMUNITY','CONFERENCE','CONCLUSION','COP/']\n",
    "    patterns +=['IPCC','JOINT','ISSUE','LULUCF','L…','SUMMARY','CLIMATE CHANGE','GUIDELINES','PROGRAMME','ROUND TABLE','PROTOCOL','CONTACT GROUP']         \n",
    "    patterns +=['COULD DO MORE','PROPOSAL','FORUM','MECHANISM','CORRIDORS','XII','XIX','XIV','XVI','II','XVII','XX','XY','I GHG','I MRV','I P','I SPM','CONGRESS',                               'BUSSINESS']\n",
    "    patterns +=['CENTER','CHANGE','CLAIMED','FACILITY','POLICY','GLOBAL ENVIRONMENT','INFORMAL','PEOPLE','FORWARD','WORKSHOP','.','/','-','ARTICLE','+',\"D 'IVOIRE\",'\\\\']\n",
    "    patterns = [p.title for p in patterns]\n",
    "    black_list = []\n",
    "    for e in entities:\n",
    "        for w in black_list:\n",
    "            if(w in e):\n",
    "                black_list.append(e)\n",
    "    return black_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_length1(list_entities):\n",
    "    black_l = set()\n",
    "    for e in list_entities:\n",
    "\n",
    "        e_splited = word_tokenize(e)\n",
    "        if(len(e_splited) == 1):\n",
    "            if(len(e_splited[0])==1):\n",
    "                black_l.add(e)\n",
    "\n",
    "    return list(black_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meetings(list_entities):\n",
    "    list_meeting = ['SB','COP','UNFCCC','ADP','AGBM','IPCC','INC','FCCC','CMP','WG','WGI','WGII','WGIII','AWG']\n",
    "    list_meeting = [m.lower for m in meetings]\n",
    "    black_list = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        for w in e_splited:\n",
    "            if(w in list_meeting):\n",
    "                black_list.append(e)\n",
    "    return black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(list_entities):\n",
    "    regex2 = r'a\\s[a-z]+'\n",
    "    black_list = []\n",
    "    for e in list_entities:\n",
    "        black_list += re.findall(regex2,e)\n",
    "    return black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbs(list_entities):\n",
    "    black_list = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        for w in e_splited:\n",
    "            if(w.endswith('ing') or w.endswith('tion') or w.endswith('tions') or w.endswith('tive') or w.endswith('ments')or w.endswith('ed')or w.endswith('ment')):\n",
    "                black_list.append(e)\n",
    "\n",
    "    return black_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_long_words(list_entities):\n",
    "    black_list4 = []\n",
    "    for e in list_entities:\n",
    "        e_splited = word_tokenize(e)\n",
    "        if(len(e_splited)>5):\n",
    "            black_list4.append(e)\n",
    "    return black_list4\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_noun(entities):\n",
    "    month = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "    month = [m for m in month]\n",
    "    black_list = []\n",
    "    for e in entities:\n",
    "        for m in month:\n",
    "            if(m in e):\n",
    "                black_list.append(e)\n",
    "    return black_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_sentences(entities): \n",
    "    black_list1 = black_list_start(entities)\n",
    "\n",
    "    black_list1 += too_long_words(entities)\n",
    "\n",
    "    black_list1 += common_noun(entities)\n",
    "\n",
    "    black_list1 += patterns_longuer(entities)\n",
    "\n",
    "    black_list1 += sentence(entities)\n",
    "\n",
    "    black_list1 += patterns_smaller(entities)\n",
    "\n",
    "    black_list1 += verbs(entities)\n",
    "\n",
    "    black_list1 += meetings(entities)\n",
    "    \n",
    "    black_list1 += number(entities)\n",
    " \n",
    "    black_list1 += words_length1(entities)\n",
    "\n",
    "    entities_f = set(entities).difference(set(black_list1))\n",
    "    return list(set(entities_f))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the entities in uppercase from sentences.txt and write them in the file entities_uppercase.txt\n",
    "def extract_entities():\n",
    "    sentences = read(\"sentences202.txt\")\n",
    "    lowercase = 0\n",
    "    entities_b = []\n",
    "    for s in sentences:\n",
    "        list_w = [' '.join(b) for a, b in itertools.groupby(word_tokenize(s.strip()), key=str.isupper) if a]\n",
    "        entities_b +=list_w\n",
    "    write(entities_b, \"entities_c.txt\") \n",
    "    entities = remove_unwanted_sentences(list(set(entities_b)))\n",
    "    write(entities, \"entities.txt\")\n",
    "    print(len(entities))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "29\n"
    }
   ],
   "source": [
    "extract_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}