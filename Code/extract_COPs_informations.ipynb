{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import libraries needed\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTML content\n",
    "\n",
    "page = urllib.request.urlopen('https://enb.iisd.org/enb/vol12/').read()\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that extracts a date from a given string \n",
    "\n",
    "def extract_date(sdate):\n",
    "    m = re.findall('\\d{4}|\\d{2}|January|February|March|April|May|June|July|August|September|October|November|December|\\d{1}',sdate)\n",
    "    month = {'January':1,'February':2,'March':3,'April':4,'May':5,'June':6,'July':7,'August':8,'September':9,'October':10,'November':11,'December':12}\n",
    "    if(len(m)==0):\n",
    "        d=0\n",
    "    if(len(m)==5):\n",
    "        d = (int(m[0]),month[m[1]],int(m[len(m)-1]))\n",
    "    if(len(m)==4):\n",
    "        d = (int(m[0]),month[m[2]],int(m[len(m)-1]))\n",
    "    if(len(m)==3):\n",
    "        d = (int(m[0]),month[m[1]],int(m[2]))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that extracts digit from a given string\n",
    "def extract_number(sname):\n",
    "    for i in sname.split():\n",
    "        \n",
    "        if i.isdigit():\n",
    "            return int(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that helps to compose and order the list\n",
    "def compute_list(list_string):\n",
    "    list_cop = []\n",
    "\n",
    "    for s in list_string:\n",
    "        l = s.split('|')\n",
    "      \n",
    "        list_cop.append((extract_number(l[0]),extract_date(l[1]),l[2]))\n",
    "        \n",
    "    list_cop.sort(key=lambda a: a[0], reverse=False)\n",
    "\n",
    "    return list_cop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that extract the list of all the COPs from a webpage and return a list containing all the COPs with their number, date and place\n",
    "def extract_list_COPs(soup):\n",
    "    ## find all the different COPs (not named the same way)\n",
    "    #Case 1 :\"COP\"+\"\\s\"+\"\\d\"+\"\\s\"+\".\"+\"\\s\" \n",
    "        #COP 1-9\n",
    "    list = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"\\d\"+\"\\s\"+\".\"+\"\\s\"))\n",
    "\n",
    "    #Case 2 : \"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"\\d\"\n",
    "        #COP 10,23,24,25\n",
    "    list += soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"\\d\"))\n",
    "    \n",
    "    #Case 3 :\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\s\"+\"\\d\"+\".\"+\".\"\n",
    "        #COP 11,12,13,14,15,16,21,22\n",
    "    list_2 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\s\"+\"\\d\"+\".\"+\".\"))\n",
    "\n",
    "    # Case 4 : \"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\d\"+\"\\s\"\n",
    "        #COP 17,18,19\n",
    "    list_3 = soup.find_all(string=re.compile(\"COP\"+\"\\s\"+\"[1-2][0-9]\"+\"\\s\"+\".\"+\"\\s\"+\"CMP\"+\"\\d\"+\"\\s\"))\n",
    "    \n",
    "    ## Clean the lists to have all the same structure\n",
    "    # Clean list_2\n",
    "    for i in range(len(list_2)) :\n",
    "        list_2[i] =  re.sub(\"- CMP\"+\"\\s\"+\".\"+\".\", '', list_2[i])\n",
    "\n",
    "\n",
    "    # Clean list_3\n",
    "    for i in range(len(list_3)) :\n",
    "        list_3[i] =  re.sub(\"- CMP\"+\".\", '', list_3[i])\n",
    "    \n",
    "    #combine all the lists\n",
    "    list += list_2\n",
    "    list += list_3\n",
    "    \n",
    "    return compute_list(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that extract all the issues for each COPs and return a list in which for each COPs we have the issues corresponding with their number, date and html link \n",
    "def extract_details_COPS(soup):\n",
    "    detail_COPs = []\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "\n",
    "        for col in row.find_all('td'):\n",
    "        \n",
    "            if(\"Issue\" in col.string):\n",
    "                a = row.find_previous_sibling('tr')\n",
    "                b= a.find_next('th')\n",
    "                detail = []\n",
    "            \n",
    "                if(\"</h3>COP\" in str(b) and not(\"BIS\" in str(b))):\n",
    "                    date_td = col.find_next_sibling('td')\n",
    "                \n",
    "                    while(\"Summary\" != date_td.string and \"Summary and Analysis\" != date_td.string):\n",
    "                        ## extract issue number\n",
    "                        issue = int(re.findall('\\d+',col.string)[0])\n",
    "                        \n",
    "                        pdf_td = date_td.find_next_sibling('td')\n",
    "                        ## extract html link\n",
    "                        html_td = pdf_td.find_next_sibling('td')\n",
    "                        html = html_td.find('a',href=True)['href']\n",
    "                    \n",
    "                        ## extract date\n",
    "                        date = extract_date(date_td.string)    \n",
    "                        \n",
    "                        ##prepare for the next issue \n",
    "                        col = pdf_td.find_next('tr').find_next('td')\n",
    "                        date_td = col.find_next_sibling('td')\n",
    "                        \n",
    "                        ##add the issue into the list\n",
    "                        detail.append((issue,date,html))\n",
    "\n",
    "                    detail_COPs.append(detail)\n",
    "    return detail_COPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how extract_list_COPS works\n",
    "list_cops = extract_list_COPs(soup)\n",
    "\n",
    "for cops_n in list_cops:\n",
    "    print(cops_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how extract_details_COPs works\n",
    "detail_cops = extract_details_COPS(soup)\n",
    "k=1\n",
    "for detail_n in detail_cops:\n",
    "    print(\"COP\",k)\n",
    "    for j in detail_n:\n",
    "        print(j)\n",
    "        \n",
    "    k+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
