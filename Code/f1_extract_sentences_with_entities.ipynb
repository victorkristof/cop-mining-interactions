{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_extract_sentences_with_entities\n",
    "From the file sentences.txt extract all the sentence who contain entities and write them into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected(sentences,s):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    outF = open(s, \"w\")\n",
    "    for line in sentences :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected_issue(list_with_two_entities, number):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    \n",
    "    s = \"s2e\"+str(number)\n",
    "    outF = open(s+\".txt\", \"w\")\n",
    "    for line in list_with_two_entities :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_txt_sentences(txt_file):\n",
    "    \"\"\" Open file that contains all the sentences.\"\"\"\n",
    "    list_tp = open(txt_file)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char.\"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"SPlit the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "\n",
    "    line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"sentences_s1e.txt\"))\n",
    "    print(len(sentences))\n",
    "    sentences = list(set(sentences))\n",
    "    print(len(sentences))\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detetc entity\n",
    "    \n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            for e2 in list_entities:\n",
    "                if(set([e1,e2]).issubset(set(tokens)) and e1 != e2):\n",
    "                    sentences_s2e.append(s)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done:\n",
    "                break\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    s = \"sentences_s2e.txt\"\n",
    "    write_list_collected(sentences_s2e,s)\n",
    "    return sentences_s2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s1e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"sentences.txt\"))\n",
    "    sentences = list(set(sentences))\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s1e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            if(set([e1]).issubset(set(tokens)) ):\n",
    "                sentences_s1e.append(s)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    s = \"sentences_s1e.txt\"\n",
    "    write_list_collected(sentences_s1e,s)\n",
    "    return sentences_s1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_tagged_s2e(list_s2e):\n",
    "    \"\"\" Use NLTK to pos_tag all the sentences from list_s2e and return a list of all the sentences pos_tagged\"\"\"\n",
    "    verbs = []\n",
    "    nnp = []\n",
    "    pos_tagged = []\n",
    "    for s in list_s2e:\n",
    "        tokens = tokenize_sentence(s)\n",
    "        pos_tagged += nltk.pos_tag(tokens)   \n",
    "   \n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def find_verbs_s2e(sentences_s2e,number_most_c):\n",
    "    \"\"\" Find all the verbs VBD and return the number_most_c of them into a list\"\"\"\n",
    "    pos_tagged = find_pos_tagged_s2e(sentences_s2e)\n",
    "    verbs = filter(lambda x:x[1]=='VBD',pos_tagged)\n",
    "    verbs = list(verbs)\n",
    "    counter=collections.Counter(verbs)\n",
    "    return counter.most_common(number_most_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100.00%"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'list_with_two_entities' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d7f421313f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences_s1e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_s1e_issue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-9cd7f1eba22b>\u001b[0m in \u001b[0;36mextract_s1e_issue\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{(i+1)/len(sentences)*100:.2f}%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sentences_s1e.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mwrite_list_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_s1e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences_s1e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-6689b99e8f58>\u001b[0m in \u001b[0;36mwrite_list_collected\u001b[0;34m(sentences, s)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Write list_sentence_with_country.txt file. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_with_two_entities\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# write line to output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_with_two_entities' is not defined"
     ]
    }
   ],
   "source": [
    "sentences_s1e = extract_s1e_issue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21068\n21068\n100.00%100.00%"
    }
   ],
   "source": [
    "sentences_s2e = extract_s2e_issue()"
   ]
  }
 ]
}