{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_extract_sentences_with_entities\n",
    "From the file sentences.txt extract all the sentence who contain entities and write them into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from c1_extract_paragraphe_issue.ipynb\nIssue  34\nimporting Jupyter notebook from c2_extract_sentence_issue.ipynb\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "import collections\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected(sentences,s):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    outF = open(s, \"w\")\n",
    "    for line in sentences :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected_issue(list_with_two_entities, number):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    \n",
    "    s = \"s2e\"+str(number)\n",
    "    outF = open(s+\".txt\", \"w\")\n",
    "    for line in list_with_two_entities :\n",
    "        # write line to output file\n",
    "        outF.write(line)\n",
    "        outF.write('\\n')\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_txt_sentences(txt_file):\n",
    "    \"\"\" Open file that contains all the sentences.\"\"\"\n",
    "    list_tp = open(txt_file)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char.\"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"SPlit the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "\n",
    "    line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"sentences_s1e.txt\"))\n",
    "    print(len(sentences))\n",
    "    sentences = list(set(sentences))\n",
    "    print(len(sentences))\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detetc entity\n",
    "    \n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            for e2 in list_entities:\n",
    "                if(set([e1,e2]).issubset(set(tokens)) and e1 != e2):\n",
    "                    sentences_s2e.append(s)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done:\n",
    "                break\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    s = \"sentences_s2e.txt\"\n",
    "    write_list_collected(sentences_s2e,s)\n",
    "    return sentences_s2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s1e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"sentences.txt\"))\n",
    "    sentences = list(set(sentences))\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s1e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            if(set([e1]).issubset(set(tokens)) ):\n",
    "                sentences_s1e.append(s)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    s = \"sentences_s1e.txt\"\n",
    "    write_list_collected(sentences_s1e,s)\n",
    "    return sentences_s1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-aef259332f78>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-aef259332f78>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    return sentences_s1e = extract_s1e_issue() , sentences_s2e = extract_s2e_issue()\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences():\n",
    "    sentences_s1e = extract_s1e_issue()\n",
    "    sentences_s2e = extract_s2e_issue()\n",
    "    return sentences_s1e = extract_s1e_issue() , sentences_s2e = extract_s2e_issue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to keep only tags needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_ent_vb_conj(sentence, pattern_verb, pattern_conj, list_entities):\n",
    "    \"\"\" Keep only the patterns needed to find the interactions. \"\"\"\n",
    "    pos_tagged = sentence\n",
    "    token_s = []\n",
    "    for i in range(len(pos_tagged)):\n",
    "        w = pos_tagged[i]\n",
    "\n",
    "        if(set([w[0]]).issubset(set(list_entities))):\n",
    "            token_s.append(w)\n",
    "        if(set([w[1]]).issubset(set(pattern_verb))):\n",
    "            token_s.append(w)\n",
    "        if(set([w[0]]).issubset(set(pattern_conj))):\n",
    "            token_s.append(w)\n",
    "\n",
    "    return token_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_all_expected_conj(sentence, pattern_verb, pattern_conj, list_entities): \n",
    "    tokens = []\n",
    "    for x in sentence:\n",
    "        if(x[1]!='IN'):\n",
    "            tokens.append(x)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_tagged_s2e(list_s2e):\n",
    "    \"\"\" Use NLTK to pos_tag all the sentences from list_s2e and return a list of all the sentences pos_tagged\"\"\"\n",
    "    verbs = []\n",
    "    nnp = []\n",
    "    pos_tagged = []\n",
    "    for s in list_s2e:\n",
    "        s = s.replace('\\\\t','')\n",
    "        tokens = tokenize_sentence(s)\n",
    "        pos_tagged.append(nltk.pos_tag(tokens))  \n",
    "   \n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(pos_tagged, function):\n",
    "\n",
    "    # List of patterns needed\n",
    "    pattern_verbs = ['VBD','VBN']\n",
    "    pattern_conj = ['and','with','while']\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    \n",
    "    # Find all the pattern needed in sentences pos_tagged\n",
    "    groups = []\n",
    "    for x in pos_tagged:\n",
    "        groups.append(function(x, pattern_verbs, pattern_conj, list_entities))\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verbs_between(pos_tagged):\n",
    "# Look for all the vebrs between entities : interactions\n",
    "    verbs_interactions = []\n",
    "    pattern_verbs = ['VBD','VBN']\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    link = []\n",
    "\n",
    "    for g in groups:\n",
    "        for i in range(len(g)-2):\n",
    "            x = g[i]\n",
    "            y = g[i+1]\n",
    "            z = g[i+2]\n",
    "            if(x[0] in list_entities and y[1] in pattern_verbs and z[0] in list_entities ):\n",
    "                verbs_interactions.append(y[0])\n",
    "\n",
    "\n",
    "\n",
    "    s = \"verbs_interactions\"\n",
    "    write_list_collected(list(set(verbs_interactions)))\n",
    "\n",
    "    return list(set(verbs_interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verbs_e_v_e(pos_tagged):\n",
    "# Look for all the vebrs between entities : interactions\n",
    "    verbs_interactions = []\n",
    "    pattern_verbs = ['VBD','VBN']\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    link = []\n",
    "\n",
    "    for g in groups:\n",
    "        for i in range(len(g)-2):\n",
    "            x = g[i]\n",
    "            y = g[i+1]\n",
    "            z = g[i+2]\n",
    "            if(x[0] in list_entities and y[1] in pattern_verbs and z[0] in list_entities ):\n",
    "                verbs_interactions.append(y[0])\n",
    "                link.append((str(x[0])+\" - \"+str(y[0])+\" - \"+str(z[0])))\n",
    "    \n",
    "    s = \"link_pattern_e_v_e\"\n",
    "    write_list_collected(link,s)\n",
    "\n",
    "    s = \"verbs_e_v_e\"\n",
    "    write_list_collected(list(set(verbs_interactions)),s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_linked_verbs(sentence, pattern_verbs, pattern_conj, list_entities):\n",
    "    tokens = []\n",
    "    for i in range(len(sentence)):\n",
    "        if(sentence[i][1] != 'IN'):\n",
    "            tokens.append(sentence[i])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find verbs for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_verbs_e_v_e(pos_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5198\n[('SOUTH AFRICA', 'NNP'), ('and', 'CC'), ('JAPAN', 'NNP'), ('underscored', 'JJ'), ('comparability', 'NN'), ('and', 'CC'), ('CHINA', 'NNP'), ('noted', 'VBD'), ('it', 'PRP'), ('includes', 'VBZ'), ('inter', 'JJ'), ('alia', 'NNS'), ('comparable', 'JJ'), ('efforts', 'NNS'), ('and', 'CC'), ('legally', 'RB'), ('binding', 'VBG'), ('commitments', 'NNS'), ('.', '.')]\n"
    }
   ],
   "source": [
    "print(len(pos_tagged))\n",
    "groups = find_patterns(pos_tagged, keep_all_expected_conj)\n",
    "print(groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_conj(groups, conj_sup, list_entities):\n",
    "    \"\"\" Detect collaboration between two entities linked with and. \"\"\"\n",
    "    group = []\n",
    "    for i in range(len(groups[0])-2):\n",
    "        x = groups[0][i]\n",
    "        y = groups[0][i+1]\n",
    "        z = groups[0][i+2]\n",
    "        print(x,y,z)\n",
    "\n",
    "        if(x[0] in list_entities and y[1] in conj_sup and z[0] in list_entities):\n",
    "            group.append((x[0],z[0]))\n",
    "\n",
    "    for i in range(len(group)-1):\n",
    "        if(set(group[i]).intersection(set(group[i+1])) != set()):\n",
    "            group[i] = list(set(group[i]).union(set(group[i+1])))\n",
    "            group[i+1] = []\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = find_pos_tagged_s2e(open(\"sentences_s2e.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_neighbors(groups):\n",
    "     \"\"\" Detect collaboration between two entities linked ,. \"\"\"\n",
    "    group = []\n",
    "    for i in range(len(groups[0])-1):\n",
    "        x = groups[0][i]\n",
    "        y = groups[0][i+1]\n",
    "\n",
    "        if(x[0] in list_entities and y[0] in list_entities):\n",
    "            group.append((x[0],y[0]))\n",
    "\n",
    "    for i in range(len(group)-1):\n",
    "\n",
    "        if(set(group[i]).intersection(set(group[i+1])) != set()):\n",
    "            group[i] = list(set(group[i]).union(set(group[i+1])))\n",
    "            group[i+1] = []\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_by_verbs(groups, verbs_for):\n",
    "     \"\"\" Detect collaboration between two entities linked with verb of support. \"\"\"\n",
    "    group = []\n",
    "    for i in range(len(groups[0])-2):\n",
    "        x = groups[0][i]\n",
    "        y = groups[0][i+1]\n",
    "        z = groups[0][i+2]\n",
    "        print(x,y,z)\n",
    "        if(x[0] in list_entities and y[0] in verbs_for and z[0] in list_entities):\n",
    "            group.append([x[0],z[0]])\n",
    "            \n",
    "    for i in range(len(group)-1):\n",
    "        if(set(group[i]).intersection(set(group[i+1])) != set()):\n",
    "\n",
    "            group[i] = list(set(group[i]).union(set(group[i+1])))\n",
    "            group[i+1] = []\n",
    "\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(pos_tagged,func):  \n",
    "    \"\"\" Find all the pattern needed in sentences pos_tagged. Need to call it with find_linked_verbs or find_groups.  \"\"\"\n",
    "    # List of patterns needed\n",
    "    pattern_verbs = ['VBD','VBN']\n",
    "    pattern_conj = ['and','with','while']\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "    print(len(pos_tagged))\n",
    "    groups = []\n",
    "    for sentence in pos_tagged:\n",
    "        groups.append(func(sentence, pattern_verbs, pattern_conj, list_entities))\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to find parties with same interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_s = find_pos_tagged_s2e(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"The G-77/CHINA supported by CLIMATE ACTION NETWORK and others, opposed by JAPAN AND CHILE\"]\n",
    "pattern_verbs = ['VBD','VBN']\n",
    "pattern_conj = ['CC']\n",
    "list_entities = [s.replace('\\n','') for s in list(open('entities_clean.txt'))]\n",
    "verbs_against = ['opposed']\n",
    "verbs_for = ['supported']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}