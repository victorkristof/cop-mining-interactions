{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_extract_sentences_with_entities\n",
    "From the file sentences.txt extract all the sentence who contain entities and write them into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "importing Jupyter notebook from c1_extract_paragraphe_issue.ipynb\nIssue  34\nimporting Jupyter notebook from c2_extract_sentence_issue.ipynb\nimporting Jupyter notebook from g1_generate_dictionary.ipynb\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/tatianacogne/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import import_ipynb\n",
    "import c1_extract_paragraphe_issue as c1 \n",
    "import c2_extract_sentence_issue as c2 \n",
    "import g1_generate_dictionary as g1\n",
    "import collections\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected_issue_org(data, issue_number):\n",
    "    \"\"\"Write list_sentence_with_country.txt file. \"\"\"\n",
    "    s = \"Text/original_\"+str(issue_number)+\".csv\"\n",
    "    with open(s, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        #header\n",
    "        #writer.writerow(('issue_number','country 1','country 2','behalf','support','spokewith','agreement','delay','opposition','criticism','cooperation','sentence'))\n",
    "        #list\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_collected_issue(list_with_two_entities, number,generated):\n",
    "    if(generated):\n",
    "        s = \"Text/interactions_\"+str(number)+\".csv\"\n",
    "    \"\"\" Generate list_meetings.csv file.\"\"\"\n",
    "    with open(s, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        #header\n",
    "        writer.writerow(('issue_number','country 1','country 2','behalf','support','spokewith','agreement','delay','opposition','criticism','cooperation','sentence'))\n",
    "        #list\n",
    "        writer.writerows(list_with_two_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_txt_sentences(txt_file):\n",
    "    \"\"\" Open file that contains all the sentences.\"\"\"\n",
    "    list_tp = open(txt_file)\n",
    "    return list_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tp(sentence):\n",
    "    \"\"\" Clean the sentence by removing special char.\"\"\"\n",
    "    s = sentence.replace(\"\\r\\n\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\r\\n\",\" \")\n",
    "    s = s.replace('\\t','')\n",
    "    s = s.replace(\"\\s\\s+\",\" \")\n",
    "    s = s.replace(\"\\\\.\",\" \")\n",
    "    s = s.replace(\"\\\\r\\\\n\",\" \")\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"Text/sentences_s1e.txt\"))\n",
    "    sentences = list(set(sentences))[20000:20100]\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    #Split line into words with tokenizer to detetc entity\n",
    "    \n",
    "        tokens = tokenize_sentence(s)\n",
    "        done = False\n",
    "        for e1 in list_entities: \n",
    "            for e2 in list_entities:\n",
    "                if(set([e1,e2]).issubset(set(tokens)) and e1 != e2):\n",
    "                    sentences_s2e.append(s)\n",
    "                    done = True\n",
    "                    break\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    s = \"Text/sentences_s2e.txt\"\n",
    "    write_list_collected(sentences_s2e,s)\n",
    "    return sentences_s2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s1e_issue():\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    sentences = c2.extract_from_txt_sentences(open(\"Text/sentences.txt\"))\n",
    "    sentences = list(set(sentences))\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s1e = []\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        s = s.replace('\\\\t','')\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "        tokens = tokenize_sentence(s,False)\n",
    "\n",
    "        if(len(set(tokens).intersection(set(ENTITIES)) )> 1):\n",
    "            sentences_s1e.append(s)\n",
    "\n",
    "        print(f'{(i+1)/len(sentences)*100:.2f}%', end='\\r')\n",
    "    s = \"Text/sentences_s1e.txt\"\n",
    "    issue_number = 1111\n",
    "    c2.write_sentences(issue_number,sentences_s1e)\n",
    "    return sentences_s1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s2e_issue_number(issue_number):\n",
    "    \"\"\" Extract all the sentence with at least one party inside. \"\"\"\n",
    "\n",
    "    #List sentences\n",
    "    #paragraphes = c1.extract_paragraphes_from_issue(number)\n",
    "    p = c1.extract_paragraphes_from_issue(issue_number)\n",
    "    sentences = c2.extract_from_txt_sentences(p)\n",
    "    #list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))]\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    #Create list that wil contain all the sentences with at least two entities\n",
    "    sentences_s2 = []\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "    #Split line into words with tokenizer to detect entity\n",
    "        tokens = tokenize_sentence(s,False)\n",
    "\n",
    "        if(len(set(tokens).intersection(set(ENTITIES)) )> 1):\n",
    "            sentences_s2.append(s)\n",
    "    c2.write_sentences(issue_number,sentences_s2)\n",
    "    return sentences_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Party Groupings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_party_groupings(sentences):\n",
    "    links = ['for the','on behalf of the ']\n",
    "    list_l = []\n",
    "    for s in sentences:\n",
    "\n",
    "        l = re.findall('for the.*?,| for the.*?and',s)\n",
    "        l = [s.replace('for the ','') for s in l if s.replace('for the ','')[0].isupper()]\n",
    "        l = [s.replace(',','') for s in l]\n",
    "\n",
    "\n",
    "        b = re.findall('on behalf of  .*?,| on behalf of the .*?said| on behalf of the .*?and',s)\n",
    "        b = [s.replace('on behalf of  ','') for s in b]\n",
    "        b = [s.replace(',','') for s in b]\n",
    "\n",
    "        c = re.findall('speaking for the.*?,|speaking for the .*?said|speaking for the .*?and',s)\n",
    "        c = [s.replace('speaking for  ','') for s in b]\n",
    "        c = [s.replace(',','') for s in b]\n",
    "        \n",
    "        list_l +=  l+c+b\n",
    "\n",
    "    write_list_collected(list(set(list_l)),\"party_groupings.txt\")\n",
    "    x = list(set(list_l))\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to filter and analyse the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define functions to pos tag the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, country):\n",
    "    \"\"\"Split the sentence in a way that the entities are together and will be able to be detected.\"\"\"\n",
    "    # Extract list entities\n",
    "    list_entities = [s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]\n",
    "    list_entities = [s.replace('class=\"textstory\"','') for s in list_entities]\n",
    "    \n",
    "    tokens_entities = [l.split(' ') for l in list_entities]\n",
    "    if(country):\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        tokens_entities.append(['for'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "        tokens_entities.append(['speaking','for'])\n",
    "    else :\n",
    "\n",
    "        tokens_entities.append(['on','behalf','of','the'])\n",
    "        tokens_entities.append(['spoke','with'])\n",
    "        tokens_entities.append(['on','behalf','of'])\n",
    "        tokens_entities.append(['for','the'])\n",
    "        tokens_entities.append(['US','$'])\n",
    "        tokens_entities.append(['speaking','for','the'])\n",
    "    tokens_entities.append(['supported','by'])\n",
    "    tokens_entities.append(['opposed','by'])\n",
    "    tokenizer1 = MWETokenizer(tokens_entities, separator=' ')\n",
    "    tokenizer2 = MWETokenizer([['G-77','CHINA']], separator='/')\n",
    "    tokenizer3 = MWETokenizer([['G-77/',' CHINA']], separator=' ')\n",
    "    if(type(sentence) == list):\n",
    "        line = sentence[0].replace(\",\",\"\")\n",
    "    else: \n",
    "        line = sentence.replace(\",\",\"\")\n",
    "    line_splited = word_tokenize(line)\n",
    "    tokens = tokenizer1.tokenize(line_splited) \n",
    "    tokens = tokenizer2.tokenize(tokens) \n",
    "    tokens = tokenizer3.tokenize(tokens) \n",
    "    tokens = [clean_tp(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[('92s', 'CD')]]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "find_pos_tagged_s2e(['92s '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = list(set([s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]))\n",
    "COALITIONS = []\n",
    "SUPPORTS_LINKS = ['with','and','for the','on behalf of the','supported by','speaking for the']\n",
    "OPPOSITION_LINKS= ['opposed by','while','opposed by the']\n",
    "LIST_TAGS = ['IN', 'CC', 'NN', 'NNP', 'JJ','NNPS','MD','VBP','VB','VBZ','VBD','RB','VBN','PRP']\n",
    "PARTY_GROUPINGS = sorted(set([s.replace('\\n','').upper() for s in list(open('Text/party_grouping_clean.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/party_grouping_clean.txt'))] + [s.replace('\\n','') for s in list(open('Text/party_grouping_clean.txt'))]))\n",
    "list_entities = list(set([s.replace('\\n','') for s in list(open('Text/entities_interactions.txt'))] + [s.replace('\\n','').title() for s in list(open('Text/entities_interactions.txt'))]))\n",
    "PARTIES = sorted(set(list_entities).difference(set(PARTY_GROUPINGS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Need to keep all the following tag : 'IN', 'CC', 'NN', 'NNP', 'JJ','NNPS','MD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(pos_tagged, list_tags):  \n",
    "    \"\"\" Find all the pattern in list_tags needed in sentences pos_tagged. \"\"\"\n",
    "    groups = [x[0] for x in pos_tagged[0] if x[1] in list_tags]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Define the groups : 1 (cooperation only) or two groups (opposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_1g(groups): \n",
    "    \"\"\" Find all the entities in the groups. Return a list of entities\"\"\"\n",
    "    groups = [g.replace(',','') for g in groups]\n",
    "    entities = set(groups).intersection(set(ENTITIES))\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2g(groups, opp):\n",
    "    \"\"\" Find all the entities for each groups. Return two lists of entities\"\"\"\n",
    "\n",
    "    index = groups.index(opp)\n",
    "\n",
    "    g1 = find_1g(groups[:index])\n",
    "    g2 = find_1g(groups[index +1:])\n",
    "\n",
    "    return g1, g2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_groups_cooperations(groups):\n",
    "    \"\"\" Return one or two groups with only entities and the original sentence. Return a list with one or two list\"\"\"\n",
    "\n",
    "    # Case 1 : Opposition between two groups\n",
    "    if(set(OPPOSITION_LINKS).intersection(set(groups)) != set()):\n",
    "        opp = list(set(OPPOSITION_LINKS).intersection(set(groups)))[0]\n",
    "        \n",
    "        g1, g2 = find_2g(groups, opp)\n",
    "        return [g1, g2]\n",
    "\n",
    "    # Case 2 : Only support\n",
    "    else:\n",
    "        g1 = find_1g(groups)\n",
    "        return [g1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Detect \"on behalf of\"/ \"for the\"/speaking for for coalition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coalitions(groups, sentence):\n",
    "    \"\"\" Remove all the parties in groups that speak for a coalition, return list group updated if the case. \"\"\"\n",
    "    groups_update = []\n",
    "    list_removed = []\n",
    "\n",
    "    for group in groups: \n",
    "        set_group = set(group)\n",
    "        token =tokenize_sentence(sentence,False)\n",
    "\n",
    "        links = ['for the','on behalf of the','speaking for the','on behalf of','for']\n",
    "\n",
    "        if(set(token).intersection(set(links)) != set()):\n",
    "            for i in range(len(token)-2):\n",
    "\n",
    "                if(token[i] in PARTIES and token[i+1] in links and token[i+2] in PARTY_GROUPINGS and token[i] in set_group):\n",
    "\n",
    "                    l = list(set(token).intersection(set(links)))[0]\n",
    "                    list_removed.append((token[i],l))\n",
    "                    \n",
    "                    set_group.remove(token[i])\n",
    "\n",
    "            groups_update.append(list(set_group))\n",
    "        else : \n",
    "            groups_update.append(group)\n",
    "    return groups_update, list_removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Find cooperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations  \n",
    "import itertools\n",
    "def rSubset(arr, cop): \n",
    "    # return list of all subsets of length r \n",
    "    # to deal with duplicate subsets use  \n",
    "    # set(list(combinations(arr, r))) \n",
    "\n",
    "    l = list(set(list(itertools.product(arr, arr))))\n",
    "\n",
    "    return [(c1.upper(),c2.upper(),cop) for c1,c2 in l if c1 != c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behalf_of(sentence, group_cooperation, link):\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "    for x in countries_B:\n",
    "        tuples.append((list(country_A)[0].upper(),x,['behalf','cooperation']))\n",
    "        \n",
    "    tuples += rSubset(list(countries_B),['behalf of','agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supported_by(sentence, group_cooperation, link ):\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "\n",
    "    for x in countries_B:\n",
    "        \n",
    "        tuples.append((x,list(country_A)[0].upper(),['support','cooperation']))\n",
    "        \n",
    "    tuples += rSubset(list(countries_B),['agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_c(sentence, group_cooperation, link):\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "\n",
    "    countries_B = set(group_cooperation).difference(set(country_A))\n",
    "    tuples = []  \n",
    "    for x in countries_B:\n",
    "        tuples.append((list(country_A)[0].upper(),x,['spokeWith','cooperation']))\n",
    "        tuples.append((x,list(country_A)[0].upper(),['spokeWith','cooperation']))\n",
    "\n",
    "    tuples += rSubset(list(countries_B), ['spokeWith','agreement','cooperation'])\n",
    "\n",
    "    return sorted(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(link, sentence, group_cooperation):\n",
    "    \"\"\"Verify that there is entities in both sides of the link\"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "\n",
    "    index = token.index(list(link)[0])\n",
    "    country_A = set(token[:index]).intersection(set(group_cooperation))\n",
    "    return country_A != set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coop(sentence, group_cooperation):\n",
    "    \"\"\"group_cooperation i one list \"\"\"\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "\n",
    "    behalf = ['speaking for','on behalf of']\n",
    "    behalf = set(token).intersection(set(behalf))\n",
    "\n",
    "    support = ['supported by','supported by the']\n",
    "\n",
    "    support = set(token).intersection(set(support))\n",
    "\n",
    "\n",
    "    tuples = []\n",
    "    cooperation = []\n",
    "    if(behalf != set()):\n",
    "        return behalf_of(sentence, group_cooperation, behalf)\n",
    "    if(support != set() and  check_link(support, sentence, group_cooperation)):\n",
    "        return supported_by(sentence, group_cooperation, support)\n",
    "\n",
    "    else :\n",
    "        return rSubset(group_cooperation,['agreement','cooperation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def opposed_by(sentence, group_cooperation, link):\n",
    "    \n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = group_cooperation[0]\n",
    "    countries_B = group_cooperation[1]\n",
    "\n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "\n",
    "    tuples = [(c2.upper(),c1.upper(),['opposition']) for c1,c2 in tuples] \n",
    "\n",
    "\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence.split(list(link)[0])\n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop([splited[0]], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop([splited[1]], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticized_by(sentence, group_cooperation, link):\n",
    "    token =tokenize_sentence(sentence, True)\n",
    "    index = token.index(list(link)[0])\n",
    "    countries_A = list(set(group_cooperation[0]).intersection(set(token[:index])))\n",
    "    countries_B = list(set(group_cooperation[0]).intersection(set(token[index+1:])))\n",
    "    \n",
    "    # Create tuples for the opposition\n",
    "    tuples = list(itertools.product(countries_A, countries_B))\n",
    "    tuples = [(c1.upper(),c2.upper(),['criticism']) for c1,c2 in tuples]\n",
    "\n",
    "    #Add cooperation between both groups\n",
    "    splited = sentence[0].split(list(link)[0])\n",
    "    \n",
    "    if(len(countries_A)!=1):\n",
    "        tuples_A = coop(splited[0], countries_A)\n",
    "        tuples += tuples_A\n",
    "    \n",
    "    if(len(countries_B)!=1):\n",
    "        tuples_B = coop(splited[1], countries_B)\n",
    "        tuples += tuples_B\n",
    "    \n",
    "    return tuples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_line(issue_number, x):\n",
    "    \"\"\"  'behalf'\t 'support'\t 'spokewith'\t 'agreement'\t 'delay'\t 'opposition'\t 'criticism'\t 'cooperation'\"\"\"\n",
    "    dict_interactions = {'behalf' : 0, 'support' : 0, 'agreement':0 , 'opposition':0,'criticism' :0, 'cooperation':0}\n",
    "    for int in x[2]:\n",
    "        dict_interactions[int] = 1\n",
    "    values = list(dict_interactions.values())\n",
    "\n",
    "    v = []\n",
    "    v.append(issue_number)\n",
    "    v.append(x[0])\n",
    "    v.append(x[1])\n",
    "    v += values\n",
    "    v.append(x[3])\n",
    "    return v\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_double_s(sentences):\n",
    "\n",
    "    tags_wanted = ['VBD','MD']\n",
    "    words_wanted = ['and']\n",
    "    s2 = []\n",
    "    sentences_filtered = []\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    for s in sentences:\n",
    "        pos_tagged = find_pos_tagged_s2e([s])\n",
    "        \n",
    "        filtered = [x for x in pos_tagged[0] if x[0] in PARTIES or x[0] in words_wanted or x[1] in tags_wanted]\n",
    "\n",
    "        if(len(filtered)>= 5):\n",
    "            for i in range(len(filtered)-4):\n",
    "                if( filtered[i][0] in PARTIES and filtered[i+1][1] in tags_wanted and filtered[i+2][0] == 'and' and  filtered[i+3][0] in PARTIES and  filtered[i+4][1]in tags_wanted):\n",
    "\n",
    "                    s_to_filter.append((s,filtered[i+1]))\n",
    "                else : \n",
    "                    s_filtered.append(s)\n",
    "\n",
    "        else:\n",
    "            s_filtered.append(s)\n",
    "        \n",
    "    for s in s_to_filter:\n",
    "\n",
    "        index = s[0].index(s[1][0])\n",
    "        s1 = s[0][:index]\n",
    "        s2 = s[0][index+1:]\n",
    "        s_filtered.append(s1)\n",
    "        s_filtered.append(s2)\n",
    "\n",
    "    \n",
    "    return list(set(s_filtered))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_on_99s_programme(sentences):\n",
    "\n",
    "    set_sentences = set(sentences)\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    for s in sentences:\n",
    "\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "      \n",
    "        for i in range(len(pos_tagged)-1):\n",
    "   \n",
    "            if(pos_tagged[i][0] == 'on' and pos_tagged[i+1][0] in ENTITIES):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i+1][0]))\n",
    "                set_sentences.remove(s)\n",
    "\n",
    "            if((pos_tagged[i+1][0] == '92s' or pos_tagged[i+1][0] == 'Programme') and pos_tagged[i][0] in ENTITIES):\n",
    "                s_to_filter.append((s,pos_tagged[i][0]))\n",
    "                set_sentences.remove(s)\n",
    "    \n",
    "    for s in s_to_filter:\n",
    "\n",
    "        s_f = s[0].replace(s[1],'')\n",
    "        s_filtered.append(s_f)\n",
    "\n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "    \n",
    "    return list(set(s_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(group_cooperation):\n",
    "    if(len(group_cooperation) == 2 or len(group_cooperation[0])>1):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doubles(sentence, group_cooperation):\n",
    "\n",
    "    gc_new = []\n",
    "    \n",
    "    for g in group_cooperation:\n",
    "\n",
    "        g_upper = [e.upper() for e in g]\n",
    "        g_title = [e.title() for e in g]\n",
    "        if(len(set(g_upper)) != len(g)):\n",
    "            gc_new.append(list(set(g).difference(set(g_title))))\n",
    "            removed = list(set(g).intersection(set(g_title)))\n",
    "            for r in removed:\n",
    "                sentence = sentence.replace(r,'')\n",
    "        else:\n",
    "            gc_new.append(g)\n",
    "    return gc_new, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inversions(sentences):\n",
    "\n",
    "    set_sentences = set(sentences)\n",
    "    s_to_filter = []\n",
    "    s_filtered = []\n",
    "    verbs_int =['Supported','Opposed']\n",
    "    for s in sentences:\n",
    "\n",
    "        pos_tagged = find_pos_tagged_s2e([s])[0]\n",
    "\n",
    "        for i in range(len(pos_tagged)-3):\n",
    "\n",
    "            if(pos_tagged[i][0] in verbs_int and pos_tagged[i+1][0] == 'by' and pos_tagged[i+2][0] in ENTITIES and pos_tagged[i+3][0] in ENTITIES):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0],pos_tagged[i+1][0],pos_tagged[i+2][0],pos_tagged[i+3][0]))\n",
    "                set_sentences.remove(s)\n",
    "        ## The EU ...\n",
    "        for i in range(len(pos_tagged)-4):\n",
    "\n",
    "            if(pos_tagged[i][0] in verbs_int and pos_tagged[i+1][0] == 'by' and pos_tagged[i+2][0] == 'the' and pos_tagged[i+3][0] in ENTITIES and pos_tagged[i+4][0] in ENTITIES):\n",
    "\n",
    "                s_to_filter.append((s,pos_tagged[i][0],pos_tagged[i+1][0],pos_tagged[i+3][0],pos_tagged[i+4][0]))\n",
    "                set_sentences.remove(s)\n",
    "        \n",
    "    \n",
    "    for s in s_to_filter:\n",
    "        sentence = s[0]\n",
    "        verb = s[1]\n",
    "        by = s[2]\n",
    "        c_a = s[3]\n",
    "        c_b = s[4]\n",
    "        s_f =sentence.replace(by,'').replace(c_a,'').replace(c_b,'').replace(verb,'')\n",
    "        s_f = c_b + \" \"+ verb.lower() + \" \" + by + \" \" + c_a + \" \" + s_f\n",
    "        s_filtered.append(s_f)\n",
    "\n",
    " \n",
    "    s_filtered += list(set_sentences)\n",
    "    \n",
    "    return list(set(s_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "\n",
    "    for key, value in DICTIONARY.items(): \n",
    "        if (len(value) == 1 and val == value[0]): \n",
    "             key_country = key \n",
    "        if(len(value) > 1 and val in value):\n",
    "            key_country = key\n",
    "\n",
    "            \n",
    "    return DICTIONARY_NUM[key_country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id(interactions_):\n",
    "\n",
    "    for i in interactions_:\n",
    "        c_a = i[1]\n",
    "\n",
    "\n",
    "        c_b = i[2]\n",
    "\n",
    "        id_cb = 9999\n",
    "        id_ca = 9999\n",
    "        if(c_a in NAMES):\n",
    "            id_ca = get_key(c_a)\n",
    "        if(c_a.upper() in NAMES):\n",
    "            \n",
    "            id_ca = get_key(c_a.upper())\n",
    "        if(c_b in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b)\n",
    "        if(c_b.upper() in NAMES):\n",
    "\n",
    "            id_cb = get_key(c_b.upper())\n",
    "\n",
    "        i.append(id_ca)\n",
    "        i.append(id_cb)\n",
    "\n",
    "    return interactions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cooperations):\n",
    "    id_ca = [x[10] for x in cooperations]\n",
    "    id_cb = [x[11] for x in cooperations]\n",
    "    behalf = [x[3] for x in cooperations]\n",
    "    support = [x[4] for x in cooperations]\n",
    "    agreement =[x[5] for x in cooperations]\n",
    "    opposition =[x[6] for x in cooperations]\n",
    "    criticism =[x[7] for x in cooperations]\n",
    "    cooperation =[x[8] for x in cooperations]\n",
    "    dict_issue = {'type': 'generated','issue': cooperations[0][0],'id_ca':id_ca,'id_cb':id_cb, 'behalf':behalf,'support':support,'agreement':agreement,'opposition':opposition,'criticism':criticism,'cooperation':cooperation}\n",
    "    df = pd.DataFrame(dict_issue)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(sentences, issue_number):\n",
    "    interactions_ = []\n",
    "    sentences_int = []\n",
    "    for sentence_original in sentences:\n",
    "\n",
    "        sentence_original = re.sub('class=\"ENB-Body\" align=\"justify\"','',sentence_original)\n",
    "        sentence_original = re.sub('>','',sentence_original)\n",
    "        \n",
    "        pos_tagged = find_pos_tagged_s2e([sentence_original])\n",
    "\n",
    "        tags_filtered = find_patterns(pos_tagged, LIST_TAGS)\n",
    "\n",
    "        group_cooperation = detect_groups_cooperations(tags_filtered)\n",
    "        group_cooperation, list_removed  = find_coalitions(group_cooperation, sentence_original)\n",
    "            \n",
    "        interactions = []\n",
    "        u = ''\n",
    "\n",
    "        sentence = sentence_original\n",
    "        for c in list_removed:\n",
    "            sentence = u.join(sentence.split(c[0]))\n",
    "            sentence = u.join(sentence.split(c[1]))\n",
    "\n",
    "        # Remove double (CROATIA, Croatia)\n",
    "        group_cooperation, sentence = check_doubles(sentence, group_cooperation)\n",
    "\n",
    "        # Check if interactions after all the filters\n",
    "        interaction_bool = check_interaction(group_cooperation)\n",
    "\n",
    "        if(interaction_bool):\n",
    "            sentences_int.append(sentence_original)\n",
    "\n",
    "            token = tokenize_sentence(sentence, True)\n",
    "            opposition = ['opposed by','while','opposed by the']\n",
    "            opposition = set(token).intersection(set(opposition))\n",
    "\n",
    "            criticism = ['criticized']\n",
    "            criticism= set(token).intersection(set(criticism))\n",
    "            \n",
    "            cooperations = []\n",
    "\n",
    "            if(opposition != set() and len(group_cooperation) ==2):\n",
    "                    # group_cooperation contain 2 lists\n",
    "\n",
    "                interactions += opposed_by(sentence, group_cooperation, opposition)\n",
    "\n",
    "            else:\n",
    "                if(criticism != set()):\n",
    "                        # group_cooperation contain 1 list ?\n",
    "                    interactions += criticized_by(sentence, group_cooperation, criticism)\n",
    "                            \n",
    "                else :\n",
    "        \n",
    "                        # group_cooperation contain 1 list\n",
    "                    interactions += coop(sentence, group_cooperation[0])\n",
    "            s = str(sentence_original)        \n",
    "            interactions = [(x[0],x[1],x[2],s) for x in interactions]\n",
    "\n",
    "            interactions_ += interactions\n",
    "        \n",
    "    interactions_ = [ write_line(issue_number, x) for x in interactions_]\n",
    "    interactions_ = add_id(interactions_)\n",
    "    print('Number of observations : ', len(interactions_))\n",
    "    print('Number of sentences : ', len(sentences_int))\n",
    "    #c2.write_sentences(issue_number,sentences_int)\n",
    "    write_list_collected_issue(interactions_ ,issue_number, True)\n",
    "    df = create_df(interactions_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://enb.iisd.org/vol12/enb12255e.html\nIssue  255\n"
    }
   ],
   "source": [
    "issue_number = 255\n",
    "sentences = extract_s2e_issue_number(issue_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY = g1.compute_dictionary()\n",
    "n = 0\n",
    "DICTIONARY_NUM = dict()\n",
    "for k in DICTIONARY:\n",
    "\n",
    "    DICTIONARY_NUM[k] = n \n",
    "    n +=1\n",
    "NAMES = []\n",
    "for x in DICTIONARY.values():\n",
    "    if(len(x) == 1):\n",
    "        NAMES.append(x[0])\n",
    "\n",
    "    else:\n",
    "        NAMES += [k for k in x]\n",
    "\n",
    "n = 0\n",
    "DICTIONARY_NUM = dict()\n",
    "for k in DICTIONARY:\n",
    "\n",
    "    DICTIONARY_NUM[k] = n \n",
    "    n +=1\n",
    "NAMES = []\n",
    "for x in DICTIONARY.values():\n",
    "    if(len(x) == 1):\n",
    "        NAMES.append(x[0])\n",
    "\n",
    "    else:\n",
    "        NAMES += [k for k in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWITZERLAND noted that the new criteria seek to simplify the incremental cost principle and the US welcomed the differentiation in criteria applied to the General Trust Fund and SCCF.\n",
    "\n",
    "The REPUBLIC OF KOREA offered to host a global workshop. MOZAMBIQUE requested that a workshop be held in her country.\n",
    "\n",
    "\n",
    "MICRONESIA emphasized SIDS� diffi culties in accessing\\nfunds. NEW ZEALAND suggested that the GEF should be more\\nfl exible and provide support for small-scale projects in SIDS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit162d60da764a43dc83edcb73a443ab01",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}